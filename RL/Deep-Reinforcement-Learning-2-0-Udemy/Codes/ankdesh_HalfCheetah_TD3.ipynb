{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        \n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage [int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward, done = self.storage[i]\n",
    "            batch_states.append(np.array(state,copy=False))\n",
    "            batch_next_states.append(np.array(next_state,copy=False))\n",
    "            batch_actions.append(np.array(action,copy=False))\n",
    "            batch_rewards.append(np.array(reward,copy=False))\n",
    "            batch_dones.append(np.array(done,copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1,1), np.array(batch_dones).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300,action_dim)\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300,1)\n",
    "\n",
    "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_5 = nn.Linear(400, 300)\n",
    "        self.layer_6 = nn.Linear(300,1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        \n",
    "        x2 = F.relu(self.layer_4(xu))\n",
    "        x2 = F.relu(self.layer_5(x2))\n",
    "        x2 = self.layer_6(x2) \n",
    "        \n",
    "        return x1, x2\n",
    "    \n",
    "    def Q1(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TD3(object):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_actions):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        \n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "        \n",
    "    def select_action (self, state):\n",
    "        state = torch.Tensor(state.reshape(1,-1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def train (self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.9, policy_freq = 2):\n",
    "        for it in range(iterations):\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "            \n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "            done = torch.Tensor(batch_dones).to(device)\n",
    "                \n",
    "            next_action = self.actor_target(next_state)\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            target_Q1, target_Q2 = self.critic(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "        \n",
    "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "            \n",
    "            current_Q1, current_Q2 = self.critic(state, action)\n",
    "            \n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "            \n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "            \n",
    "            if it % policy_freq == 0:\n",
    "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1429.426163\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 1000 Episode Num: 1 Reward: -1421.196814044661\n",
      "Total Timesteps: 2000 Episode Num: 2 Reward: -1381.458921209207\n",
      "Total Timesteps: 3000 Episode Num: 3 Reward: -1347.956393223308\n",
      "Total Timesteps: 4000 Episode Num: 4 Reward: -1345.3740576898224\n",
      "Total Timesteps: 5000 Episode Num: 5 Reward: -1142.1139223371756\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1594.953123\n",
      "---------------------------------------\n",
      "Total Timesteps: 6000 Episode Num: 6 Reward: -1328.2884139486823\n",
      "Total Timesteps: 7000 Episode Num: 7 Reward: -1237.062369387229\n",
      "Total Timesteps: 8000 Episode Num: 8 Reward: -1364.5720257589855\n",
      "Total Timesteps: 9000 Episode Num: 9 Reward: -1143.6733862080475\n",
      "Total Timesteps: 10000 Episode Num: 10 Reward: -1028.4001936918824\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1461.010950\n",
      "---------------------------------------\n",
      "Total Timesteps: 11000 Episode Num: 11 Reward: -1403.063188609053\n",
      "Total Timesteps: 12000 Episode Num: 12 Reward: -1548.8304495519828\n",
      "Total Timesteps: 13000 Episode Num: 13 Reward: -1563.3438544207838\n",
      "Total Timesteps: 14000 Episode Num: 14 Reward: -1613.2509222876654\n",
      "Total Timesteps: 15000 Episode Num: 15 Reward: -1487.558073682566\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1594.464403\n",
      "---------------------------------------\n",
      "Total Timesteps: 16000 Episode Num: 16 Reward: -1578.4433278896608\n",
      "Total Timesteps: 17000 Episode Num: 17 Reward: -1666.6637024722886\n",
      "Total Timesteps: 18000 Episode Num: 18 Reward: -1665.2588895258416\n",
      "Total Timesteps: 19000 Episode Num: 19 Reward: 323.46876103125607\n",
      "Total Timesteps: 20000 Episode Num: 20 Reward: 266.17566156303695\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -175.064000\n",
      "---------------------------------------\n",
      "Total Timesteps: 21000 Episode Num: 21 Reward: 272.6482788197348\n",
      "Total Timesteps: 22000 Episode Num: 22 Reward: 239.99578239367617\n",
      "Total Timesteps: 23000 Episode Num: 23 Reward: 285.6864665557053\n",
      "Total Timesteps: 24000 Episode Num: 24 Reward: -1621.25412003172\n",
      "Total Timesteps: 25000 Episode Num: 25 Reward: -530.9008625895059\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 74.934366\n",
      "---------------------------------------\n",
      "Total Timesteps: 26000 Episode Num: 26 Reward: -1444.7698459657859\n",
      "Total Timesteps: 27000 Episode Num: 27 Reward: 283.32121958990734\n",
      "Total Timesteps: 28000 Episode Num: 28 Reward: -1642.8026234257977\n",
      "Total Timesteps: 29000 Episode Num: 29 Reward: -1595.5690330814612\n",
      "Total Timesteps: 30000 Episode Num: 30 Reward: 160.30436409659404\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -787.889731\n",
      "---------------------------------------\n",
      "Total Timesteps: 31000 Episode Num: 31 Reward: -1290.9208454330244\n",
      "Total Timesteps: 32000 Episode Num: 32 Reward: -1282.772431087769\n",
      "Total Timesteps: 33000 Episode Num: 33 Reward: 152.99929915500186\n",
      "Total Timesteps: 34000 Episode Num: 34 Reward: -1517.5240868421483\n",
      "Total Timesteps: 35000 Episode Num: 35 Reward: 164.90251323967703\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -960.302356\n",
      "---------------------------------------\n",
      "Total Timesteps: 36000 Episode Num: 36 Reward: -1491.5334137744112\n",
      "Total Timesteps: 37000 Episode Num: 37 Reward: -1474.6968847509604\n",
      "Total Timesteps: 38000 Episode Num: 38 Reward: -1507.9600596980724\n",
      "Total Timesteps: 39000 Episode Num: 39 Reward: 152.89852868401593\n",
      "Total Timesteps: 40000 Episode Num: 40 Reward: 151.51312685864718\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -773.464105\n",
      "---------------------------------------\n",
      "Total Timesteps: 41000 Episode Num: 41 Reward: 156.35038259554213\n",
      "Total Timesteps: 42000 Episode Num: 42 Reward: 149.53196872165316\n",
      "Total Timesteps: 43000 Episode Num: 43 Reward: -1335.5980455569918\n",
      "Total Timesteps: 44000 Episode Num: 44 Reward: -1519.430175882231\n",
      "Total Timesteps: 45000 Episode Num: 45 Reward: 152.20342270780486\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -841.733698\n",
      "---------------------------------------\n",
      "Total Timesteps: 46000 Episode Num: 46 Reward: 156.16402392054667\n",
      "Total Timesteps: 47000 Episode Num: 47 Reward: 159.64395816193718\n",
      "Total Timesteps: 48000 Episode Num: 48 Reward: 148.10528655048952\n",
      "Total Timesteps: 49000 Episode Num: 49 Reward: -1486.0261688259743\n",
      "Total Timesteps: 50000 Episode Num: 50 Reward: 152.02496419614369\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -434.394429\n",
      "---------------------------------------\n",
      "Total Timesteps: 51000 Episode Num: 51 Reward: -1448.6074356489514\n",
      "Total Timesteps: 52000 Episode Num: 52 Reward: -1292.7724843805345\n",
      "Total Timesteps: 53000 Episode Num: 53 Reward: -1380.697383026686\n",
      "Total Timesteps: 54000 Episode Num: 54 Reward: -1247.9393069227333\n",
      "Total Timesteps: 55000 Episode Num: 55 Reward: -1479.565171685152\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -628.428318\n",
      "---------------------------------------\n",
      "Total Timesteps: 56000 Episode Num: 56 Reward: -1470.4336444160092\n",
      "Total Timesteps: 57000 Episode Num: 57 Reward: 192.44222014365306\n",
      "Total Timesteps: 58000 Episode Num: 58 Reward: 148.3381759594261\n",
      "Total Timesteps: 59000 Episode Num: 59 Reward: -1515.0790646605342\n",
      "Total Timesteps: 60000 Episode Num: 60 Reward: -1319.9851778858367\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -857.056222\n",
      "---------------------------------------\n",
      "Total Timesteps: 61000 Episode Num: 61 Reward: -899.5478053987281\n",
      "Total Timesteps: 62000 Episode Num: 62 Reward: 164.1372028267513\n",
      "Total Timesteps: 63000 Episode Num: 63 Reward: -1470.5789884419223\n",
      "Total Timesteps: 64000 Episode Num: 64 Reward: 157.47051262028245\n",
      "Total Timesteps: 65000 Episode Num: 65 Reward: -1673.6422366717866\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -837.402956\n",
      "---------------------------------------\n",
      "Total Timesteps: 66000 Episode Num: 66 Reward: -1341.1705381482366\n",
      "Total Timesteps: 67000 Episode Num: 67 Reward: -1694.4581566918578\n",
      "Total Timesteps: 68000 Episode Num: 68 Reward: -1699.4866746243404\n",
      "Total Timesteps: 69000 Episode Num: 69 Reward: -1698.7689562845292\n",
      "Total Timesteps: 70000 Episode Num: 70 Reward: -1683.4510254374188\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.060381\n",
      "---------------------------------------\n",
      "Total Timesteps: 71000 Episode Num: 71 Reward: -1693.937653712718\n",
      "Total Timesteps: 72000 Episode Num: 72 Reward: -1688.7074055954283\n",
      "Total Timesteps: 73000 Episode Num: 73 Reward: -1693.1406079008254\n",
      "Total Timesteps: 74000 Episode Num: 74 Reward: -1580.4857328642245\n",
      "Total Timesteps: 75000 Episode Num: 75 Reward: -1592.7489219121385\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1590.117915\n",
      "---------------------------------------\n",
      "Total Timesteps: 76000 Episode Num: 76 Reward: -1582.0420839437813\n",
      "Total Timesteps: 77000 Episode Num: 77 Reward: -1604.8635931752485\n",
      "Total Timesteps: 78000 Episode Num: 78 Reward: -1602.6924374435594\n",
      "Total Timesteps: 79000 Episode Num: 79 Reward: -1727.7994093640248\n",
      "Total Timesteps: 80000 Episode Num: 80 Reward: -1587.9652642654\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1195.989111\n",
      "---------------------------------------\n",
      "Total Timesteps: 81000 Episode Num: 81 Reward: -884.4210501117936\n",
      "Total Timesteps: 82000 Episode Num: 82 Reward: -1406.6145504577923\n",
      "Total Timesteps: 83000 Episode Num: 83 Reward: -636.4457620868972\n",
      "Total Timesteps: 84000 Episode Num: 84 Reward: -753.3744654523015\n",
      "Total Timesteps: 85000 Episode Num: 85 Reward: -1690.403416547467\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1699.257305\n",
      "---------------------------------------\n",
      "Total Timesteps: 86000 Episode Num: 86 Reward: -1692.0226283649565\n",
      "Total Timesteps: 87000 Episode Num: 87 Reward: -1694.9892143049192\n",
      "Total Timesteps: 88000 Episode Num: 88 Reward: -1695.2081776184561\n",
      "Total Timesteps: 89000 Episode Num: 89 Reward: -1704.3007567613924\n",
      "Total Timesteps: 90000 Episode Num: 90 Reward: -1688.2286533816794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.572761\n",
      "---------------------------------------\n",
      "Total Timesteps: 91000 Episode Num: 91 Reward: -1697.680248342473\n",
      "Total Timesteps: 92000 Episode Num: 92 Reward: -1694.4753060026972\n",
      "Total Timesteps: 93000 Episode Num: 93 Reward: -1688.9614078909021\n",
      "Total Timesteps: 94000 Episode Num: 94 Reward: -1685.1218151419964\n",
      "Total Timesteps: 95000 Episode Num: 95 Reward: -1683.6890968087741\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.950838\n",
      "---------------------------------------\n",
      "Total Timesteps: 96000 Episode Num: 96 Reward: -1687.842431427437\n",
      "Total Timesteps: 97000 Episode Num: 97 Reward: -1678.4798697054432\n",
      "Total Timesteps: 98000 Episode Num: 98 Reward: -1694.235300438494\n",
      "Total Timesteps: 99000 Episode Num: 99 Reward: -1695.773831844798\n",
      "Total Timesteps: 100000 Episode Num: 100 Reward: -1692.1523333541602\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1698.822399\n",
      "---------------------------------------\n",
      "Total Timesteps: 101000 Episode Num: 101 Reward: -1693.122215410347\n",
      "Total Timesteps: 102000 Episode Num: 102 Reward: -1695.454373940699\n",
      "Total Timesteps: 103000 Episode Num: 103 Reward: -1697.4571234073971\n",
      "Total Timesteps: 104000 Episode Num: 104 Reward: -1695.454189337753\n",
      "Total Timesteps: 105000 Episode Num: 105 Reward: -1697.6531463913775\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1698.603585\n",
      "---------------------------------------\n",
      "Total Timesteps: 106000 Episode Num: 106 Reward: -1690.6416162695502\n",
      "Total Timesteps: 107000 Episode Num: 107 Reward: -1692.4152855349112\n",
      "Total Timesteps: 108000 Episode Num: 108 Reward: -1695.4546591042822\n",
      "Total Timesteps: 109000 Episode Num: 109 Reward: -1697.6814432489907\n",
      "Total Timesteps: 110000 Episode Num: 110 Reward: -1691.2556703513608\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.395878\n",
      "---------------------------------------\n",
      "Total Timesteps: 111000 Episode Num: 111 Reward: -1699.0209994475076\n",
      "Total Timesteps: 112000 Episode Num: 112 Reward: -1692.308520967754\n",
      "Total Timesteps: 113000 Episode Num: 113 Reward: -1704.2283730651616\n",
      "Total Timesteps: 114000 Episode Num: 114 Reward: -1689.7380421121236\n",
      "Total Timesteps: 115000 Episode Num: 115 Reward: -1692.8472654084626\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.852867\n",
      "---------------------------------------\n",
      "Total Timesteps: 116000 Episode Num: 116 Reward: -1691.8441231258998\n",
      "Total Timesteps: 117000 Episode Num: 117 Reward: -1691.489992469281\n",
      "Total Timesteps: 118000 Episode Num: 118 Reward: -1686.4454868439034\n",
      "Total Timesteps: 119000 Episode Num: 119 Reward: -1689.376483003969\n",
      "Total Timesteps: 120000 Episode Num: 120 Reward: -1690.688360403874\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.026431\n",
      "---------------------------------------\n",
      "Total Timesteps: 121000 Episode Num: 121 Reward: -1695.1625957645977\n",
      "Total Timesteps: 122000 Episode Num: 122 Reward: -1700.1094598753416\n",
      "Total Timesteps: 123000 Episode Num: 123 Reward: -1699.6829653656232\n",
      "Total Timesteps: 124000 Episode Num: 124 Reward: -1688.8575614204567\n",
      "Total Timesteps: 125000 Episode Num: 125 Reward: -1698.7878724682453\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.831677\n",
      "---------------------------------------\n",
      "Total Timesteps: 126000 Episode Num: 126 Reward: -1689.887397544196\n",
      "Total Timesteps: 127000 Episode Num: 127 Reward: -1700.9605597456004\n",
      "Total Timesteps: 128000 Episode Num: 128 Reward: -1700.7512764781056\n",
      "Total Timesteps: 129000 Episode Num: 129 Reward: -1700.2698818640533\n",
      "Total Timesteps: 130000 Episode Num: 130 Reward: -1696.8374767348464\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.668554\n",
      "---------------------------------------\n",
      "Total Timesteps: 131000 Episode Num: 131 Reward: -1690.759069827625\n",
      "Total Timesteps: 132000 Episode Num: 132 Reward: -1693.955511899542\n",
      "Total Timesteps: 133000 Episode Num: 133 Reward: -1687.8151215679109\n",
      "Total Timesteps: 134000 Episode Num: 134 Reward: -1692.7282905159195\n",
      "Total Timesteps: 135000 Episode Num: 135 Reward: -1692.4041617185148\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.213308\n",
      "---------------------------------------\n",
      "Total Timesteps: 136000 Episode Num: 136 Reward: -1690.4441655216137\n",
      "Total Timesteps: 137000 Episode Num: 137 Reward: -1692.8119954135254\n",
      "Total Timesteps: 138000 Episode Num: 138 Reward: -1693.3233926765406\n",
      "Total Timesteps: 139000 Episode Num: 139 Reward: -1703.5941204335688\n",
      "Total Timesteps: 140000 Episode Num: 140 Reward: -1691.8952588887962\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.778025\n",
      "---------------------------------------\n",
      "Total Timesteps: 141000 Episode Num: 141 Reward: -1691.190715895039\n",
      "Total Timesteps: 142000 Episode Num: 142 Reward: -1681.2319397175334\n",
      "Total Timesteps: 143000 Episode Num: 143 Reward: -1693.5018427456225\n",
      "Total Timesteps: 144000 Episode Num: 144 Reward: -1688.5364000004445\n",
      "Total Timesteps: 145000 Episode Num: 145 Reward: -1688.7753190544802\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.971590\n",
      "---------------------------------------\n",
      "Total Timesteps: 146000 Episode Num: 146 Reward: -1703.7439448022008\n",
      "Total Timesteps: 147000 Episode Num: 147 Reward: -1693.113245477206\n",
      "Total Timesteps: 148000 Episode Num: 148 Reward: -1691.8020621142698\n",
      "Total Timesteps: 149000 Episode Num: 149 Reward: -1699.6707616109397\n",
      "Total Timesteps: 150000 Episode Num: 150 Reward: -1689.7142618640462\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1706.402783\n",
      "---------------------------------------\n",
      "Total Timesteps: 151000 Episode Num: 151 Reward: -1697.2644633304524\n",
      "Total Timesteps: 152000 Episode Num: 152 Reward: -1692.9842308804418\n",
      "Total Timesteps: 153000 Episode Num: 153 Reward: -1691.0785919981765\n",
      "Total Timesteps: 154000 Episode Num: 154 Reward: -1700.9879007826835\n",
      "Total Timesteps: 155000 Episode Num: 155 Reward: -1693.4140760183855\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1705.160049\n",
      "---------------------------------------\n",
      "Total Timesteps: 156000 Episode Num: 156 Reward: -1700.4189164553047\n",
      "Total Timesteps: 157000 Episode Num: 157 Reward: -1691.3985669926712\n",
      "Total Timesteps: 158000 Episode Num: 158 Reward: -1700.278828980956\n",
      "Total Timesteps: 159000 Episode Num: 159 Reward: -1688.5866090909278\n",
      "Total Timesteps: 160000 Episode Num: 160 Reward: -1689.859975609639\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.682050\n",
      "---------------------------------------\n",
      "Total Timesteps: 161000 Episode Num: 161 Reward: -1688.693700300724\n",
      "Total Timesteps: 162000 Episode Num: 162 Reward: -1695.8922222079086\n",
      "Total Timesteps: 163000 Episode Num: 163 Reward: -1691.0779638486742\n",
      "Total Timesteps: 164000 Episode Num: 164 Reward: -1688.1165987640754\n",
      "Total Timesteps: 165000 Episode Num: 165 Reward: -1693.5744317097676\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.677144\n",
      "---------------------------------------\n",
      "Total Timesteps: 166000 Episode Num: 166 Reward: -1694.125600685414\n",
      "Total Timesteps: 167000 Episode Num: 167 Reward: -1686.7858108564758\n",
      "Total Timesteps: 168000 Episode Num: 168 Reward: -1692.5135557472647\n",
      "Total Timesteps: 169000 Episode Num: 169 Reward: -1688.4347736687732\n",
      "Total Timesteps: 170000 Episode Num: 170 Reward: -1694.8312908070639\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1704.481779\n",
      "---------------------------------------\n",
      "Total Timesteps: 171000 Episode Num: 171 Reward: -1700.9975994180243\n",
      "Total Timesteps: 172000 Episode Num: 172 Reward: -1694.9832348385132\n",
      "Total Timesteps: 173000 Episode Num: 173 Reward: -1687.7545371478236\n",
      "Total Timesteps: 174000 Episode Num: 174 Reward: -1694.4637651694331\n",
      "Total Timesteps: 175000 Episode Num: 175 Reward: -1687.7033915223255\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.814444\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 176000 Episode Num: 176 Reward: -1692.7259910714422\n",
      "Total Timesteps: 177000 Episode Num: 177 Reward: -1691.585908541653\n",
      "Total Timesteps: 178000 Episode Num: 178 Reward: -1698.7445979877089\n",
      "Total Timesteps: 179000 Episode Num: 179 Reward: -1695.017559302943\n",
      "Total Timesteps: 180000 Episode Num: 180 Reward: -1698.6303054262698\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.827414\n",
      "---------------------------------------\n",
      "Total Timesteps: 181000 Episode Num: 181 Reward: -1698.3166709995064\n",
      "Total Timesteps: 182000 Episode Num: 182 Reward: -1693.0806918905762\n",
      "Total Timesteps: 183000 Episode Num: 183 Reward: -1688.115875846005\n",
      "Total Timesteps: 184000 Episode Num: 184 Reward: -1695.533752562598\n",
      "Total Timesteps: 185000 Episode Num: 185 Reward: -1690.105850036978\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.190273\n",
      "---------------------------------------\n",
      "Total Timesteps: 186000 Episode Num: 186 Reward: -1695.8294401077594\n",
      "Total Timesteps: 187000 Episode Num: 187 Reward: -1692.9252025064125\n",
      "Total Timesteps: 188000 Episode Num: 188 Reward: -1689.2167266652184\n",
      "Total Timesteps: 189000 Episode Num: 189 Reward: -1688.1858758069754\n",
      "Total Timesteps: 190000 Episode Num: 190 Reward: -1694.44184155329\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.583846\n",
      "---------------------------------------\n",
      "Total Timesteps: 191000 Episode Num: 191 Reward: -1675.4335395482847\n",
      "Total Timesteps: 192000 Episode Num: 192 Reward: -1685.480956391325\n",
      "Total Timesteps: 193000 Episode Num: 193 Reward: -1680.324181745772\n",
      "Total Timesteps: 194000 Episode Num: 194 Reward: -1699.6386691386144\n",
      "Total Timesteps: 195000 Episode Num: 195 Reward: -1697.9203561428162\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.664313\n",
      "---------------------------------------\n",
      "Total Timesteps: 196000 Episode Num: 196 Reward: -1694.5107944126223\n",
      "Total Timesteps: 197000 Episode Num: 197 Reward: -1688.2563364760924\n",
      "Total Timesteps: 198000 Episode Num: 198 Reward: -1687.2929739711537\n",
      "Total Timesteps: 199000 Episode Num: 199 Reward: -1696.989635512091\n",
      "Total Timesteps: 200000 Episode Num: 200 Reward: -1694.3281471187706\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1704.194920\n",
      "---------------------------------------\n",
      "Total Timesteps: 201000 Episode Num: 201 Reward: -1693.8390764777619\n",
      "Total Timesteps: 202000 Episode Num: 202 Reward: -1681.5895649845795\n",
      "Total Timesteps: 203000 Episode Num: 203 Reward: -1702.9106627735184\n",
      "Total Timesteps: 204000 Episode Num: 204 Reward: -1694.350966109631\n",
      "Total Timesteps: 205000 Episode Num: 205 Reward: -1697.7842177203959\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.350472\n",
      "---------------------------------------\n",
      "Total Timesteps: 206000 Episode Num: 206 Reward: -1688.873332135109\n",
      "Total Timesteps: 207000 Episode Num: 207 Reward: -1691.615283766053\n",
      "Total Timesteps: 208000 Episode Num: 208 Reward: -1691.3053062357928\n",
      "Total Timesteps: 209000 Episode Num: 209 Reward: -1698.0561819704942\n",
      "Total Timesteps: 210000 Episode Num: 210 Reward: -1693.4829504644279\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.435619\n",
      "---------------------------------------\n",
      "Total Timesteps: 211000 Episode Num: 211 Reward: -1697.6538515227548\n",
      "Total Timesteps: 212000 Episode Num: 212 Reward: -1685.5417167503604\n",
      "Total Timesteps: 213000 Episode Num: 213 Reward: -1688.4905825252085\n",
      "Total Timesteps: 214000 Episode Num: 214 Reward: -1695.637004408723\n",
      "Total Timesteps: 215000 Episode Num: 215 Reward: -1690.3520665049014\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.966768\n",
      "---------------------------------------\n",
      "Total Timesteps: 216000 Episode Num: 216 Reward: -1689.1425335705444\n",
      "Total Timesteps: 217000 Episode Num: 217 Reward: -1694.673465369136\n",
      "Total Timesteps: 218000 Episode Num: 218 Reward: -1694.066716785249\n",
      "Total Timesteps: 219000 Episode Num: 219 Reward: -1695.4619511807186\n",
      "Total Timesteps: 220000 Episode Num: 220 Reward: -1681.717973557228\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.322477\n",
      "---------------------------------------\n",
      "Total Timesteps: 221000 Episode Num: 221 Reward: -1693.2225244715019\n",
      "Total Timesteps: 222000 Episode Num: 222 Reward: -1689.0759504478024\n",
      "Total Timesteps: 223000 Episode Num: 223 Reward: -1694.2546520982692\n",
      "Total Timesteps: 224000 Episode Num: 224 Reward: -1683.0948492945704\n",
      "Total Timesteps: 225000 Episode Num: 225 Reward: -1698.7453135395872\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.592089\n",
      "---------------------------------------\n",
      "Total Timesteps: 226000 Episode Num: 226 Reward: -1698.1572394090704\n",
      "Total Timesteps: 227000 Episode Num: 227 Reward: -1690.568373698444\n",
      "Total Timesteps: 228000 Episode Num: 228 Reward: -1690.584885305599\n",
      "Total Timesteps: 229000 Episode Num: 229 Reward: -1682.590029125031\n",
      "Total Timesteps: 230000 Episode Num: 230 Reward: -1688.2329197851172\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.053430\n",
      "---------------------------------------\n",
      "Total Timesteps: 231000 Episode Num: 231 Reward: -1693.976867029035\n",
      "Total Timesteps: 232000 Episode Num: 232 Reward: -1697.8748769929518\n",
      "Total Timesteps: 233000 Episode Num: 233 Reward: -1687.9262739811059\n",
      "Total Timesteps: 234000 Episode Num: 234 Reward: -1694.6352053269302\n",
      "Total Timesteps: 235000 Episode Num: 235 Reward: -1689.125945331065\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.698165\n",
      "---------------------------------------\n",
      "Total Timesteps: 236000 Episode Num: 236 Reward: -1691.454079233897\n",
      "Total Timesteps: 237000 Episode Num: 237 Reward: -1690.7698089529154\n",
      "Total Timesteps: 238000 Episode Num: 238 Reward: -1688.140334780816\n",
      "Total Timesteps: 239000 Episode Num: 239 Reward: -1694.6815196319649\n",
      "Total Timesteps: 240000 Episode Num: 240 Reward: -1697.265884029982\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.277963\n",
      "---------------------------------------\n",
      "Total Timesteps: 241000 Episode Num: 241 Reward: -1689.7802455979227\n",
      "Total Timesteps: 242000 Episode Num: 242 Reward: -1701.2125234184646\n",
      "Total Timesteps: 243000 Episode Num: 243 Reward: -1686.056721749195\n",
      "Total Timesteps: 244000 Episode Num: 244 Reward: -1689.4735352073033\n",
      "Total Timesteps: 245000 Episode Num: 245 Reward: -1699.0693566693074\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.469232\n",
      "---------------------------------------\n",
      "Total Timesteps: 246000 Episode Num: 246 Reward: -1691.7130726855223\n",
      "Total Timesteps: 247000 Episode Num: 247 Reward: -1691.6329909213002\n",
      "Total Timesteps: 248000 Episode Num: 248 Reward: -1699.871670425577\n",
      "Total Timesteps: 249000 Episode Num: 249 Reward: -1696.1021064589938\n",
      "Total Timesteps: 250000 Episode Num: 250 Reward: -1694.4877000899808\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.980703\n",
      "---------------------------------------\n",
      "Total Timesteps: 251000 Episode Num: 251 Reward: -1694.7039108885133\n",
      "Total Timesteps: 252000 Episode Num: 252 Reward: -1690.6232027444034\n",
      "Total Timesteps: 253000 Episode Num: 253 Reward: -1691.6259458573747\n",
      "Total Timesteps: 254000 Episode Num: 254 Reward: -1704.2287746514244\n",
      "Total Timesteps: 255000 Episode Num: 255 Reward: -1695.7898120423422\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.391389\n",
      "---------------------------------------\n",
      "Total Timesteps: 256000 Episode Num: 256 Reward: -1690.763680522111\n",
      "Total Timesteps: 257000 Episode Num: 257 Reward: -1694.7371305687207\n",
      "Total Timesteps: 258000 Episode Num: 258 Reward: -1690.825638288588\n",
      "Total Timesteps: 259000 Episode Num: 259 Reward: -1694.7383767365668\n",
      "Total Timesteps: 260000 Episode Num: 260 Reward: -1692.6944794950523\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.977681\n",
      "---------------------------------------\n",
      "Total Timesteps: 261000 Episode Num: 261 Reward: -1686.7284947617377\n",
      "Total Timesteps: 262000 Episode Num: 262 Reward: -1694.6137172515673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 263000 Episode Num: 263 Reward: -1691.0088988075038\n",
      "Total Timesteps: 264000 Episode Num: 264 Reward: -1696.3623230745186\n",
      "Total Timesteps: 265000 Episode Num: 265 Reward: -1686.9803292437362\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.440980\n",
      "---------------------------------------\n",
      "Total Timesteps: 266000 Episode Num: 266 Reward: -1694.6884669965143\n",
      "Total Timesteps: 267000 Episode Num: 267 Reward: -1698.4030392385102\n",
      "Total Timesteps: 268000 Episode Num: 268 Reward: -1687.7254264790406\n",
      "Total Timesteps: 269000 Episode Num: 269 Reward: -1695.2954517820804\n",
      "Total Timesteps: 270000 Episode Num: 270 Reward: -1699.037814414343\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.498753\n",
      "---------------------------------------\n",
      "Total Timesteps: 271000 Episode Num: 271 Reward: -1695.259784225226\n",
      "Total Timesteps: 272000 Episode Num: 272 Reward: -1693.8479072152415\n",
      "Total Timesteps: 273000 Episode Num: 273 Reward: -1697.7971331736976\n",
      "Total Timesteps: 274000 Episode Num: 274 Reward: -1691.466906592326\n",
      "Total Timesteps: 275000 Episode Num: 275 Reward: -1688.0777431327879\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.663201\n",
      "---------------------------------------\n",
      "Total Timesteps: 276000 Episode Num: 276 Reward: -1692.3017879126355\n",
      "Total Timesteps: 277000 Episode Num: 277 Reward: -1691.7116777673598\n",
      "Total Timesteps: 278000 Episode Num: 278 Reward: -1693.115456756322\n",
      "Total Timesteps: 279000 Episode Num: 279 Reward: -1688.1427141115569\n",
      "Total Timesteps: 280000 Episode Num: 280 Reward: -1689.8609272067768\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1698.709718\n",
      "---------------------------------------\n",
      "Total Timesteps: 281000 Episode Num: 281 Reward: -1693.6676699193306\n",
      "Total Timesteps: 282000 Episode Num: 282 Reward: -1685.8766369595264\n",
      "Total Timesteps: 283000 Episode Num: 283 Reward: -1677.0171887896715\n",
      "Total Timesteps: 284000 Episode Num: 284 Reward: -1693.0519321495115\n",
      "Total Timesteps: 285000 Episode Num: 285 Reward: -1699.9449280454965\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.053121\n",
      "---------------------------------------\n",
      "Total Timesteps: 286000 Episode Num: 286 Reward: -1691.3520116600703\n",
      "Total Timesteps: 287000 Episode Num: 287 Reward: -1695.3639332375453\n",
      "Total Timesteps: 288000 Episode Num: 288 Reward: -1694.9939215525617\n",
      "Total Timesteps: 289000 Episode Num: 289 Reward: -1699.0372788228601\n",
      "Total Timesteps: 290000 Episode Num: 290 Reward: -1689.2727447633322\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.580993\n",
      "---------------------------------------\n",
      "Total Timesteps: 291000 Episode Num: 291 Reward: -1693.342376496298\n",
      "Total Timesteps: 292000 Episode Num: 292 Reward: -1688.2575425292096\n",
      "Total Timesteps: 293000 Episode Num: 293 Reward: -1679.591684013504\n",
      "Total Timesteps: 294000 Episode Num: 294 Reward: -1688.4209456461217\n",
      "Total Timesteps: 295000 Episode Num: 295 Reward: -1696.2678463346974\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1699.607208\n",
      "---------------------------------------\n",
      "Total Timesteps: 296000 Episode Num: 296 Reward: -1686.957637581765\n",
      "Total Timesteps: 297000 Episode Num: 297 Reward: -1691.8769193149203\n",
      "Total Timesteps: 298000 Episode Num: 298 Reward: -1692.611100266683\n",
      "Total Timesteps: 299000 Episode Num: 299 Reward: -1692.6594099435392\n",
      "Total Timesteps: 300000 Episode Num: 300 Reward: -1690.7187457348805\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.507753\n",
      "---------------------------------------\n",
      "Total Timesteps: 301000 Episode Num: 301 Reward: -1688.2084237570896\n",
      "Total Timesteps: 302000 Episode Num: 302 Reward: -1690.124330672155\n",
      "Total Timesteps: 303000 Episode Num: 303 Reward: -1696.7058266622128\n",
      "Total Timesteps: 304000 Episode Num: 304 Reward: -1682.3206192044577\n",
      "Total Timesteps: 305000 Episode Num: 305 Reward: -1698.8135306468057\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.650990\n",
      "---------------------------------------\n",
      "Total Timesteps: 306000 Episode Num: 306 Reward: -1690.629017845869\n",
      "Total Timesteps: 307000 Episode Num: 307 Reward: -1694.6734804199243\n",
      "Total Timesteps: 308000 Episode Num: 308 Reward: -1690.3953812127424\n",
      "Total Timesteps: 309000 Episode Num: 309 Reward: -1689.6460432802842\n",
      "Total Timesteps: 310000 Episode Num: 310 Reward: -1692.5232472129815\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.192171\n",
      "---------------------------------------\n",
      "Total Timesteps: 311000 Episode Num: 311 Reward: -1691.8237647524488\n",
      "Total Timesteps: 312000 Episode Num: 312 Reward: -1688.599504280001\n",
      "Total Timesteps: 313000 Episode Num: 313 Reward: -1694.4225852283323\n",
      "Total Timesteps: 314000 Episode Num: 314 Reward: -1697.5676381943772\n",
      "Total Timesteps: 315000 Episode Num: 315 Reward: -1695.651225839111\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.623846\n",
      "---------------------------------------\n",
      "Total Timesteps: 316000 Episode Num: 316 Reward: -1684.2412618357712\n",
      "Total Timesteps: 317000 Episode Num: 317 Reward: -1694.575218169545\n",
      "Total Timesteps: 318000 Episode Num: 318 Reward: -1690.514071726168\n",
      "Total Timesteps: 319000 Episode Num: 319 Reward: -1683.2180572702346\n",
      "Total Timesteps: 320000 Episode Num: 320 Reward: -1693.573874787407\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.522492\n",
      "---------------------------------------\n",
      "Total Timesteps: 321000 Episode Num: 321 Reward: -1699.4609306796763\n",
      "Total Timesteps: 322000 Episode Num: 322 Reward: -1684.5545527926517\n",
      "Total Timesteps: 323000 Episode Num: 323 Reward: -1693.5982238939155\n",
      "Total Timesteps: 324000 Episode Num: 324 Reward: -1699.3336446317799\n",
      "Total Timesteps: 325000 Episode Num: 325 Reward: -1694.4415288042787\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.486078\n",
      "---------------------------------------\n",
      "Total Timesteps: 326000 Episode Num: 326 Reward: -1685.172556093389\n",
      "Total Timesteps: 327000 Episode Num: 327 Reward: -1695.700980366783\n",
      "Total Timesteps: 328000 Episode Num: 328 Reward: -1691.080671712492\n",
      "Total Timesteps: 329000 Episode Num: 329 Reward: -1691.1788125816267\n",
      "Total Timesteps: 330000 Episode Num: 330 Reward: -1691.2226026788958\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1705.280340\n",
      "---------------------------------------\n",
      "Total Timesteps: 331000 Episode Num: 331 Reward: -1691.910534821217\n",
      "Total Timesteps: 332000 Episode Num: 332 Reward: -1697.3461703146368\n",
      "Total Timesteps: 333000 Episode Num: 333 Reward: -1691.1363232192668\n",
      "Total Timesteps: 334000 Episode Num: 334 Reward: -1687.9537959647282\n",
      "Total Timesteps: 335000 Episode Num: 335 Reward: -1687.81652016338\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.801196\n",
      "---------------------------------------\n",
      "Total Timesteps: 336000 Episode Num: 336 Reward: -1697.4987684843313\n",
      "Total Timesteps: 337000 Episode Num: 337 Reward: -1694.940732135635\n",
      "Total Timesteps: 338000 Episode Num: 338 Reward: -1697.8584288675825\n",
      "Total Timesteps: 339000 Episode Num: 339 Reward: -1691.9978197079242\n",
      "Total Timesteps: 340000 Episode Num: 340 Reward: -1684.046364849295\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.636105\n",
      "---------------------------------------\n",
      "Total Timesteps: 341000 Episode Num: 341 Reward: -1696.9185936909682\n",
      "Total Timesteps: 342000 Episode Num: 342 Reward: -1688.9511720944104\n",
      "Total Timesteps: 343000 Episode Num: 343 Reward: -1690.2406501170904\n",
      "Total Timesteps: 344000 Episode Num: 344 Reward: -1691.8989591797247\n",
      "Total Timesteps: 345000 Episode Num: 345 Reward: -1687.0729465049549\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.771877\n",
      "---------------------------------------\n",
      "Total Timesteps: 346000 Episode Num: 346 Reward: -1695.4946996479732\n",
      "Total Timesteps: 347000 Episode Num: 347 Reward: -1696.287635939412\n",
      "Total Timesteps: 348000 Episode Num: 348 Reward: -1698.3435361403353\n",
      "Total Timesteps: 349000 Episode Num: 349 Reward: -1698.2339767462843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 350000 Episode Num: 350 Reward: -1703.4709229140665\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.302511\n",
      "---------------------------------------\n",
      "Total Timesteps: 351000 Episode Num: 351 Reward: -1688.6134462450239\n",
      "Total Timesteps: 352000 Episode Num: 352 Reward: -1693.4503595288097\n",
      "Total Timesteps: 353000 Episode Num: 353 Reward: -1694.5640702451044\n",
      "Total Timesteps: 354000 Episode Num: 354 Reward: -1677.2998984111014\n",
      "Total Timesteps: 355000 Episode Num: 355 Reward: -1689.42625138153\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.347773\n",
      "---------------------------------------\n",
      "Total Timesteps: 356000 Episode Num: 356 Reward: -1692.4020164058602\n",
      "Total Timesteps: 357000 Episode Num: 357 Reward: -1687.3547354353293\n",
      "Total Timesteps: 358000 Episode Num: 358 Reward: -1700.860445801095\n",
      "Total Timesteps: 359000 Episode Num: 359 Reward: -1698.8282066090496\n",
      "Total Timesteps: 360000 Episode Num: 360 Reward: -1702.592123445503\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1699.218487\n",
      "---------------------------------------\n",
      "Total Timesteps: 361000 Episode Num: 361 Reward: -1688.066880866035\n",
      "Total Timesteps: 362000 Episode Num: 362 Reward: -1694.9921452967071\n",
      "Total Timesteps: 363000 Episode Num: 363 Reward: -1691.956707784728\n",
      "Total Timesteps: 364000 Episode Num: 364 Reward: -1691.7118817195278\n",
      "Total Timesteps: 365000 Episode Num: 365 Reward: -1695.3263644788149\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.772171\n",
      "---------------------------------------\n",
      "Total Timesteps: 366000 Episode Num: 366 Reward: -1695.7708489013328\n",
      "Total Timesteps: 367000 Episode Num: 367 Reward: -1693.1456623546544\n",
      "Total Timesteps: 368000 Episode Num: 368 Reward: -1692.6832862545307\n",
      "Total Timesteps: 369000 Episode Num: 369 Reward: -1692.6027951201074\n",
      "Total Timesteps: 370000 Episode Num: 370 Reward: -1696.130713389661\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.166418\n",
      "---------------------------------------\n",
      "Total Timesteps: 371000 Episode Num: 371 Reward: -1690.8217886136065\n",
      "Total Timesteps: 372000 Episode Num: 372 Reward: -1694.8328617003303\n",
      "Total Timesteps: 373000 Episode Num: 373 Reward: -1693.9487638385422\n",
      "Total Timesteps: 374000 Episode Num: 374 Reward: -1694.8690126402557\n",
      "Total Timesteps: 375000 Episode Num: 375 Reward: -1695.3354194459503\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.061578\n",
      "---------------------------------------\n",
      "Total Timesteps: 376000 Episode Num: 376 Reward: -1692.8422638439058\n",
      "Total Timesteps: 377000 Episode Num: 377 Reward: -1688.3947188479833\n",
      "Total Timesteps: 378000 Episode Num: 378 Reward: -1689.5775553941046\n",
      "Total Timesteps: 379000 Episode Num: 379 Reward: -1679.6607115061195\n",
      "Total Timesteps: 380000 Episode Num: 380 Reward: -1690.354513036077\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1699.175221\n",
      "---------------------------------------\n",
      "Total Timesteps: 381000 Episode Num: 381 Reward: -1690.533927058499\n",
      "Total Timesteps: 382000 Episode Num: 382 Reward: -1687.4555715808056\n",
      "Total Timesteps: 383000 Episode Num: 383 Reward: -1686.6227970349064\n",
      "Total Timesteps: 384000 Episode Num: 384 Reward: -1695.0541154112864\n",
      "Total Timesteps: 385000 Episode Num: 385 Reward: -1694.4493950394851\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1699.784546\n",
      "---------------------------------------\n",
      "Total Timesteps: 386000 Episode Num: 386 Reward: -1691.6924732421105\n",
      "Total Timesteps: 387000 Episode Num: 387 Reward: -1689.8548184082906\n",
      "Total Timesteps: 388000 Episode Num: 388 Reward: -1690.8919651597805\n",
      "Total Timesteps: 389000 Episode Num: 389 Reward: -1691.8031914062049\n",
      "Total Timesteps: 390000 Episode Num: 390 Reward: -1694.5919591069262\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.036639\n",
      "---------------------------------------\n",
      "Total Timesteps: 391000 Episode Num: 391 Reward: -1692.7177037750548\n",
      "Total Timesteps: 392000 Episode Num: 392 Reward: -1690.4338576402638\n",
      "Total Timesteps: 393000 Episode Num: 393 Reward: -1686.8985440510671\n",
      "Total Timesteps: 394000 Episode Num: 394 Reward: -1696.4585007980484\n",
      "Total Timesteps: 395000 Episode Num: 395 Reward: -1689.4383249588539\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.557078\n",
      "---------------------------------------\n",
      "Total Timesteps: 396000 Episode Num: 396 Reward: -1696.423863815696\n",
      "Total Timesteps: 397000 Episode Num: 397 Reward: -1697.3781174238252\n",
      "Total Timesteps: 398000 Episode Num: 398 Reward: -1693.270652605837\n",
      "Total Timesteps: 399000 Episode Num: 399 Reward: -1691.9517638852587\n",
      "Total Timesteps: 400000 Episode Num: 400 Reward: -1697.9100097726646\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.653838\n",
      "---------------------------------------\n",
      "Total Timesteps: 401000 Episode Num: 401 Reward: -1693.0476091820872\n",
      "Total Timesteps: 402000 Episode Num: 402 Reward: -1688.9963817121588\n",
      "Total Timesteps: 403000 Episode Num: 403 Reward: -1683.1767931259028\n",
      "Total Timesteps: 404000 Episode Num: 404 Reward: -1697.9674400696563\n",
      "Total Timesteps: 405000 Episode Num: 405 Reward: -1702.0557515370397\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1699.993790\n",
      "---------------------------------------\n",
      "Total Timesteps: 406000 Episode Num: 406 Reward: -1687.9701264462371\n",
      "Total Timesteps: 407000 Episode Num: 407 Reward: -1694.8262463147762\n",
      "Total Timesteps: 408000 Episode Num: 408 Reward: -1690.2105038112995\n",
      "Total Timesteps: 409000 Episode Num: 409 Reward: -1688.129442914617\n",
      "Total Timesteps: 410000 Episode Num: 410 Reward: -1692.8656677918048\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.043768\n",
      "---------------------------------------\n",
      "Total Timesteps: 411000 Episode Num: 411 Reward: -1697.385479120707\n",
      "Total Timesteps: 412000 Episode Num: 412 Reward: -1689.0287159974891\n",
      "Total Timesteps: 413000 Episode Num: 413 Reward: -1690.7925094560812\n",
      "Total Timesteps: 414000 Episode Num: 414 Reward: -1693.5344382130029\n",
      "Total Timesteps: 415000 Episode Num: 415 Reward: -1691.7033628042034\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.205822\n",
      "---------------------------------------\n",
      "Total Timesteps: 416000 Episode Num: 416 Reward: -1693.5169420175275\n",
      "Total Timesteps: 417000 Episode Num: 417 Reward: -1692.2751422829983\n",
      "Total Timesteps: 418000 Episode Num: 418 Reward: -1693.7597759012692\n",
      "Total Timesteps: 419000 Episode Num: 419 Reward: -1691.4029336876372\n",
      "Total Timesteps: 420000 Episode Num: 420 Reward: -1684.2378937238605\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.922814\n",
      "---------------------------------------\n",
      "Total Timesteps: 421000 Episode Num: 421 Reward: -1693.3230781954373\n",
      "Total Timesteps: 422000 Episode Num: 422 Reward: -1696.3245129332142\n",
      "Total Timesteps: 423000 Episode Num: 423 Reward: -1694.9575836804\n",
      "Total Timesteps: 424000 Episode Num: 424 Reward: -1696.4436650080822\n",
      "Total Timesteps: 425000 Episode Num: 425 Reward: -1699.730256640756\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.083646\n",
      "---------------------------------------\n",
      "Total Timesteps: 426000 Episode Num: 426 Reward: -1691.335108147515\n",
      "Total Timesteps: 427000 Episode Num: 427 Reward: -1692.5879905420559\n",
      "Total Timesteps: 428000 Episode Num: 428 Reward: -1688.0054126836676\n",
      "Total Timesteps: 429000 Episode Num: 429 Reward: -1690.5271132511318\n",
      "Total Timesteps: 430000 Episode Num: 430 Reward: -1695.8617035661848\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.681483\n",
      "---------------------------------------\n",
      "Total Timesteps: 431000 Episode Num: 431 Reward: -1694.807294845561\n",
      "Total Timesteps: 432000 Episode Num: 432 Reward: -1694.8588949352354\n",
      "Total Timesteps: 433000 Episode Num: 433 Reward: -1689.7489585437254\n",
      "Total Timesteps: 434000 Episode Num: 434 Reward: -1693.7889589908289\n",
      "Total Timesteps: 435000 Episode Num: 435 Reward: -1700.9527013437857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.005868\n",
      "---------------------------------------\n",
      "Total Timesteps: 436000 Episode Num: 436 Reward: -1691.028156522995\n",
      "Total Timesteps: 437000 Episode Num: 437 Reward: -1693.7461457002003\n",
      "Total Timesteps: 438000 Episode Num: 438 Reward: -1697.0271655672084\n",
      "Total Timesteps: 439000 Episode Num: 439 Reward: -1696.8774719598214\n",
      "Total Timesteps: 440000 Episode Num: 440 Reward: -1695.972257211674\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.805179\n",
      "---------------------------------------\n",
      "Total Timesteps: 441000 Episode Num: 441 Reward: -1698.4270044624022\n",
      "Total Timesteps: 442000 Episode Num: 442 Reward: -1689.1388556633294\n",
      "Total Timesteps: 443000 Episode Num: 443 Reward: -1686.4399525510125\n",
      "Total Timesteps: 444000 Episode Num: 444 Reward: -1695.3782968712117\n",
      "Total Timesteps: 445000 Episode Num: 445 Reward: -1691.513858561412\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.709683\n",
      "---------------------------------------\n",
      "Total Timesteps: 446000 Episode Num: 446 Reward: -1698.7386854015215\n",
      "Total Timesteps: 447000 Episode Num: 447 Reward: -1701.595671610931\n",
      "Total Timesteps: 448000 Episode Num: 448 Reward: -1691.5083785114584\n",
      "Total Timesteps: 449000 Episode Num: 449 Reward: -1694.102771208449\n",
      "Total Timesteps: 450000 Episode Num: 450 Reward: -1680.8975296432218\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.094057\n",
      "---------------------------------------\n",
      "Total Timesteps: 451000 Episode Num: 451 Reward: -1699.2120937640814\n",
      "Total Timesteps: 452000 Episode Num: 452 Reward: -1691.508039018749\n",
      "Total Timesteps: 453000 Episode Num: 453 Reward: -1697.3510542456718\n",
      "Total Timesteps: 454000 Episode Num: 454 Reward: -1697.3310423646983\n",
      "Total Timesteps: 455000 Episode Num: 455 Reward: -1693.4257544965253\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.034083\n",
      "---------------------------------------\n",
      "Total Timesteps: 456000 Episode Num: 456 Reward: -1690.6588961838634\n",
      "Total Timesteps: 457000 Episode Num: 457 Reward: -1693.4947645036148\n",
      "Total Timesteps: 458000 Episode Num: 458 Reward: -1693.056861934545\n",
      "Total Timesteps: 459000 Episode Num: 459 Reward: -1699.6425173288767\n",
      "Total Timesteps: 460000 Episode Num: 460 Reward: -1697.4709677884634\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1700.111321\n",
      "---------------------------------------\n",
      "Total Timesteps: 461000 Episode Num: 461 Reward: -1684.1085238683017\n",
      "Total Timesteps: 462000 Episode Num: 462 Reward: -1701.227063119334\n",
      "Total Timesteps: 463000 Episode Num: 463 Reward: -1687.059097204777\n",
      "Total Timesteps: 464000 Episode Num: 464 Reward: -1694.8086641776858\n",
      "Total Timesteps: 465000 Episode Num: 465 Reward: -1691.4989466689367\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.117108\n",
      "---------------------------------------\n",
      "Total Timesteps: 466000 Episode Num: 466 Reward: -1693.4859541343899\n",
      "Total Timesteps: 467000 Episode Num: 467 Reward: -1689.9317038948477\n",
      "Total Timesteps: 468000 Episode Num: 468 Reward: -1686.0914776317436\n",
      "Total Timesteps: 469000 Episode Num: 469 Reward: -1691.6361092752188\n",
      "Total Timesteps: 470000 Episode Num: 470 Reward: -1695.769538657773\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1704.116624\n",
      "---------------------------------------\n",
      "Total Timesteps: 471000 Episode Num: 471 Reward: -1691.7994195663052\n",
      "Total Timesteps: 472000 Episode Num: 472 Reward: -1697.552446329988\n",
      "Total Timesteps: 473000 Episode Num: 473 Reward: -1694.434234970126\n",
      "Total Timesteps: 474000 Episode Num: 474 Reward: -1692.0671726034254\n",
      "Total Timesteps: 475000 Episode Num: 475 Reward: -1688.5337885056683\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1703.007817\n",
      "---------------------------------------\n",
      "Total Timesteps: 476000 Episode Num: 476 Reward: -1694.1663927975148\n",
      "Total Timesteps: 477000 Episode Num: 477 Reward: -1686.4526552167847\n",
      "Total Timesteps: 478000 Episode Num: 478 Reward: -1693.3142505180278\n",
      "Total Timesteps: 479000 Episode Num: 479 Reward: -1696.3208854202394\n",
      "Total Timesteps: 480000 Episode Num: 480 Reward: -1693.4924842279368\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.279969\n",
      "---------------------------------------\n",
      "Total Timesteps: 481000 Episode Num: 481 Reward: -1692.5314331522256\n",
      "Total Timesteps: 482000 Episode Num: 482 Reward: -1690.8483963165822\n",
      "Total Timesteps: 483000 Episode Num: 483 Reward: -1702.4346390030555\n",
      "Total Timesteps: 484000 Episode Num: 484 Reward: -1692.0543969883165\n",
      "Total Timesteps: 485000 Episode Num: 485 Reward: -1698.562872133893\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.639339\n",
      "---------------------------------------\n",
      "Total Timesteps: 486000 Episode Num: 486 Reward: -1692.2387174101912\n",
      "Total Timesteps: 487000 Episode Num: 487 Reward: -1691.1490843971662\n",
      "Total Timesteps: 488000 Episode Num: 488 Reward: -1692.1274370039434\n",
      "Total Timesteps: 489000 Episode Num: 489 Reward: -1702.222099457515\n",
      "Total Timesteps: 490000 Episode Num: 490 Reward: -1695.9369721142082\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.564745\n",
      "---------------------------------------\n",
      "Total Timesteps: 491000 Episode Num: 491 Reward: -1695.4532749345426\n",
      "Total Timesteps: 492000 Episode Num: 492 Reward: -1691.995685711927\n",
      "Total Timesteps: 493000 Episode Num: 493 Reward: -1688.0093381286606\n",
      "Total Timesteps: 494000 Episode Num: 494 Reward: -1691.5058169471793\n",
      "Total Timesteps: 495000 Episode Num: 495 Reward: -1689.7507510814391\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1701.184124\n",
      "---------------------------------------\n",
      "Total Timesteps: 496000 Episode Num: 496 Reward: -1695.463544754466\n",
      "Total Timesteps: 497000 Episode Num: 497 Reward: -1692.3277262421639\n",
      "Total Timesteps: 498000 Episode Num: 498 Reward: -1694.6079876660947\n",
      "Total Timesteps: 499000 Episode Num: 499 Reward: -1693.1256540979405\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1702.552715\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-0e2ac853341a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msave_env_vid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/pytorch/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/pytorch/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/pytorch/lib/python3.6/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         )\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/pytorch/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/pytorch/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_frames_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtualenvs/pytorch/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
    "    return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = gym.make(env_name)\n",
    "max_episode_steps = env._max_episode_steps\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, './pytorch_models/')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
