{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXu1r8qvSzWf"
   },
   "source": [
    "# Twin-Delayed DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRzQUhuUTc0J"
   },
   "source": [
    "## Installing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAHMB0Ze8fU0"
   },
   "outputs": [],
   "source": [
    "!pip install pybullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xjm2onHdT-Av"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ikr2p0Js8iB4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pybullet_envs\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2nGdtlKVydr"
   },
   "source": [
    "## Step 1: We initialize the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5rW0IDB8nTO"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "  def __init__(self, max_size=1e6):\n",
    "    self.storage = []\n",
    "    self.max_size = max_size\n",
    "    self.ptr = 0\n",
    "\n",
    "  def add(self, transition):\n",
    "    if len(self.storage) == self.max_size:\n",
    "      self.storage[int(self.ptr)] = transition\n",
    "      self.ptr = (self.ptr + 1) % self.max_size\n",
    "    else:\n",
    "      self.storage.append(transition)\n",
    "\n",
    "  def sample(self, batch_size):\n",
    "    ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
    "    for i in ind: \n",
    "      state, next_state, action, reward, done = self.storage[i]\n",
    "      batch_states.append(np.array(state, copy=False))\n",
    "      batch_next_states.append(np.array(next_state, copy=False))\n",
    "      batch_actions.append(np.array(action, copy=False))\n",
    "      batch_rewards.append(np.array(reward, copy=False))\n",
    "      batch_dones.append(np.array(done, copy=False))\n",
    "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb7TTaHxWbQD"
   },
   "source": [
    "## Step 2: We build one neural network for the Actor model and one neural network for the Actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4CeRW4D79HL0"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRDDce8FXef7"
   },
   "source": [
    "## Step 3: We build two neural networks for the two Critic models and two neural networks for the two Critic targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCee7gwR9Jrs"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzIDuONodenW"
   },
   "source": [
    "## Steps 4 to 15: Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzd0H1xukdKe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankdesh/virtualenvs/pytorch/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka-ZRtQvjBex"
   },
   "source": [
    "## We make a function that evaluates the policy by calculating its average reward over 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qabqiYdp9wDM"
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGuKmH_ijf7U"
   },
   "source": [
    "## We set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFj6wbAo97lk"
   },
   "outputs": [],
   "source": [
    "env_name = \"HalfCheetahBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
    "seed = 0 # Random seed number\n",
    "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
    "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
    "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
    "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100 # Size of the batch\n",
    "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005 # Target network update rate\n",
    "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjwf2HCol3XP"
   },
   "source": [
    "## We create a file name for the two saved models: the Actor and Critic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fyH8N5z-o3o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: TD3_HalfCheetahBulletEnv-v0_0\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kop-C96Aml8O"
   },
   "source": [
    "## We create a folder inside which will be saved the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Src07lvY-zXb"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./results\"):\n",
    "  os.makedirs(\"./results\")\n",
    "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
    "  os.makedirs(\"./pytorch_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qEAzOd47mv1Z"
   },
   "source": [
    "## We create the PyBullet environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyQXJUIs-6BV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankdesh/virtualenvs/pytorch/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YdPG4HXnNsh"
   },
   "source": [
    "## We set seeds and we get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3RufYec_ADj"
   },
   "outputs": [],
   "source": [
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HWEgDAQxnbem"
   },
   "source": [
    "## We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTVvG7F8_EWg"
   },
   "outputs": [],
   "source": [
    "policy = TD3(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZI60VN2Unklh"
   },
   "source": [
    "## We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd-ZsdXR_LgV"
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYOpCyiDnw7s"
   },
   "source": [
    "## We define a list where all the evaluation results over 10 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhC_5XJ__Orp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1429.426163\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluations = [evaluate_policy(policy)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm-4b3p6rglE"
   },
   "source": [
    "## We create a new folder directory in which the final results (videos of the agent) will be populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTL9uMd0ru03"
   },
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp', 'brs')\n",
    "monitor_dir = mkdir(work_dir, 'monitor')\n",
    "max_episode_steps = env._max_episode_steps\n",
    "save_env_vid = False\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "31n5eb03p-Fm"
   },
   "source": [
    "## We initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vN5EvxK_QhT"
   },
   "outputs": [],
   "source": [
    "total_timesteps = 0\n",
    "timesteps_since_eval = 0\n",
    "episode_num = 0\n",
    "done = True\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9gsjvtPqLgT"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_ouY4NH_Y0I",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 1000 Episode Num: 1 Reward: -1323.5197449907573\n",
      "Total Timesteps: 2000 Episode Num: 2 Reward: -1427.9804445205154\n",
      "Total Timesteps: 3000 Episode Num: 3 Reward: -1404.7537342933217\n",
      "Total Timesteps: 4000 Episode Num: 4 Reward: -1067.1236530299375\n",
      "Total Timesteps: 5000 Episode Num: 5 Reward: -1251.4273477930474\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1440.695314\n",
      "---------------------------------------\n",
      "Total Timesteps: 6000 Episode Num: 6 Reward: -1381.0018956901756\n",
      "Total Timesteps: 7000 Episode Num: 7 Reward: -1301.4481694333342\n",
      "Total Timesteps: 8000 Episode Num: 8 Reward: -1429.5479305628003\n",
      "Total Timesteps: 9000 Episode Num: 9 Reward: -1363.989706196867\n",
      "Total Timesteps: 10000 Episode Num: 10 Reward: -1073.7882969858542\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1397.793367\n",
      "---------------------------------------\n",
      "Total Timesteps: 11000 Episode Num: 11 Reward: -1546.7858365796972\n",
      "Total Timesteps: 12000 Episode Num: 12 Reward: 317.5928955588025\n",
      "Total Timesteps: 13000 Episode Num: 13 Reward: 267.7092502584625\n",
      "Total Timesteps: 14000 Episode Num: 14 Reward: 384.7445150820895\n",
      "Total Timesteps: 15000 Episode Num: 15 Reward: 383.64289008774546\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 252.969393\n",
      "---------------------------------------\n",
      "Total Timesteps: 16000 Episode Num: 16 Reward: 312.42410581984575\n",
      "Total Timesteps: 17000 Episode Num: 17 Reward: 280.3240734087302\n",
      "Total Timesteps: 18000 Episode Num: 18 Reward: -211.67052243658122\n",
      "Total Timesteps: 19000 Episode Num: 19 Reward: -1458.976965142141\n",
      "Total Timesteps: 20000 Episode Num: 20 Reward: -1456.8638855052495\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1348.878673\n",
      "---------------------------------------\n",
      "Total Timesteps: 21000 Episode Num: 21 Reward: -1519.8006323054913\n",
      "Total Timesteps: 22000 Episode Num: 22 Reward: -357.086313490924\n",
      "Total Timesteps: 23000 Episode Num: 23 Reward: -1544.0391463339588\n",
      "Total Timesteps: 24000 Episode Num: 24 Reward: 30.59882431540116\n",
      "Total Timesteps: 25000 Episode Num: 25 Reward: -252.50674577744707\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -175.042915\n",
      "---------------------------------------\n",
      "Total Timesteps: 26000 Episode Num: 26 Reward: 404.2122865949222\n",
      "Total Timesteps: 27000 Episode Num: 27 Reward: -487.2086416043271\n",
      "Total Timesteps: 28000 Episode Num: 28 Reward: -726.8647052856259\n",
      "Total Timesteps: 29000 Episode Num: 29 Reward: -383.4039910741158\n",
      "Total Timesteps: 30000 Episode Num: 30 Reward: -1543.938464275442\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1628.783153\n",
      "---------------------------------------\n",
      "Total Timesteps: 31000 Episode Num: 31 Reward: -1563.2237878469452\n",
      "Total Timesteps: 32000 Episode Num: 32 Reward: -1554.4223724830374\n",
      "Total Timesteps: 33000 Episode Num: 33 Reward: -1673.2514303810026\n",
      "Total Timesteps: 34000 Episode Num: 34 Reward: -1330.4711982921528\n",
      "Total Timesteps: 35000 Episode Num: 35 Reward: -1548.5358780902877\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -902.028249\n",
      "---------------------------------------\n",
      "Total Timesteps: 36000 Episode Num: 36 Reward: -545.0872542055756\n",
      "Total Timesteps: 37000 Episode Num: 37 Reward: -1380.5962914731206\n",
      "Total Timesteps: 38000 Episode Num: 38 Reward: -1466.3918242131751\n",
      "Total Timesteps: 39000 Episode Num: 39 Reward: -1117.5282027398775\n",
      "Total Timesteps: 40000 Episode Num: 40 Reward: -1445.0264829707767\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1413.809785\n",
      "---------------------------------------\n",
      "Total Timesteps: 41000 Episode Num: 41 Reward: -1501.6454155305942\n",
      "Total Timesteps: 42000 Episode Num: 42 Reward: -1111.7973900739178\n",
      "Total Timesteps: 43000 Episode Num: 43 Reward: -144.86467128703057\n",
      "Total Timesteps: 44000 Episode Num: 44 Reward: -44.067695701056834\n",
      "Total Timesteps: 45000 Episode Num: 45 Reward: 32.44024806314678\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -54.892687\n",
      "---------------------------------------\n",
      "Total Timesteps: 46000 Episode Num: 46 Reward: 211.67305130836976\n",
      "Total Timesteps: 47000 Episode Num: 47 Reward: -1008.0948585138427\n",
      "Total Timesteps: 48000 Episode Num: 48 Reward: -581.6878066191712\n",
      "Total Timesteps: 49000 Episode Num: 49 Reward: 275.32991741137664\n",
      "Total Timesteps: 50000 Episode Num: 50 Reward: -38.858997785632404\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 187.120179\n",
      "---------------------------------------\n",
      "Total Timesteps: 51000 Episode Num: 51 Reward: 507.136654062938\n",
      "Total Timesteps: 52000 Episode Num: 52 Reward: 434.66281069208117\n",
      "Total Timesteps: 53000 Episode Num: 53 Reward: 204.38734792911293\n",
      "Total Timesteps: 54000 Episode Num: 54 Reward: -30.965284611692123\n",
      "Total Timesteps: 55000 Episode Num: 55 Reward: 537.9391478079403\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -541.265913\n",
      "---------------------------------------\n",
      "Total Timesteps: 56000 Episode Num: 56 Reward: -1065.3011099154596\n",
      "Total Timesteps: 57000 Episode Num: 57 Reward: -228.16764351010323\n",
      "Total Timesteps: 58000 Episode Num: 58 Reward: 404.4533339089154\n",
      "Total Timesteps: 59000 Episode Num: 59 Reward: -612.8935642854718\n",
      "Total Timesteps: 60000 Episode Num: 60 Reward: -166.63297784069334\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -146.191852\n",
      "---------------------------------------\n",
      "Total Timesteps: 61000 Episode Num: 61 Reward: -1020.5036403594089\n",
      "Total Timesteps: 62000 Episode Num: 62 Reward: 125.99973803947732\n",
      "Total Timesteps: 63000 Episode Num: 63 Reward: 635.2473293251632\n",
      "Total Timesteps: 64000 Episode Num: 64 Reward: 569.8843779144476\n",
      "Total Timesteps: 65000 Episode Num: 65 Reward: 574.007592313599\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 490.475065\n",
      "---------------------------------------\n",
      "Total Timesteps: 66000 Episode Num: 66 Reward: 519.6693538293681\n",
      "Total Timesteps: 67000 Episode Num: 67 Reward: 506.3503040449072\n",
      "Total Timesteps: 68000 Episode Num: 68 Reward: 671.94369507212\n",
      "Total Timesteps: 69000 Episode Num: 69 Reward: 668.2670230519208\n",
      "Total Timesteps: 70000 Episode Num: 70 Reward: 473.1639427652286\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 691.283635\n",
      "---------------------------------------\n",
      "Total Timesteps: 71000 Episode Num: 71 Reward: 744.0011298965377\n",
      "Total Timesteps: 72000 Episode Num: 72 Reward: 724.002702430283\n",
      "Total Timesteps: 73000 Episode Num: 73 Reward: 551.6593712027442\n",
      "Total Timesteps: 74000 Episode Num: 74 Reward: 472.1874818713521\n",
      "Total Timesteps: 75000 Episode Num: 75 Reward: 960.1311895573859\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 787.787914\n",
      "---------------------------------------\n",
      "Total Timesteps: 76000 Episode Num: 76 Reward: 652.2464381245146\n",
      "Total Timesteps: 77000 Episode Num: 77 Reward: 471.8682448731196\n",
      "Total Timesteps: 78000 Episode Num: 78 Reward: 701.7705198447796\n",
      "Total Timesteps: 79000 Episode Num: 79 Reward: 799.6273057459957\n",
      "Total Timesteps: 80000 Episode Num: 80 Reward: 709.3626680784687\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 359.510071\n",
      "---------------------------------------\n",
      "Total Timesteps: 81000 Episode Num: 81 Reward: 697.2702623673335\n",
      "Total Timesteps: 82000 Episode Num: 82 Reward: 554.9118442645234\n",
      "Total Timesteps: 83000 Episode Num: 83 Reward: 678.8121221607142\n",
      "Total Timesteps: 84000 Episode Num: 84 Reward: -161.668110632518\n",
      "Total Timesteps: 85000 Episode Num: 85 Reward: 772.8953257282875\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 718.094167\n",
      "---------------------------------------\n",
      "Total Timesteps: 86000 Episode Num: 86 Reward: 669.1121654220784\n",
      "Total Timesteps: 87000 Episode Num: 87 Reward: 707.1184272329634\n",
      "Total Timesteps: 88000 Episode Num: 88 Reward: 593.8160242464833\n",
      "Total Timesteps: 89000 Episode Num: 89 Reward: 696.0571107907446\n",
      "Total Timesteps: 90000 Episode Num: 90 Reward: 742.9280497118024\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 775.605096\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 91000 Episode Num: 91 Reward: 782.4105070337974\n",
      "Total Timesteps: 92000 Episode Num: 92 Reward: 811.3637601232394\n",
      "Total Timesteps: 93000 Episode Num: 93 Reward: 938.986298601181\n",
      "Total Timesteps: 94000 Episode Num: 94 Reward: 645.430869774569\n",
      "Total Timesteps: 95000 Episode Num: 95 Reward: 852.9116037533927\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 664.625244\n",
      "---------------------------------------\n",
      "Total Timesteps: 96000 Episode Num: 96 Reward: 654.1786113537732\n",
      "Total Timesteps: 97000 Episode Num: 97 Reward: 848.1794202891269\n",
      "Total Timesteps: 98000 Episode Num: 98 Reward: 491.3705877940055\n",
      "Total Timesteps: 99000 Episode Num: 99 Reward: 632.5527650727192\n",
      "Total Timesteps: 100000 Episode Num: 100 Reward: 722.9378161754505\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 748.505267\n",
      "---------------------------------------\n",
      "Total Timesteps: 101000 Episode Num: 101 Reward: 766.2138203317099\n",
      "Total Timesteps: 102000 Episode Num: 102 Reward: 695.3835538476976\n",
      "Total Timesteps: 103000 Episode Num: 103 Reward: -35.347640217051676\n",
      "Total Timesteps: 104000 Episode Num: 104 Reward: 640.0445790104543\n",
      "Total Timesteps: 105000 Episode Num: 105 Reward: 750.7280695938075\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 846.554920\n",
      "---------------------------------------\n",
      "Total Timesteps: 106000 Episode Num: 106 Reward: 1017.4175922981906\n",
      "Total Timesteps: 107000 Episode Num: 107 Reward: 816.1183475022781\n",
      "Total Timesteps: 108000 Episode Num: 108 Reward: 934.0218139108532\n",
      "Total Timesteps: 109000 Episode Num: 109 Reward: 945.2635456190882\n",
      "Total Timesteps: 110000 Episode Num: 110 Reward: 1075.6299337441833\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 950.222630\n",
      "---------------------------------------\n",
      "Total Timesteps: 111000 Episode Num: 111 Reward: 981.0391965754823\n",
      "Total Timesteps: 112000 Episode Num: 112 Reward: 936.3078447728196\n",
      "Total Timesteps: 113000 Episode Num: 113 Reward: 1135.4704072786274\n",
      "Total Timesteps: 114000 Episode Num: 114 Reward: 925.6518336862202\n",
      "Total Timesteps: 115000 Episode Num: 115 Reward: 1066.2415187420722\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 825.825188\n",
      "---------------------------------------\n",
      "Total Timesteps: 116000 Episode Num: 116 Reward: 1060.0946599099375\n",
      "Total Timesteps: 117000 Episode Num: 117 Reward: 1019.1221769478985\n",
      "Total Timesteps: 118000 Episode Num: 118 Reward: 1117.797747974622\n",
      "Total Timesteps: 119000 Episode Num: 119 Reward: 836.5865266078572\n",
      "Total Timesteps: 120000 Episode Num: 120 Reward: 1114.401539450026\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 925.763140\n",
      "---------------------------------------\n",
      "Total Timesteps: 121000 Episode Num: 121 Reward: 947.9025436616583\n",
      "Total Timesteps: 122000 Episode Num: 122 Reward: 1107.9026932743307\n",
      "Total Timesteps: 123000 Episode Num: 123 Reward: 776.8428011407964\n",
      "Total Timesteps: 124000 Episode Num: 124 Reward: 1069.4234294067503\n",
      "Total Timesteps: 125000 Episode Num: 125 Reward: 1194.9764922118918\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1059.539659\n",
      "---------------------------------------\n",
      "Total Timesteps: 126000 Episode Num: 126 Reward: 1083.491395182293\n",
      "Total Timesteps: 127000 Episode Num: 127 Reward: 1346.8076420461844\n",
      "Total Timesteps: 128000 Episode Num: 128 Reward: 1174.5328623943135\n",
      "Total Timesteps: 129000 Episode Num: 129 Reward: 1149.1944079546088\n",
      "Total Timesteps: 130000 Episode Num: 130 Reward: 1093.7972116407561\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1286.358226\n",
      "---------------------------------------\n",
      "Total Timesteps: 131000 Episode Num: 131 Reward: 1089.3723842135857\n",
      "Total Timesteps: 132000 Episode Num: 132 Reward: 1085.256169967866\n",
      "Total Timesteps: 133000 Episode Num: 133 Reward: 1408.0268760515744\n",
      "Total Timesteps: 134000 Episode Num: 134 Reward: 1335.583546998877\n",
      "Total Timesteps: 135000 Episode Num: 135 Reward: 1089.9473118708825\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1206.063928\n",
      "---------------------------------------\n",
      "Total Timesteps: 136000 Episode Num: 136 Reward: 1149.6366877498033\n",
      "Total Timesteps: 137000 Episode Num: 137 Reward: 1131.1449715002884\n",
      "Total Timesteps: 138000 Episode Num: 138 Reward: 1113.9675475067152\n",
      "Total Timesteps: 139000 Episode Num: 139 Reward: 973.487766897719\n",
      "Total Timesteps: 140000 Episode Num: 140 Reward: 1049.5052555000686\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1081.440357\n",
      "---------------------------------------\n",
      "Total Timesteps: 141000 Episode Num: 141 Reward: 1046.9219712950576\n",
      "Total Timesteps: 142000 Episode Num: 142 Reward: 988.8044861379457\n",
      "Total Timesteps: 143000 Episode Num: 143 Reward: 1200.5401162544933\n",
      "Total Timesteps: 144000 Episode Num: 144 Reward: 1089.6684827900324\n",
      "Total Timesteps: 145000 Episode Num: 145 Reward: 1201.9203813549898\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1220.701231\n",
      "---------------------------------------\n",
      "Total Timesteps: 146000 Episode Num: 146 Reward: 1126.6007794057325\n",
      "Total Timesteps: 147000 Episode Num: 147 Reward: 1357.9735061772012\n",
      "Total Timesteps: 148000 Episode Num: 148 Reward: 1423.294543850504\n",
      "Total Timesteps: 149000 Episode Num: 149 Reward: 1118.3174467698238\n",
      "Total Timesteps: 150000 Episode Num: 150 Reward: 1146.563785373939\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1216.405820\n",
      "---------------------------------------\n",
      "Total Timesteps: 151000 Episode Num: 151 Reward: 1178.8250478452849\n",
      "Total Timesteps: 152000 Episode Num: 152 Reward: 1418.8873725606031\n",
      "Total Timesteps: 153000 Episode Num: 153 Reward: 1398.7465475904842\n",
      "Total Timesteps: 154000 Episode Num: 154 Reward: 1158.281610782053\n",
      "Total Timesteps: 155000 Episode Num: 155 Reward: 1282.7672790970614\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1327.004682\n",
      "---------------------------------------\n",
      "Total Timesteps: 156000 Episode Num: 156 Reward: 1293.573305002236\n",
      "Total Timesteps: 157000 Episode Num: 157 Reward: 1245.0592158755005\n",
      "Total Timesteps: 158000 Episode Num: 158 Reward: 1342.7021654676205\n",
      "Total Timesteps: 159000 Episode Num: 159 Reward: 420.0059747544702\n",
      "Total Timesteps: 160000 Episode Num: 160 Reward: 1408.3183985840312\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1216.465302\n",
      "---------------------------------------\n",
      "Total Timesteps: 161000 Episode Num: 161 Reward: 1182.9524754500253\n",
      "Total Timesteps: 162000 Episode Num: 162 Reward: 1303.2175619489713\n",
      "Total Timesteps: 163000 Episode Num: 163 Reward: 1327.5753528251316\n",
      "Total Timesteps: 164000 Episode Num: 164 Reward: 1419.3951696584138\n",
      "Total Timesteps: 165000 Episode Num: 165 Reward: 1317.398729992548\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1341.366587\n",
      "---------------------------------------\n",
      "Total Timesteps: 166000 Episode Num: 166 Reward: 1419.399251313284\n",
      "Total Timesteps: 167000 Episode Num: 167 Reward: 1270.5202539409008\n",
      "Total Timesteps: 168000 Episode Num: 168 Reward: 1315.1033579152038\n",
      "Total Timesteps: 169000 Episode Num: 169 Reward: 1396.001155554398\n",
      "Total Timesteps: 170000 Episode Num: 170 Reward: 1314.199394512002\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1341.558469\n",
      "---------------------------------------\n",
      "Total Timesteps: 171000 Episode Num: 171 Reward: 1361.8107117832294\n",
      "Total Timesteps: 172000 Episode Num: 172 Reward: 1265.7542636181226\n",
      "Total Timesteps: 173000 Episode Num: 173 Reward: 1409.0646984655493\n",
      "Total Timesteps: 174000 Episode Num: 174 Reward: 1293.078004442019\n",
      "Total Timesteps: 175000 Episode Num: 175 Reward: 1300.7544652215308\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1474.728677\n",
      "---------------------------------------\n",
      "Total Timesteps: 176000 Episode Num: 176 Reward: 1406.0849294099069\n",
      "Total Timesteps: 177000 Episode Num: 177 Reward: 1318.3983087823867\n",
      "Total Timesteps: 178000 Episode Num: 178 Reward: 1471.5299352662616\n",
      "Total Timesteps: 179000 Episode Num: 179 Reward: 1389.4721463776796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 180000 Episode Num: 180 Reward: 1340.3508595513058\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1424.712276\n",
      "---------------------------------------\n",
      "Total Timesteps: 181000 Episode Num: 181 Reward: 1405.605478561652\n",
      "Total Timesteps: 182000 Episode Num: 182 Reward: 1336.4651514534803\n",
      "Total Timesteps: 183000 Episode Num: 183 Reward: 1424.5879274712443\n",
      "Total Timesteps: 184000 Episode Num: 184 Reward: -1689.6799947005213\n",
      "Total Timesteps: 185000 Episode Num: 185 Reward: -1694.680410923093\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1721.259433\n",
      "---------------------------------------\n",
      "Total Timesteps: 186000 Episode Num: 186 Reward: -1723.7410385497963\n",
      "Total Timesteps: 187000 Episode Num: 187 Reward: -1673.3390560577216\n",
      "Total Timesteps: 188000 Episode Num: 188 Reward: -1738.4487714616841\n",
      "Total Timesteps: 189000 Episode Num: 189 Reward: -1711.8771038887091\n",
      "Total Timesteps: 190000 Episode Num: 190 Reward: -1704.933728711363\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: -1826.931938\n",
      "---------------------------------------\n",
      "Total Timesteps: 191000 Episode Num: 191 Reward: -2307.659592000138\n",
      "Total Timesteps: 192000 Episode Num: 192 Reward: -1443.7330605445281\n",
      "Total Timesteps: 193000 Episode Num: 193 Reward: -1532.0796379482092\n",
      "Total Timesteps: 194000 Episode Num: 194 Reward: -1428.1987335961055\n",
      "Total Timesteps: 195000 Episode Num: 195 Reward: -150.45485195606506\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 151.209881\n",
      "---------------------------------------\n",
      "Total Timesteps: 196000 Episode Num: 196 Reward: 237.68044411094172\n",
      "Total Timesteps: 197000 Episode Num: 197 Reward: -347.2340570985403\n",
      "Total Timesteps: 198000 Episode Num: 198 Reward: 290.377005673976\n",
      "Total Timesteps: 199000 Episode Num: 199 Reward: 169.97496637024145\n",
      "Total Timesteps: 200000 Episode Num: 200 Reward: 383.94098098359285\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 302.082525\n",
      "---------------------------------------\n",
      "Total Timesteps: 201000 Episode Num: 201 Reward: 376.00035070078536\n",
      "Total Timesteps: 202000 Episode Num: 202 Reward: 298.34454768382864\n",
      "Total Timesteps: 203000 Episode Num: 203 Reward: 498.4044271723107\n",
      "Total Timesteps: 204000 Episode Num: 204 Reward: -13.823981958828048\n",
      "Total Timesteps: 205000 Episode Num: 205 Reward: 268.82923861604814\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 544.094762\n",
      "---------------------------------------\n",
      "Total Timesteps: 206000 Episode Num: 206 Reward: 533.1571416481655\n",
      "Total Timesteps: 207000 Episode Num: 207 Reward: 525.2976286031305\n",
      "Total Timesteps: 208000 Episode Num: 208 Reward: 495.12325214349505\n",
      "Total Timesteps: 209000 Episode Num: 209 Reward: 1438.9477474208882\n",
      "Total Timesteps: 210000 Episode Num: 210 Reward: 1448.6492231221123\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 649.481252\n",
      "---------------------------------------\n",
      "Total Timesteps: 211000 Episode Num: 211 Reward: 495.66822738540657\n",
      "Total Timesteps: 212000 Episode Num: 212 Reward: 1267.633929315342\n",
      "Total Timesteps: 213000 Episode Num: 213 Reward: 1118.8255810250544\n",
      "Total Timesteps: 214000 Episode Num: 214 Reward: 1398.9360991416113\n",
      "Total Timesteps: 215000 Episode Num: 215 Reward: 1409.2238498597105\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1550.959975\n",
      "---------------------------------------\n",
      "Total Timesteps: 216000 Episode Num: 216 Reward: 1458.8899905589549\n",
      "Total Timesteps: 217000 Episode Num: 217 Reward: 1452.326623140449\n",
      "Total Timesteps: 218000 Episode Num: 218 Reward: 1494.670360125012\n",
      "Total Timesteps: 219000 Episode Num: 219 Reward: 1476.770219919879\n",
      "Total Timesteps: 220000 Episode Num: 220 Reward: 1561.5964724799717\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1518.631376\n",
      "---------------------------------------\n",
      "Total Timesteps: 221000 Episode Num: 221 Reward: 1551.692930341821\n",
      "Total Timesteps: 222000 Episode Num: 222 Reward: 1415.4110857488276\n",
      "Total Timesteps: 223000 Episode Num: 223 Reward: 1476.9017672078246\n",
      "Total Timesteps: 224000 Episode Num: 224 Reward: 1421.739561583333\n",
      "Total Timesteps: 225000 Episode Num: 225 Reward: 1477.9357992939117\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1463.826099\n",
      "---------------------------------------\n",
      "Total Timesteps: 226000 Episode Num: 226 Reward: 1442.0943064817134\n",
      "Total Timesteps: 227000 Episode Num: 227 Reward: 1460.9936699915727\n",
      "Total Timesteps: 228000 Episode Num: 228 Reward: 1400.61737937984\n",
      "Total Timesteps: 229000 Episode Num: 229 Reward: 1402.6880319315567\n",
      "Total Timesteps: 230000 Episode Num: 230 Reward: 1442.2995790646812\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1375.294432\n",
      "---------------------------------------\n",
      "Total Timesteps: 231000 Episode Num: 231 Reward: 1410.6314068102072\n",
      "Total Timesteps: 232000 Episode Num: 232 Reward: 1102.167022596277\n",
      "Total Timesteps: 233000 Episode Num: 233 Reward: 1342.7279336611903\n",
      "Total Timesteps: 234000 Episode Num: 234 Reward: 1453.1732450835366\n",
      "Total Timesteps: 235000 Episode Num: 235 Reward: 1503.2583370582968\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1524.300283\n",
      "---------------------------------------\n",
      "Total Timesteps: 236000 Episode Num: 236 Reward: 1515.1673979763389\n",
      "Total Timesteps: 237000 Episode Num: 237 Reward: 1481.6300954968908\n",
      "Total Timesteps: 238000 Episode Num: 238 Reward: 1544.2913899154503\n",
      "Total Timesteps: 239000 Episode Num: 239 Reward: 1536.1953680326242\n",
      "Total Timesteps: 240000 Episode Num: 240 Reward: 1555.1710514258912\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1530.478973\n",
      "---------------------------------------\n",
      "Total Timesteps: 241000 Episode Num: 241 Reward: 1535.8452991297215\n",
      "Total Timesteps: 242000 Episode Num: 242 Reward: 1571.0514694408923\n",
      "Total Timesteps: 243000 Episode Num: 243 Reward: 1561.7895163380408\n",
      "Total Timesteps: 244000 Episode Num: 244 Reward: 1503.1323250948942\n",
      "Total Timesteps: 245000 Episode Num: 245 Reward: 1543.873031117565\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1555.047642\n",
      "---------------------------------------\n",
      "Total Timesteps: 246000 Episode Num: 246 Reward: 1546.8871563953114\n",
      "Total Timesteps: 247000 Episode Num: 247 Reward: 1571.825196599826\n",
      "Total Timesteps: 248000 Episode Num: 248 Reward: 1568.544359291644\n",
      "Total Timesteps: 249000 Episode Num: 249 Reward: 1549.2353003061548\n",
      "Total Timesteps: 250000 Episode Num: 250 Reward: 1606.3504451567483\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1560.943419\n",
      "---------------------------------------\n",
      "Total Timesteps: 251000 Episode Num: 251 Reward: 1536.0562417534225\n",
      "Total Timesteps: 252000 Episode Num: 252 Reward: 1546.9542684194823\n",
      "Total Timesteps: 253000 Episode Num: 253 Reward: 1584.527260143164\n",
      "Total Timesteps: 254000 Episode Num: 254 Reward: 1567.083913421483\n",
      "Total Timesteps: 255000 Episode Num: 255 Reward: 1526.084425412718\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1534.769448\n",
      "---------------------------------------\n",
      "Total Timesteps: 256000 Episode Num: 256 Reward: 1486.6630540057595\n",
      "Total Timesteps: 257000 Episode Num: 257 Reward: 1558.6328836151347\n",
      "Total Timesteps: 258000 Episode Num: 258 Reward: 1567.306012167911\n",
      "Total Timesteps: 259000 Episode Num: 259 Reward: 1680.5491852158455\n",
      "Total Timesteps: 260000 Episode Num: 260 Reward: 1511.2381985530646\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1621.005000\n",
      "---------------------------------------\n",
      "Total Timesteps: 261000 Episode Num: 261 Reward: 1523.9217853149294\n",
      "Total Timesteps: 262000 Episode Num: 262 Reward: 1646.361702182451\n",
      "Total Timesteps: 263000 Episode Num: 263 Reward: 1605.3905734765945\n",
      "Total Timesteps: 264000 Episode Num: 264 Reward: 1569.4328268991521\n",
      "Total Timesteps: 265000 Episode Num: 265 Reward: 1600.6938766378782\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1643.070706\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 266000 Episode Num: 266 Reward: 1597.291081368024\n",
      "Total Timesteps: 267000 Episode Num: 267 Reward: 1592.5684571536792\n",
      "Total Timesteps: 268000 Episode Num: 268 Reward: 1639.0056241637697\n",
      "Total Timesteps: 269000 Episode Num: 269 Reward: 1601.8048845673125\n",
      "Total Timesteps: 270000 Episode Num: 270 Reward: 1631.4831094688109\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1622.239209\n",
      "---------------------------------------\n",
      "Total Timesteps: 271000 Episode Num: 271 Reward: 1549.781088039545\n",
      "Total Timesteps: 272000 Episode Num: 272 Reward: 1577.8022143232934\n",
      "Total Timesteps: 273000 Episode Num: 273 Reward: 1593.5843120805537\n",
      "Total Timesteps: 274000 Episode Num: 274 Reward: 1597.4318258768792\n",
      "Total Timesteps: 275000 Episode Num: 275 Reward: 1535.6360212737616\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1637.083490\n",
      "---------------------------------------\n",
      "Total Timesteps: 276000 Episode Num: 276 Reward: 1597.650138732052\n",
      "Total Timesteps: 277000 Episode Num: 277 Reward: 1676.330878873416\n",
      "Total Timesteps: 278000 Episode Num: 278 Reward: 1631.4770774340047\n",
      "Total Timesteps: 279000 Episode Num: 279 Reward: 1529.3801674989224\n",
      "Total Timesteps: 280000 Episode Num: 280 Reward: 1680.0405712186148\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1647.601117\n",
      "---------------------------------------\n",
      "Total Timesteps: 281000 Episode Num: 281 Reward: 1557.7537440657704\n",
      "Total Timesteps: 282000 Episode Num: 282 Reward: 1596.5192415683755\n",
      "Total Timesteps: 283000 Episode Num: 283 Reward: 1650.333456114865\n",
      "Total Timesteps: 284000 Episode Num: 284 Reward: 1559.7939757322683\n",
      "Total Timesteps: 285000 Episode Num: 285 Reward: 1627.5167082429102\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1686.988195\n",
      "---------------------------------------\n",
      "Total Timesteps: 286000 Episode Num: 286 Reward: 1643.4108389706062\n",
      "Total Timesteps: 287000 Episode Num: 287 Reward: 1608.9715799091293\n",
      "Total Timesteps: 288000 Episode Num: 288 Reward: 1614.411469271693\n",
      "Total Timesteps: 289000 Episode Num: 289 Reward: 1629.3945199433883\n",
      "Total Timesteps: 290000 Episode Num: 290 Reward: 1634.37174930729\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1474.698255\n",
      "---------------------------------------\n",
      "Total Timesteps: 291000 Episode Num: 291 Reward: 1362.897544963044\n",
      "Total Timesteps: 292000 Episode Num: 292 Reward: 1677.1710577240588\n",
      "Total Timesteps: 293000 Episode Num: 293 Reward: 1645.5836587850204\n",
      "Total Timesteps: 294000 Episode Num: 294 Reward: 1575.382643227021\n",
      "Total Timesteps: 295000 Episode Num: 295 Reward: 1640.3345260913768\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1691.523727\n",
      "---------------------------------------\n",
      "Total Timesteps: 296000 Episode Num: 296 Reward: 1668.0081669965011\n",
      "Total Timesteps: 297000 Episode Num: 297 Reward: 1643.0175374787918\n",
      "Total Timesteps: 298000 Episode Num: 298 Reward: 1660.7234033792\n",
      "Total Timesteps: 299000 Episode Num: 299 Reward: 1565.4289028044282\n",
      "Total Timesteps: 300000 Episode Num: 300 Reward: 1637.182199152555\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1721.349997\n",
      "---------------------------------------\n",
      "Total Timesteps: 301000 Episode Num: 301 Reward: 1660.7142846333422\n",
      "Total Timesteps: 302000 Episode Num: 302 Reward: 1645.1770779772403\n",
      "Total Timesteps: 303000 Episode Num: 303 Reward: 1719.398659140676\n",
      "Total Timesteps: 304000 Episode Num: 304 Reward: 1715.667023713809\n",
      "Total Timesteps: 305000 Episode Num: 305 Reward: 1725.6094811643977\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1637.080969\n",
      "---------------------------------------\n",
      "Total Timesteps: 306000 Episode Num: 306 Reward: 1350.7880100548277\n",
      "Total Timesteps: 307000 Episode Num: 307 Reward: 1720.331792717252\n",
      "Total Timesteps: 308000 Episode Num: 308 Reward: 1728.037032132758\n",
      "Total Timesteps: 309000 Episode Num: 309 Reward: 1715.7677278925123\n",
      "Total Timesteps: 310000 Episode Num: 310 Reward: 1743.4627990066174\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1768.624651\n",
      "---------------------------------------\n",
      "Total Timesteps: 311000 Episode Num: 311 Reward: 1743.2903264375711\n",
      "Total Timesteps: 312000 Episode Num: 312 Reward: 1720.982598614544\n",
      "Total Timesteps: 313000 Episode Num: 313 Reward: 1756.2311145315837\n",
      "Total Timesteps: 314000 Episode Num: 314 Reward: 1752.9330148428367\n",
      "Total Timesteps: 315000 Episode Num: 315 Reward: 1737.1819517867857\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1773.383090\n",
      "---------------------------------------\n",
      "Total Timesteps: 316000 Episode Num: 316 Reward: 1755.190325214062\n",
      "Total Timesteps: 317000 Episode Num: 317 Reward: 1743.9442729236357\n",
      "Total Timesteps: 318000 Episode Num: 318 Reward: 1783.291485246214\n",
      "Total Timesteps: 319000 Episode Num: 319 Reward: 1730.647003592391\n",
      "Total Timesteps: 320000 Episode Num: 320 Reward: 1661.7737203686656\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1782.407751\n",
      "---------------------------------------\n",
      "Total Timesteps: 321000 Episode Num: 321 Reward: 1689.0424150629315\n",
      "Total Timesteps: 322000 Episode Num: 322 Reward: 1599.9107548554562\n",
      "Total Timesteps: 323000 Episode Num: 323 Reward: 1739.3754131708383\n",
      "Total Timesteps: 324000 Episode Num: 324 Reward: 1748.085728699807\n",
      "Total Timesteps: 325000 Episode Num: 325 Reward: 1732.4672002176367\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1812.409465\n",
      "---------------------------------------\n",
      "Total Timesteps: 326000 Episode Num: 326 Reward: 1768.6652963998192\n",
      "Total Timesteps: 327000 Episode Num: 327 Reward: 1722.0608390746177\n",
      "Total Timesteps: 328000 Episode Num: 328 Reward: 1717.0114040247881\n",
      "Total Timesteps: 329000 Episode Num: 329 Reward: 1613.4897096505563\n",
      "Total Timesteps: 330000 Episode Num: 330 Reward: 1778.8708074149476\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1820.792675\n",
      "---------------------------------------\n",
      "Total Timesteps: 331000 Episode Num: 331 Reward: 1749.6637198245924\n",
      "Total Timesteps: 332000 Episode Num: 332 Reward: 1772.494290779363\n",
      "Total Timesteps: 333000 Episode Num: 333 Reward: 1733.7278716387489\n",
      "Total Timesteps: 334000 Episode Num: 334 Reward: 1715.3544060072707\n",
      "Total Timesteps: 335000 Episode Num: 335 Reward: 1766.5251905261084\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1818.456064\n",
      "---------------------------------------\n",
      "Total Timesteps: 336000 Episode Num: 336 Reward: 1748.6629342705933\n",
      "Total Timesteps: 337000 Episode Num: 337 Reward: 1767.9869948362586\n",
      "Total Timesteps: 338000 Episode Num: 338 Reward: 1779.377650601881\n",
      "Total Timesteps: 339000 Episode Num: 339 Reward: 1762.3502325503275\n",
      "Total Timesteps: 340000 Episode Num: 340 Reward: 1772.6400402082268\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1812.083149\n",
      "---------------------------------------\n",
      "Total Timesteps: 341000 Episode Num: 341 Reward: 1736.2426295340263\n",
      "Total Timesteps: 342000 Episode Num: 342 Reward: 1732.3284791894425\n",
      "Total Timesteps: 343000 Episode Num: 343 Reward: 1798.5843545603575\n",
      "Total Timesteps: 344000 Episode Num: 344 Reward: 1781.2246512384245\n",
      "Total Timesteps: 345000 Episode Num: 345 Reward: 1576.5774993056418\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1816.185914\n",
      "---------------------------------------\n",
      "Total Timesteps: 346000 Episode Num: 346 Reward: 1738.3240362821687\n",
      "Total Timesteps: 347000 Episode Num: 347 Reward: 1779.132250261603\n",
      "Total Timesteps: 348000 Episode Num: 348 Reward: 1782.5792055966385\n",
      "Total Timesteps: 349000 Episode Num: 349 Reward: 1754.7803982498\n",
      "Total Timesteps: 350000 Episode Num: 350 Reward: 1827.9705955268746\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1848.943752\n",
      "---------------------------------------\n",
      "Total Timesteps: 351000 Episode Num: 351 Reward: 1854.2485126362214\n",
      "Total Timesteps: 352000 Episode Num: 352 Reward: 1783.6023633729747\n",
      "Total Timesteps: 353000 Episode Num: 353 Reward: 1801.3874428247104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 354000 Episode Num: 354 Reward: 1844.7829874438305\n",
      "Total Timesteps: 355000 Episode Num: 355 Reward: 1696.9585839664974\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1781.867593\n",
      "---------------------------------------\n",
      "Total Timesteps: 356000 Episode Num: 356 Reward: 1737.9827574419853\n",
      "Total Timesteps: 357000 Episode Num: 357 Reward: 1799.4771885642635\n",
      "Total Timesteps: 358000 Episode Num: 358 Reward: 1806.8431991969967\n",
      "Total Timesteps: 359000 Episode Num: 359 Reward: 1833.0981328703465\n",
      "Total Timesteps: 360000 Episode Num: 360 Reward: 1802.9494184832668\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1886.404975\n",
      "---------------------------------------\n",
      "Total Timesteps: 361000 Episode Num: 361 Reward: 1854.7918336237278\n",
      "Total Timesteps: 362000 Episode Num: 362 Reward: 1813.3925775739413\n",
      "Total Timesteps: 363000 Episode Num: 363 Reward: 1764.3057367627273\n",
      "Total Timesteps: 364000 Episode Num: 364 Reward: 1807.405292653953\n",
      "Total Timesteps: 365000 Episode Num: 365 Reward: 1821.2446584458398\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1879.845158\n",
      "---------------------------------------\n",
      "Total Timesteps: 366000 Episode Num: 366 Reward: 1803.991782613597\n",
      "Total Timesteps: 367000 Episode Num: 367 Reward: 1819.7053530757503\n",
      "Total Timesteps: 368000 Episode Num: 368 Reward: 1846.477979103924\n",
      "Total Timesteps: 369000 Episode Num: 369 Reward: 1858.231564849057\n",
      "Total Timesteps: 370000 Episode Num: 370 Reward: 1805.2619539779864\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1839.919597\n",
      "---------------------------------------\n",
      "Total Timesteps: 371000 Episode Num: 371 Reward: 1832.1413802241198\n",
      "Total Timesteps: 372000 Episode Num: 372 Reward: 1888.8333466446818\n",
      "Total Timesteps: 373000 Episode Num: 373 Reward: 1854.5867749667173\n",
      "Total Timesteps: 374000 Episode Num: 374 Reward: 1817.2625119425152\n",
      "Total Timesteps: 375000 Episode Num: 375 Reward: 1847.6431381587488\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1920.067509\n",
      "---------------------------------------\n",
      "Total Timesteps: 376000 Episode Num: 376 Reward: 1881.6756355017835\n",
      "Total Timesteps: 377000 Episode Num: 377 Reward: 1865.2217037150301\n",
      "Total Timesteps: 378000 Episode Num: 378 Reward: 1842.3108036388458\n",
      "Total Timesteps: 379000 Episode Num: 379 Reward: 1841.1025060216416\n",
      "Total Timesteps: 380000 Episode Num: 380 Reward: 1859.8908712891457\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1909.186628\n",
      "---------------------------------------\n",
      "Total Timesteps: 381000 Episode Num: 381 Reward: 1811.3923699465126\n",
      "Total Timesteps: 382000 Episode Num: 382 Reward: 1875.8633450595544\n",
      "Total Timesteps: 383000 Episode Num: 383 Reward: 1769.2284043743405\n",
      "Total Timesteps: 384000 Episode Num: 384 Reward: 1864.935206615854\n",
      "Total Timesteps: 385000 Episode Num: 385 Reward: 1841.7308190304207\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1905.165941\n",
      "---------------------------------------\n",
      "Total Timesteps: 386000 Episode Num: 386 Reward: 1825.0313547238009\n",
      "Total Timesteps: 387000 Episode Num: 387 Reward: 1896.5175303525828\n",
      "Total Timesteps: 388000 Episode Num: 388 Reward: 1854.4827852327708\n",
      "Total Timesteps: 389000 Episode Num: 389 Reward: 1889.7417784954803\n",
      "Total Timesteps: 390000 Episode Num: 390 Reward: 1869.1683126072667\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1941.720409\n",
      "---------------------------------------\n",
      "Total Timesteps: 391000 Episode Num: 391 Reward: 1843.6798080975184\n",
      "Total Timesteps: 392000 Episode Num: 392 Reward: 1866.1721208338827\n",
      "Total Timesteps: 393000 Episode Num: 393 Reward: 1867.3758136571041\n",
      "Total Timesteps: 394000 Episode Num: 394 Reward: 1838.1310924148747\n",
      "Total Timesteps: 395000 Episode Num: 395 Reward: 1903.7664435780175\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1918.403466\n",
      "---------------------------------------\n",
      "Total Timesteps: 396000 Episode Num: 396 Reward: 1855.2581509677307\n",
      "Total Timesteps: 397000 Episode Num: 397 Reward: 1855.4880138073208\n",
      "Total Timesteps: 398000 Episode Num: 398 Reward: 1823.222987795583\n",
      "Total Timesteps: 399000 Episode Num: 399 Reward: 1814.320323162001\n",
      "Total Timesteps: 400000 Episode Num: 400 Reward: 1852.348877369428\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1835.102446\n",
      "---------------------------------------\n",
      "Total Timesteps: 401000 Episode Num: 401 Reward: 1746.3326862221338\n",
      "Total Timesteps: 402000 Episode Num: 402 Reward: 1808.5742200875768\n",
      "Total Timesteps: 403000 Episode Num: 403 Reward: 1883.2392872521955\n",
      "Total Timesteps: 404000 Episode Num: 404 Reward: 1921.505347114522\n",
      "Total Timesteps: 405000 Episode Num: 405 Reward: 1841.6240550713264\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1919.379087\n",
      "---------------------------------------\n",
      "Total Timesteps: 406000 Episode Num: 406 Reward: 1832.697475022926\n",
      "Total Timesteps: 407000 Episode Num: 407 Reward: 1881.7314016955709\n",
      "Total Timesteps: 408000 Episode Num: 408 Reward: 1892.9128386788634\n",
      "Total Timesteps: 409000 Episode Num: 409 Reward: 1833.8192644034398\n",
      "Total Timesteps: 410000 Episode Num: 410 Reward: 1890.1720661735528\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1961.726325\n",
      "---------------------------------------\n",
      "Total Timesteps: 411000 Episode Num: 411 Reward: 1877.748884987311\n",
      "Total Timesteps: 412000 Episode Num: 412 Reward: 1887.5044881986273\n",
      "Total Timesteps: 413000 Episode Num: 413 Reward: 1926.5849221519436\n",
      "Total Timesteps: 414000 Episode Num: 414 Reward: 1887.0899831973668\n",
      "Total Timesteps: 415000 Episode Num: 415 Reward: 1937.7256323876422\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1914.437156\n",
      "---------------------------------------\n",
      "Total Timesteps: 416000 Episode Num: 416 Reward: 1864.7281558616635\n",
      "Total Timesteps: 417000 Episode Num: 417 Reward: 1834.1230533845105\n",
      "Total Timesteps: 418000 Episode Num: 418 Reward: 1901.4251641761127\n",
      "Total Timesteps: 419000 Episode Num: 419 Reward: 1868.739274875916\n",
      "Total Timesteps: 420000 Episode Num: 420 Reward: 1898.7896029923031\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1989.605699\n",
      "---------------------------------------\n",
      "Total Timesteps: 421000 Episode Num: 421 Reward: 1924.8772835131192\n",
      "Total Timesteps: 422000 Episode Num: 422 Reward: 1859.0777697265653\n",
      "Total Timesteps: 423000 Episode Num: 423 Reward: 1888.0630920576396\n",
      "Total Timesteps: 424000 Episode Num: 424 Reward: 1908.4187670462616\n",
      "Total Timesteps: 425000 Episode Num: 425 Reward: 1874.6339069977291\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1915.496870\n",
      "---------------------------------------\n",
      "Total Timesteps: 426000 Episode Num: 426 Reward: 1843.9022570858558\n",
      "Total Timesteps: 427000 Episode Num: 427 Reward: 1882.670603193312\n",
      "Total Timesteps: 428000 Episode Num: 428 Reward: 1835.3854876129703\n",
      "Total Timesteps: 429000 Episode Num: 429 Reward: 1969.339181534746\n",
      "Total Timesteps: 430000 Episode Num: 430 Reward: 1707.6150149408236\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1983.187371\n",
      "---------------------------------------\n",
      "Total Timesteps: 431000 Episode Num: 431 Reward: 1904.1805556514805\n",
      "Total Timesteps: 432000 Episode Num: 432 Reward: 1941.6024224584892\n",
      "Total Timesteps: 433000 Episode Num: 433 Reward: 1919.0603712454536\n",
      "Total Timesteps: 434000 Episode Num: 434 Reward: 1913.4653300220093\n",
      "Total Timesteps: 435000 Episode Num: 435 Reward: 1856.230443316385\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 2014.112675\n",
      "---------------------------------------\n",
      "Total Timesteps: 436000 Episode Num: 436 Reward: 1896.0658741297427\n",
      "Total Timesteps: 437000 Episode Num: 437 Reward: 1890.4630803689076\n",
      "Total Timesteps: 438000 Episode Num: 438 Reward: 1816.2746817254763\n",
      "Total Timesteps: 439000 Episode Num: 439 Reward: 1919.715575073345\n",
      "Total Timesteps: 440000 Episode Num: 440 Reward: 1933.4246481086745\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1946.376151\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Timesteps: 441000 Episode Num: 441 Reward: 1864.5501405971768\n",
      "Total Timesteps: 442000 Episode Num: 442 Reward: 1912.7804528286372\n",
      "Total Timesteps: 443000 Episode Num: 443 Reward: 1892.2052401765116\n",
      "Total Timesteps: 444000 Episode Num: 444 Reward: 1927.7141121632055\n",
      "Total Timesteps: 445000 Episode Num: 445 Reward: 1898.549661754476\n",
      "---------------------------------------\n",
      "Average Reward over the Evaluation Step: 1955.569542\n",
      "---------------------------------------\n",
      "Total Timesteps: 446000 Episode Num: 446 Reward: 1859.7551834160188\n",
      "Total Timesteps: 447000 Episode Num: 447 Reward: 1873.1707495729506\n",
      "Total Timesteps: 448000 Episode Num: 448 Reward: 1868.7151572880787\n",
      "Total Timesteps: 449000 Episode Num: 449 Reward: 1973.6637699349262\n",
      "Total Timesteps: 450000 Episode Num: 450 Reward: 1846.5166441375934\n"
     ]
    }
   ],
   "source": [
    "# We start the main loop over 500,000 timesteps\n",
    "while total_timesteps < max_timesteps:\n",
    "  \n",
    "  # If the episode is done\n",
    "  if done:\n",
    "\n",
    "    # If we are not at the very beginning, we start the training process of the model\n",
    "    if total_timesteps != 0:\n",
    "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
    "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "\n",
    "    # We evaluate the episode and we save the policy\n",
    "    if timesteps_since_eval >= eval_freq:\n",
    "      timesteps_since_eval %= eval_freq\n",
    "      evaluations.append(evaluate_policy(policy))\n",
    "      policy.save(file_name, directory=\"./pytorch_models\")\n",
    "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
    "    \n",
    "    # When the training step is done, we reset the state of the environment\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Set the Done to False\n",
    "    done = False\n",
    "    \n",
    "    # Set rewards and episode timesteps to zero\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num += 1\n",
    "  \n",
    "  # Before 10000 timesteps, we play random actions\n",
    "  if total_timesteps < start_timesteps:\n",
    "    action = env.action_space.sample()\n",
    "  else: # After 10000 timesteps, we switch to the model\n",
    "    action = policy.select_action(np.array(obs))\n",
    "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "    if expl_noise != 0:\n",
    "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
    "  \n",
    "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "  new_obs, reward, done, _ = env.step(action)\n",
    "  \n",
    "  # We check if the episode is done\n",
    "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
    "  \n",
    "  # We increase the total reward\n",
    "  episode_reward += reward\n",
    "  \n",
    "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
    "\n",
    "  # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
    "  obs = new_obs\n",
    "  episode_timesteps += 1\n",
    "  total_timesteps += 1\n",
    "  timesteps_since_eval += 1\n",
    "\n",
    "# We add the last policy evaluation to our list of evaluations and we save our model\n",
    "evaluations.append(evaluate_policy(policy))\n",
    "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
    "np.save(\"./results/%s\" % (file_name), evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wi6e2-_pu05e"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW4d1YAMqif1"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    super(Actor, self).__init__()\n",
    "    self.layer_1 = nn.Linear(state_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, action_dim)\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_1(x))\n",
    "    x = F.relu(self.layer_2(x))\n",
    "    x = self.max_action * torch.tanh(self.layer_3(x)) \n",
    "    return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim):\n",
    "    super(Critic, self).__init__()\n",
    "    # Defining the first Critic neural network\n",
    "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_2 = nn.Linear(400, 300)\n",
    "    self.layer_3 = nn.Linear(300, 1)\n",
    "    # Defining the second Critic neural network\n",
    "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
    "    self.layer_5 = nn.Linear(400, 300)\n",
    "    self.layer_6 = nn.Linear(300, 1)\n",
    "\n",
    "  def forward(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    # Forward-Propagation on the first Critic Neural Network\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    # Forward-Propagation on the second Critic Neural Network\n",
    "    x2 = F.relu(self.layer_4(xu))\n",
    "    x2 = F.relu(self.layer_5(x2))\n",
    "    x2 = self.layer_6(x2)\n",
    "    return x1, x2\n",
    "\n",
    "  def Q1(self, x, u):\n",
    "    xu = torch.cat([x, u], 1)\n",
    "    x1 = F.relu(self.layer_1(xu))\n",
    "    x1 = F.relu(self.layer_2(x1))\n",
    "    x1 = self.layer_3(x1)\n",
    "    return x1\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Building the whole Training Process into a class\n",
    "\n",
    "class TD3(object):\n",
    "  \n",
    "  def __init__(self, state_dim, action_dim, max_action):\n",
    "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "    self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "    self.critic = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "    self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "    self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "    self.max_action = max_action\n",
    "\n",
    "  def select_action(self, state):\n",
    "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "    return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
    "    \n",
    "    for it in range(iterations):\n",
    "      \n",
    "      # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
    "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
    "      state = torch.Tensor(batch_states).to(device)\n",
    "      next_state = torch.Tensor(batch_next_states).to(device)\n",
    "      action = torch.Tensor(batch_actions).to(device)\n",
    "      reward = torch.Tensor(batch_rewards).to(device)\n",
    "      done = torch.Tensor(batch_dones).to(device)\n",
    "      \n",
    "      # Step 5: From the next state s’, the Actor target plays the next action a’\n",
    "      next_action = self.actor_target(next_state)\n",
    "      \n",
    "      # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
    "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "      noise = noise.clamp(-noise_clip, noise_clip)\n",
    "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "      \n",
    "      # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
    "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "      \n",
    "      # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
    "      target_Q = torch.min(target_Q1, target_Q2)\n",
    "      \n",
    "      # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
    "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
    "      \n",
    "      # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
    "      current_Q1, current_Q2 = self.critic(state, action)\n",
    "      \n",
    "      # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
    "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "      \n",
    "      # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
    "      self.critic_optimizer.zero_grad()\n",
    "      critic_loss.backward()\n",
    "      self.critic_optimizer.step()\n",
    "      \n",
    "      # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "      if it % policy_freq == 0:\n",
    "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "  \n",
    "  # Making a save method to save a trained model\n",
    "  def save(self, filename, directory):\n",
    "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "  \n",
    "  # Making a load method to load a pre-trained model\n",
    "  def load(self, filename, directory):\n",
    "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
    "\n",
    "def evaluate_policy(policy, eval_episodes=10):\n",
    "  avg_reward = 0.\n",
    "  for _ in range(eval_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "      action = policy.select_action(np.array(obs))\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      avg_reward += reward\n",
    "  avg_reward /= eval_episodes\n",
    "  print (\"---------------------------------------\")\n",
    "  print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
    "  print (\"---------------------------------------\")\n",
    "  return avg_reward\n",
    "\n",
    "env_name = \"HalfCheetahBulletEnv-v0\"\n",
    "seed = 0\n",
    "\n",
    "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
    "print (\"---------------------------------------\")\n",
    "print (\"Settings: %s\" % (file_name))\n",
    "print (\"---------------------------------------\")\n",
    "\n",
    "eval_episodes = 10\n",
    "save_env_vid = True\n",
    "env = gym.make(env_name)\n",
    "max_episode_steps = env._max_episode_steps\n",
    "if save_env_vid:\n",
    "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "  env.reset()\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "policy = TD3(state_dim, action_dim, max_action)\n",
    "policy.load(file_name, './pytorch_models/')\n",
    "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TD3_Half_Cheetah.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
