{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x75a015023350>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "Display(visible=False, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankdesh/explore/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo, RecordEpisodeStatistics, TimeLimit\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_gpus = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(episode=0):\n",
    "  video_file = open(f'./videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
    "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, hidden_size, obs_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)    \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 2])\n",
      "tensor([[ 0.0580,  0.0497],\n",
      "        [-0.0133,  0.0088]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dqn = DQN(128, 4, 2) \n",
    "sample_obs = torch.rand(2, 4) # 2 observations, each with 4 features\n",
    "chosen_action = sample_dqn(sample_obs) # 2 actions, one for each observation\n",
    "print (sample_dqn)\n",
    "print (sample_obs.shape)\n",
    "print (chosen_action.shape)\n",
    "print (chosen_action)\n",
    "_, action = torch.max(chosen_action, dim=1)\n",
    "torch.tensor([int(x.item()) for x in action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Îµ-greedy policy\n",
    "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        state = torch.tensor([state]).to(device=device)\n",
    "        q_values = net(state)\n",
    "        _, action = torch.max(q_values, dim=1)\n",
    "        action = torch.tensor([int(x.item()) for x in action])\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaysBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class RLDatasest(IterableDataset):\n",
    "\n",
    "    def __init__(self, buffer, sample_size=200):\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for experience in self.buffer.sample(self.sample_size):\n",
    "            yield experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(name):\n",
    "    env = gym.make(name, render_mode='rgb_array')\n",
    "    env = TimeLimit(env, max_episode_steps=400)\n",
    "    env = RecordVideo(env, video_folder='./videos', episode_trigger=lambda x: x % 50 == 0)\n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankdesh/explore/.venv/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/ankdesh/explore/LearnTry-ML/RL/Udemy-Advanced-RL/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = create_environment(\"LunarLander-v3\")\n",
    "for episode in range(10):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.utilities.types import TRAIN_DATALOADERS\n",
    "\n",
    "\n",
    "class DeepQLearning(LightningModule):\n",
    "    def __init__(self, env_name, policy = epsilon_greedy, capacity = 100000, batch_size=256, lr=1e-3, hidden_size=128, \n",
    "                 gamma=0.99, loss_fn=F.smooth_l1_loss, optim=AdamW, epsilon_start=1.0, epsilon_end=0.15, epsilon_last_episode=100,\n",
    "                 samples_per_epoch = 10000, sync_rate=10):\n",
    "        super(DeepQLearning, self).__init__()\n",
    "        self.env = create_environment(env_name)\n",
    "        self.q_net = DQN(hidden_size, self.env.observation_space.shape[0], self.env.action_space.n)\n",
    "\n",
    "        self.target_q_net = copy.deepcopy(self.q_net)\n",
    "\n",
    "        self.policy = policy\n",
    "        self.buffer = ReplaysBuffer(capacity)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
    "            print(f\"Populating buffer: {len(self.buffer)}/{self.hparams.samples_per_epoch * 100}%\")\n",
    "            self.play_episode(epsilon=self.hparams.epsilon_start)\n",
    "\n",
    "    @torch.no_grad        \n",
    "    def play_episode(self, policy=None, epsilon=0.0):\n",
    "        done = False\n",
    "        state = self.env.reset()\n",
    "        while not done:\n",
    "            if policy:\n",
    "                action = policy(state, self.env, self.q_net, epsilon)\n",
    "            else:\n",
    "                action = self.env.action_space.sample() # raondom action to increase exploration\n",
    "            next_state, reward, done, truncated, info = self.env.step(action)\n",
    "            experience = (state, action, reward, done, next_state)\n",
    "            self.buffer.append(experience)\n",
    "            state = next_state\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.q_net(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
    "        return [q_net_optimizer]\n",
    "    \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = RLDatasest(self.buffer, self.hparams.samples_per_epoch)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.hparams.batch_size)\n",
    "        return dataloader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        actions = actions.unsqueeze(1)\n",
    "        rewards = rewards.unsqueeze(1)\n",
    "        dones = dones.unsqueeze(1)\n",
    "\n",
    "        state_action_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        next_action_values, _ = self.target_q_net(next_states).max(dim=1, keepdim=True)[0]\n",
    "        next_action_values[dones] = 0.0\n",
    "\n",
    "        expected_state_action_values = rewards + self.hparams.gamma * next_action_values \n",
    "\n",
    "        loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n",
    "\n",
    "        self.log(\"episode/Q_error\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        epsilon = max(self.hparams.eps_end, \n",
    "                      self.hparams.end_start  - self.current_epoch / self.hparams.epsilon_last_episode)\n",
    "        \n",
    "        self.play_episode(policy=self.policy, epsilon=epsilon)\n",
    "        self.log('episode/Return', self.env.return_queue[-1])\n",
    "\n",
    "        if self.current_epoch % self.hparams.sync_rate == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'lightning_logs/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! rm -r videos/\n",
    "! rm -r lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating buffer: 0/1000000%\n",
      "Populating buffer: 89/1000000%\n",
      "Populating buffer: 250/1000000%\n",
      "Populating buffer: 326/1000000%\n",
      "Populating buffer: 401/1000000%\n",
      "Populating buffer: 523/1000000%\n",
      "Populating buffer: 619/1000000%\n",
      "Populating buffer: 677/1000000%\n",
      "Populating buffer: 750/1000000%\n",
      "Populating buffer: 851/1000000%\n",
      "Populating buffer: 972/1000000%\n",
      "Populating buffer: 1053/1000000%\n",
      "Populating buffer: 1156/1000000%\n",
      "Populating buffer: 1226/1000000%\n",
      "Populating buffer: 1299/1000000%\n",
      "Populating buffer: 1392/1000000%\n",
      "Populating buffer: 1461/1000000%\n",
      "Populating buffer: 1562/1000000%\n",
      "Populating buffer: 1690/1000000%\n",
      "Populating buffer: 1782/1000000%\n",
      "Populating buffer: 1910/1000000%\n",
      "Populating buffer: 1986/1000000%\n",
      "Populating buffer: 2092/1000000%\n",
      "Populating buffer: 2160/1000000%\n",
      "Populating buffer: 2268/1000000%\n",
      "Populating buffer: 2352/1000000%\n",
      "Populating buffer: 2452/1000000%\n",
      "Populating buffer: 2556/1000000%\n",
      "Populating buffer: 2676/1000000%\n",
      "Populating buffer: 2773/1000000%\n",
      "Populating buffer: 2891/1000000%\n",
      "Populating buffer: 2957/1000000%\n",
      "Populating buffer: 3027/1000000%\n",
      "Populating buffer: 3135/1000000%\n",
      "Populating buffer: 3237/1000000%\n",
      "Populating buffer: 3324/1000000%\n",
      "Populating buffer: 3450/1000000%\n",
      "Populating buffer: 3537/1000000%\n",
      "Populating buffer: 3652/1000000%\n",
      "Populating buffer: 3733/1000000%\n",
      "Populating buffer: 3832/1000000%\n",
      "Populating buffer: 3894/1000000%\n",
      "Populating buffer: 3969/1000000%\n",
      "Populating buffer: 4042/1000000%\n",
      "Populating buffer: 4162/1000000%\n",
      "Populating buffer: 4274/1000000%\n",
      "Populating buffer: 4367/1000000%\n",
      "Populating buffer: 4431/1000000%\n",
      "Populating buffer: 4519/1000000%\n",
      "Populating buffer: 4596/1000000%\n",
      "Populating buffer: 4675/1000000%\n",
      "Populating buffer: 4834/1000000%\n",
      "Populating buffer: 4934/1000000%\n",
      "Populating buffer: 5071/1000000%\n",
      "Populating buffer: 5165/1000000%\n",
      "Populating buffer: 5298/1000000%\n",
      "Populating buffer: 5382/1000000%\n",
      "Populating buffer: 5482/1000000%\n",
      "Populating buffer: 5552/1000000%\n",
      "Populating buffer: 5626/1000000%\n",
      "Populating buffer: 5718/1000000%\n",
      "Populating buffer: 5813/1000000%\n",
      "Populating buffer: 5904/1000000%\n",
      "Populating buffer: 5992/1000000%\n",
      "Populating buffer: 6097/1000000%\n",
      "Populating buffer: 6204/1000000%\n",
      "Populating buffer: 6283/1000000%\n",
      "Populating buffer: 6374/1000000%\n",
      "Populating buffer: 6484/1000000%\n",
      "Populating buffer: 6589/1000000%\n",
      "Populating buffer: 6655/1000000%\n",
      "Populating buffer: 6742/1000000%\n",
      "Populating buffer: 6865/1000000%\n",
      "Populating buffer: 6982/1000000%\n",
      "Populating buffer: 7085/1000000%\n",
      "Populating buffer: 7169/1000000%\n",
      "Populating buffer: 7266/1000000%\n",
      "Populating buffer: 7382/1000000%\n",
      "Populating buffer: 7479/1000000%\n",
      "Populating buffer: 7546/1000000%\n",
      "Populating buffer: 7634/1000000%\n",
      "Populating buffer: 7705/1000000%\n",
      "Populating buffer: 7777/1000000%\n",
      "Populating buffer: 7905/1000000%\n",
      "Populating buffer: 8017/1000000%\n",
      "Populating buffer: 8120/1000000%\n",
      "Populating buffer: 8213/1000000%\n",
      "Populating buffer: 8290/1000000%\n",
      "Populating buffer: 8355/1000000%\n",
      "Populating buffer: 8458/1000000%\n",
      "Populating buffer: 8556/1000000%\n",
      "Populating buffer: 8627/1000000%\n",
      "Populating buffer: 8748/1000000%\n",
      "Populating buffer: 8849/1000000%\n",
      "Populating buffer: 8959/1000000%\n",
      "Populating buffer: 9047/1000000%\n",
      "Populating buffer: 9154/1000000%\n",
      "Populating buffer: 9248/1000000%\n",
      "Populating buffer: 9346/1000000%\n",
      "Populating buffer: 9436/1000000%\n",
      "Populating buffer: 9500/1000000%\n",
      "Populating buffer: 9600/1000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating buffer: 9684/1000000%\n",
      "Populating buffer: 9795/1000000%\n",
      "Populating buffer: 9875/1000000%\n",
      "Populating buffer: 9960/1000000%\n"
     ]
    }
   ],
   "source": [
    "algo = DeepQLearning(\"LunarLander-v3\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs = 10000,\n",
    "    callbacks=[EarlyStopping(monitor=\"episode/Return\", mode=\"max\", patience=500)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
