body:
  children:
  - $ref: '#/texts/0'
  - $ref: '#/texts/1'
  - $ref: '#/texts/2'
  - $ref: '#/texts/3'
  - $ref: '#/texts/4'
  - $ref: '#/texts/5'
  - $ref: '#/texts/6'
  - $ref: '#/groups/0'
  - $ref: '#/texts/10'
  - $ref: '#/texts/11'
  - $ref: '#/texts/12'
  - $ref: '#/texts/13'
  - $ref: '#/texts/14'
  - $ref: '#/texts/15'
  - $ref: '#/texts/16'
  - $ref: '#/texts/17'
  - $ref: '#/texts/18'
  - $ref: '#/texts/19'
  - $ref: '#/texts/20'
  - $ref: '#/texts/21'
  - $ref: '#/texts/22'
  - $ref: '#/texts/23'
  - $ref: '#/texts/24'
  - $ref: '#/texts/25'
  - $ref: '#/texts/26'
  - $ref: '#/texts/27'
  - $ref: '#/texts/28'
  - $ref: '#/texts/29'
  - $ref: '#/texts/30'
  - $ref: '#/pictures/0'
  - $ref: '#/texts/65'
  - $ref: '#/texts/66'
  - $ref: '#/texts/67'
  - $ref: '#/texts/68'
  - $ref: '#/texts/69'
  - $ref: '#/texts/70'
  - $ref: '#/texts/71'
  - $ref: '#/texts/72'
  - $ref: '#/pictures/1'
  - $ref: '#/pictures/2'
  - $ref: '#/texts/87'
  - $ref: '#/texts/88'
  - $ref: '#/texts/89'
  - $ref: '#/texts/90'
  - $ref: '#/texts/91'
  - $ref: '#/texts/92'
  - $ref: '#/texts/93'
  - $ref: '#/texts/94'
  - $ref: '#/texts/95'
  - $ref: '#/texts/96'
  - $ref: '#/texts/97'
  - $ref: '#/texts/98'
  - $ref: '#/texts/99'
  - $ref: '#/texts/100'
  - $ref: '#/texts/101'
  - $ref: '#/texts/102'
  - $ref: '#/texts/103'
  - $ref: '#/texts/104'
  - $ref: '#/texts/105'
  - $ref: '#/groups/1'
  - $ref: '#/texts/109'
  - $ref: '#/texts/110'
  - $ref: '#/texts/111'
  - $ref: '#/texts/112'
  - $ref: '#/texts/113'
  - $ref: '#/texts/114'
  - $ref: '#/texts/115'
  - $ref: '#/tables/0'
  - $ref: '#/texts/117'
  - $ref: '#/texts/118'
  - $ref: '#/texts/119'
  - $ref: '#/texts/120'
  - $ref: '#/texts/121'
  - $ref: '#/texts/122'
  - $ref: '#/texts/123'
  - $ref: '#/texts/124'
  - $ref: '#/texts/125'
  - $ref: '#/texts/126'
  - $ref: '#/texts/127'
  - $ref: '#/texts/128'
  - $ref: '#/texts/129'
  - $ref: '#/texts/130'
  - $ref: '#/texts/131'
  - $ref: '#/texts/132'
  - $ref: '#/texts/133'
  - $ref: '#/texts/134'
  - $ref: '#/texts/135'
  - $ref: '#/texts/136'
  - $ref: '#/texts/137'
  - $ref: '#/texts/138'
  - $ref: '#/texts/139'
  - $ref: '#/texts/140'
  - $ref: '#/texts/141'
  - $ref: '#/texts/142'
  - $ref: '#/texts/143'
  - $ref: '#/texts/144'
  - $ref: '#/tables/1'
  - $ref: '#/texts/146'
  - $ref: '#/texts/147'
  - $ref: '#/texts/148'
  - $ref: '#/texts/149'
  - $ref: '#/texts/150'
  - $ref: '#/texts/151'
  - $ref: '#/texts/152'
  - $ref: '#/texts/153'
  - $ref: '#/texts/154'
  - $ref: '#/texts/155'
  - $ref: '#/texts/156'
  - $ref: '#/texts/157'
  - $ref: '#/tables/2'
  - $ref: '#/texts/159'
  - $ref: '#/texts/160'
  - $ref: '#/texts/161'
  - $ref: '#/texts/162'
  - $ref: '#/texts/163'
  - $ref: '#/texts/164'
  - $ref: '#/texts/165'
  - $ref: '#/texts/166'
  - $ref: '#/tables/3'
  - $ref: '#/texts/168'
  - $ref: '#/texts/169'
  - $ref: '#/texts/170'
  - $ref: '#/texts/171'
  - $ref: '#/texts/172'
  - $ref: '#/texts/173'
  - $ref: '#/texts/174'
  - $ref: '#/texts/175'
  - $ref: '#/texts/176'
  - $ref: '#/texts/177'
  - $ref: '#/groups/2'
  - $ref: '#/texts/182'
  - $ref: '#/tables/4'
  - $ref: '#/texts/183'
  - $ref: '#/tables/5'
  - $ref: '#/texts/184'
  - $ref: '#/texts/185'
  - $ref: '#/pictures/3'
  - $ref: '#/texts/222'
  - $ref: '#/texts/223'
  - $ref: '#/pictures/4'
  - $ref: '#/texts/299'
  - $ref: '#/texts/300'
  - $ref: '#/pictures/5'
  - $ref: '#/texts/376'
  content_layer: body
  label: unspecified
  name: _root_
  self_ref: '#/body'
form_items: []
furniture:
  children: []
  content_layer: furniture
  label: unspecified
  name: _root_
  self_ref: '#/furniture'
groups:
- children:
  - $ref: '#/texts/7'
  - $ref: '#/texts/8'
  - $ref: '#/texts/9'
  content_layer: body
  label: key_value_area
  name: group
  parent:
    $ref: '#/body'
  self_ref: '#/groups/0'
- children:
  - $ref: '#/texts/106'
  - $ref: '#/texts/107'
  - $ref: '#/texts/108'
  content_layer: body
  label: list
  name: list
  parent:
    $ref: '#/body'
  self_ref: '#/groups/1'
- children:
  - $ref: '#/texts/178'
  - $ref: '#/texts/179'
  - $ref: '#/texts/180'
  - $ref: '#/texts/181'
  content_layer: body
  label: list
  name: list
  parent:
    $ref: '#/body'
  self_ref: '#/groups/2'
key_value_items: []
name: sample
origin:
  binary_hash: 2949302674760005271
  filename: sample.pdf
  mimetype: application/pdf
pages:
  '1':
    page_no: 1
    size:
      height: 792.0
      width: 612.0
  '10':
    page_no: 10
    size:
      height: 792.0
      width: 612.0
  '11':
    page_no: 11
    size:
      height: 792.0
      width: 612.0
  '12':
    page_no: 12
    size:
      height: 792.0
      width: 612.0
  '13':
    page_no: 13
    size:
      height: 792.0
      width: 612.0
  '14':
    page_no: 14
    size:
      height: 792.0
      width: 612.0
  '15':
    page_no: 15
    size:
      height: 792.0
      width: 612.0
  '2':
    page_no: 2
    size:
      height: 792.0
      width: 612.0
  '3':
    page_no: 3
    size:
      height: 792.0
      width: 612.0
  '4':
    page_no: 4
    size:
      height: 792.0
      width: 612.0
  '5':
    page_no: 5
    size:
      height: 792.0
      width: 612.0
  '6':
    page_no: 6
    size:
      height: 792.0
      width: 612.0
  '7':
    page_no: 7
    size:
      height: 792.0
      width: 612.0
  '8':
    page_no: 8
    size:
      height: 792.0
      width: 612.0
  '9':
    page_no: 9
    size:
      height: 792.0
      width: 612.0
pictures:
- annotations: []
  captions:
  - $ref: '#/texts/31'
  children:
  - $ref: '#/texts/31'
  - $ref: '#/texts/32'
  - $ref: '#/texts/33'
  - $ref: '#/texts/34'
  - $ref: '#/texts/35'
  - $ref: '#/texts/36'
  - $ref: '#/texts/37'
  - $ref: '#/texts/38'
  - $ref: '#/texts/39'
  - $ref: '#/texts/40'
  - $ref: '#/texts/41'
  - $ref: '#/texts/42'
  - $ref: '#/texts/43'
  - $ref: '#/texts/44'
  - $ref: '#/texts/45'
  - $ref: '#/texts/46'
  - $ref: '#/texts/47'
  - $ref: '#/texts/48'
  - $ref: '#/texts/49'
  - $ref: '#/texts/50'
  - $ref: '#/texts/51'
  - $ref: '#/texts/52'
  - $ref: '#/texts/53'
  - $ref: '#/texts/54'
  - $ref: '#/texts/55'
  - $ref: '#/texts/56'
  - $ref: '#/texts/57'
  - $ref: '#/texts/58'
  - $ref: '#/texts/59'
  - $ref: '#/texts/60'
  - $ref: '#/texts/61'
  - $ref: '#/texts/62'
  - $ref: '#/texts/63'
  - $ref: '#/texts/64'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 397.27667236328125
      coord_origin: BOTTOMLEFT
      l: 196.17898559570312
      r: 415.5890808105469
      t: 719.2168273925781
    charspan:
    - 0
    - 0
    page_no: 3
  references: []
  self_ref: '#/pictures/0'
- annotations: []
  captions: []
  children:
  - $ref: '#/texts/73'
  - $ref: '#/texts/74'
  - $ref: '#/texts/75'
  - $ref: '#/texts/76'
  - $ref: '#/texts/77'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 572.2972564697266
      coord_origin: BOTTOMLEFT
      l: 175.12269592285156
      r: 239.90185546875
      t: 697.6979827880859
    charspan:
    - 0
    - 0
    page_no: 4
  references: []
  self_ref: '#/pictures/1'
- annotations: []
  captions:
  - $ref: '#/texts/78'
  children:
  - $ref: '#/texts/78'
  - $ref: '#/texts/79'
  - $ref: '#/texts/80'
  - $ref: '#/texts/81'
  - $ref: '#/texts/82'
  - $ref: '#/texts/83'
  - $ref: '#/texts/84'
  - $ref: '#/texts/85'
  - $ref: '#/texts/86'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 553.0907287597656
      coord_origin: BOTTOMLEFT
      l: 346.84661865234375
      r: 467.69573974609375
      t: 720.5292129516602
    charspan:
    - 0
    - 0
    page_no: 4
  references: []
  self_ref: '#/pictures/2'
- annotations: []
  captions:
  - $ref: '#/texts/186'
  children:
  - $ref: '#/texts/186'
  - $ref: '#/texts/187'
  - $ref: '#/texts/188'
  - $ref: '#/texts/189'
  - $ref: '#/texts/190'
  - $ref: '#/texts/191'
  - $ref: '#/texts/192'
  - $ref: '#/texts/193'
  - $ref: '#/texts/194'
  - $ref: '#/texts/195'
  - $ref: '#/texts/196'
  - $ref: '#/texts/197'
  - $ref: '#/texts/198'
  - $ref: '#/texts/199'
  - $ref: '#/texts/200'
  - $ref: '#/texts/201'
  - $ref: '#/texts/202'
  - $ref: '#/texts/203'
  - $ref: '#/texts/204'
  - $ref: '#/texts/205'
  - $ref: '#/texts/206'
  - $ref: '#/texts/207'
  - $ref: '#/texts/208'
  - $ref: '#/texts/209'
  - $ref: '#/texts/210'
  - $ref: '#/texts/211'
  - $ref: '#/texts/212'
  - $ref: '#/texts/213'
  - $ref: '#/texts/214'
  - $ref: '#/texts/215'
  - $ref: '#/texts/216'
  - $ref: '#/texts/217'
  - $ref: '#/texts/218'
  - $ref: '#/texts/219'
  - $ref: '#/texts/220'
  - $ref: '#/texts/221'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 489.86871337890625
      coord_origin: BOTTOMLEFT
      l: 119.5811538696289
      r: 503.2312927246094
      t: 691.8293991088867
    charspan:
    - 0
    - 0
    page_no: 13
  references: []
  self_ref: '#/pictures/3'
- annotations: []
  captions:
  - $ref: '#/texts/224'
  children:
  - $ref: '#/texts/224'
  - $ref: '#/texts/225'
  - $ref: '#/texts/226'
  - $ref: '#/texts/227'
  - $ref: '#/texts/228'
  - $ref: '#/texts/229'
  - $ref: '#/texts/230'
  - $ref: '#/texts/231'
  - $ref: '#/texts/232'
  - $ref: '#/texts/233'
  - $ref: '#/texts/234'
  - $ref: '#/texts/235'
  - $ref: '#/texts/236'
  - $ref: '#/texts/237'
  - $ref: '#/texts/238'
  - $ref: '#/texts/239'
  - $ref: '#/texts/240'
  - $ref: '#/texts/241'
  - $ref: '#/texts/242'
  - $ref: '#/texts/243'
  - $ref: '#/texts/244'
  - $ref: '#/texts/245'
  - $ref: '#/texts/246'
  - $ref: '#/texts/247'
  - $ref: '#/texts/248'
  - $ref: '#/texts/249'
  - $ref: '#/texts/250'
  - $ref: '#/texts/251'
  - $ref: '#/texts/252'
  - $ref: '#/texts/253'
  - $ref: '#/texts/254'
  - $ref: '#/texts/255'
  - $ref: '#/texts/256'
  - $ref: '#/texts/257'
  - $ref: '#/texts/258'
  - $ref: '#/texts/259'
  - $ref: '#/texts/260'
  - $ref: '#/texts/261'
  - $ref: '#/texts/262'
  - $ref: '#/texts/263'
  - $ref: '#/texts/264'
  - $ref: '#/texts/265'
  - $ref: '#/texts/266'
  - $ref: '#/texts/267'
  - $ref: '#/texts/268'
  - $ref: '#/texts/269'
  - $ref: '#/texts/270'
  - $ref: '#/texts/271'
  - $ref: '#/texts/272'
  - $ref: '#/texts/273'
  - $ref: '#/texts/274'
  - $ref: '#/texts/275'
  - $ref: '#/texts/276'
  - $ref: '#/texts/277'
  - $ref: '#/texts/278'
  - $ref: '#/texts/279'
  - $ref: '#/texts/280'
  - $ref: '#/texts/281'
  - $ref: '#/texts/282'
  - $ref: '#/texts/283'
  - $ref: '#/texts/284'
  - $ref: '#/texts/285'
  - $ref: '#/texts/286'
  - $ref: '#/texts/287'
  - $ref: '#/texts/288'
  - $ref: '#/texts/289'
  - $ref: '#/texts/290'
  - $ref: '#/texts/291'
  - $ref: '#/texts/292'
  - $ref: '#/texts/293'
  - $ref: '#/texts/294'
  - $ref: '#/texts/295'
  - $ref: '#/texts/296'
  - $ref: '#/texts/297'
  - $ref: '#/texts/298'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 186.96783447265625
      coord_origin: BOTTOMLEFT
      l: 120.5051040649414
      r: 501.6039733886719
      t: 621.9672241210938
    charspan:
    - 0
    - 0
    page_no: 14
  references: []
  self_ref: '#/pictures/4'
- annotations: []
  captions:
  - $ref: '#/texts/301'
  children:
  - $ref: '#/texts/301'
  - $ref: '#/texts/302'
  - $ref: '#/texts/303'
  - $ref: '#/texts/304'
  - $ref: '#/texts/305'
  - $ref: '#/texts/306'
  - $ref: '#/texts/307'
  - $ref: '#/texts/308'
  - $ref: '#/texts/309'
  - $ref: '#/texts/310'
  - $ref: '#/texts/311'
  - $ref: '#/texts/312'
  - $ref: '#/texts/313'
  - $ref: '#/texts/314'
  - $ref: '#/texts/315'
  - $ref: '#/texts/316'
  - $ref: '#/texts/317'
  - $ref: '#/texts/318'
  - $ref: '#/texts/319'
  - $ref: '#/texts/320'
  - $ref: '#/texts/321'
  - $ref: '#/texts/322'
  - $ref: '#/texts/323'
  - $ref: '#/texts/324'
  - $ref: '#/texts/325'
  - $ref: '#/texts/326'
  - $ref: '#/texts/327'
  - $ref: '#/texts/328'
  - $ref: '#/texts/329'
  - $ref: '#/texts/330'
  - $ref: '#/texts/331'
  - $ref: '#/texts/332'
  - $ref: '#/texts/333'
  - $ref: '#/texts/334'
  - $ref: '#/texts/335'
  - $ref: '#/texts/336'
  - $ref: '#/texts/337'
  - $ref: '#/texts/338'
  - $ref: '#/texts/339'
  - $ref: '#/texts/340'
  - $ref: '#/texts/341'
  - $ref: '#/texts/342'
  - $ref: '#/texts/343'
  - $ref: '#/texts/344'
  - $ref: '#/texts/345'
  - $ref: '#/texts/346'
  - $ref: '#/texts/347'
  - $ref: '#/texts/348'
  - $ref: '#/texts/349'
  - $ref: '#/texts/350'
  - $ref: '#/texts/351'
  - $ref: '#/texts/352'
  - $ref: '#/texts/353'
  - $ref: '#/texts/354'
  - $ref: '#/texts/355'
  - $ref: '#/texts/356'
  - $ref: '#/texts/357'
  - $ref: '#/texts/358'
  - $ref: '#/texts/359'
  - $ref: '#/texts/360'
  - $ref: '#/texts/361'
  - $ref: '#/texts/362'
  - $ref: '#/texts/363'
  - $ref: '#/texts/364'
  - $ref: '#/texts/365'
  - $ref: '#/texts/366'
  - $ref: '#/texts/367'
  - $ref: '#/texts/368'
  - $ref: '#/texts/369'
  - $ref: '#/texts/370'
  - $ref: '#/texts/371'
  - $ref: '#/texts/372'
  - $ref: '#/texts/373'
  - $ref: '#/texts/374'
  - $ref: '#/texts/375'
  content_layer: body
  footnotes: []
  label: picture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 201.849609375
      coord_origin: BOTTOMLEFT
      l: 120.37181091308594
      r: 499.4662170410156
      t: 607.6979675292969
    charspan:
    - 0
    - 0
    page_no: 15
  references: []
  self_ref: '#/pictures/5'
schema_name: DoclingDocument
tables:
- annotations: []
  captions:
  - $ref: '#/texts/116'
  children:
  - $ref: '#/texts/116'
  content_layer: body
  data:
    grid:
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 0
        text: ''
      - bbox:
          b: 137.2548828125
          coord_origin: TOPLEFT
          l: 339.83770751953125
          r: 382.8064270019531
          t: 128.29852294921875
        col_span: 1
        column_header: true
        end_col_offset_idx: 3
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: Operations
      - bbox:
          b: 126.3558349609375
          coord_origin: TOPLEFT
          l: 124.66654968261719
          r: 487.3251037597656
          t: 117.3895263671875
        col_span: 1
        column_header: true
        end_col_offset_idx: 4
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 0
        text: Layer Type Complexity per Layer Sequential Maximum Path Length
    - - bbox:
          b: 147.8695068359375
          coord_origin: TOPLEFT
          l: 124.96542358398438
          r: 181.403564453125
          t: 140.935546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: Self-Attention
      - bbox:
          b: 150.220703125
          coord_origin: TOPLEFT
          l: 264.8841552734375
          r: 281.83563232421875
          t: 140.26806640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 1
        text: "O(n 2 \xB7  d)"
      - bbox:
          b: 150.220703125
          coord_origin: TOPLEFT
          l: 351.54217529296875
          r: 370.656005859375
          t: 140.26806640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 1
        text: O(1)
      - bbox:
          b: 150.220703125
          coord_origin: TOPLEFT
          l: 431.4961853027344
          r: 450.6100158691406
          t: 140.26806640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 1
        text: O(1)
    - - bbox:
          b: 159.212646484375
          coord_origin: TOPLEFT
          l: 124.71639251708984
          r: 163.8395233154297
          t: 152.52777099609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: Recurrent
      - bbox:
          b: 161.60369873046875
          coord_origin: TOPLEFT
          l: 264.8841857910156
          r: 281.835693359375
          t: 151.65106201171875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 2
        text: "O(n  \xB7 d 2 )"
      - bbox:
          b: 161.60369873046875
          coord_origin: TOPLEFT
          l: 351.04217529296875
          r: 371.1642150878906
          t: 151.65106201171875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 2
        text: O(n)
      - bbox:
          b: 161.60369873046875
          coord_origin: TOPLEFT
          l: 430.9971923828125
          r: 451.11822509765625
          t: 151.65106201171875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 2
        text: O(n)
    - - bbox:
          b: 170.634521484375
          coord_origin: TOPLEFT
          l: 124.82601165771484
          r: 180.75604248046875
          t: 163.7005615234375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: Convolutional
      - bbox:
          b: 172.9857177734375
          coord_origin: TOPLEFT
          l: 258.5372314453125
          r: 279.4258728027344
          t: 163.0330810546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 3
        text: "O(k \xB7  n  \xB7 d 2 )"
      - bbox:
          b: 172.9857177734375
          coord_origin: TOPLEFT
          l: 351.5422058105469
          r: 370.6560363769531
          t: 163.0330810546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 3
        text: O(1)
      - bbox:
          b: 172.9857177734375
          coord_origin: TOPLEFT
          l: 418.2972106933594
          r: 463.80975341796875
          t: 163.0330810546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 3
        text: O(logk(n))
    - - bbox:
          b: 183.16741943359375
          coord_origin: TOPLEFT
          l: 124.96548461914062
          r: 227.27142333984375
          t: 174.60955810546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: Self-Attention (restricted)
      - bbox:
          b: 183.89471435546875
          coord_origin: TOPLEFT
          l: 261.13623046875
          r: 276.74176025390625
          t: 173.94207763671875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 4
        text: "O(r  \xB7  n  \xB7 d)"
      - bbox:
          b: 183.89471435546875
          coord_origin: TOPLEFT
          l: 351.5422058105469
          r: 370.6560363769531
          t: 173.94207763671875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 4
        text: O(1)
      - bbox:
          b: 183.89471435546875
          coord_origin: TOPLEFT
          l: 426.1212158203125
          r: 455.9952392578125
          t: 173.94207763671875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 4
        text: O(n/r)
    num_cols: 4
    num_rows: 5
    table_cells:
    - bbox:
        b: 126.3558349609375
        coord_origin: TOPLEFT
        l: 124.66654968261719
        r: 487.3251037597656
        t: 117.3895263671875
      col_span: 1
      column_header: true
      end_col_offset_idx: 4
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 0
      text: Layer Type Complexity per Layer Sequential Maximum Path Length
    - bbox:
        b: 137.2548828125
        coord_origin: TOPLEFT
        l: 339.83770751953125
        r: 382.8064270019531
        t: 128.29852294921875
      col_span: 1
      column_header: true
      end_col_offset_idx: 3
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 0
      text: Operations
    - bbox:
        b: 147.8695068359375
        coord_origin: TOPLEFT
        l: 124.96542358398438
        r: 181.403564453125
        t: 140.935546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: Self-Attention
    - bbox:
        b: 150.220703125
        coord_origin: TOPLEFT
        l: 264.8841552734375
        r: 281.83563232421875
        t: 140.26806640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 1
      text: "O(n 2 \xB7  d)"
    - bbox:
        b: 150.220703125
        coord_origin: TOPLEFT
        l: 351.54217529296875
        r: 370.656005859375
        t: 140.26806640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 1
      text: O(1)
    - bbox:
        b: 150.220703125
        coord_origin: TOPLEFT
        l: 431.4961853027344
        r: 450.6100158691406
        t: 140.26806640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 1
      text: O(1)
    - bbox:
        b: 159.212646484375
        coord_origin: TOPLEFT
        l: 124.71639251708984
        r: 163.8395233154297
        t: 152.52777099609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: Recurrent
    - bbox:
        b: 161.60369873046875
        coord_origin: TOPLEFT
        l: 264.8841857910156
        r: 281.835693359375
        t: 151.65106201171875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 2
      text: "O(n  \xB7 d 2 )"
    - bbox:
        b: 161.60369873046875
        coord_origin: TOPLEFT
        l: 351.04217529296875
        r: 371.1642150878906
        t: 151.65106201171875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 2
      text: O(n)
    - bbox:
        b: 161.60369873046875
        coord_origin: TOPLEFT
        l: 430.9971923828125
        r: 451.11822509765625
        t: 151.65106201171875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 2
      text: O(n)
    - bbox:
        b: 170.634521484375
        coord_origin: TOPLEFT
        l: 124.82601165771484
        r: 180.75604248046875
        t: 163.7005615234375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: Convolutional
    - bbox:
        b: 172.9857177734375
        coord_origin: TOPLEFT
        l: 258.5372314453125
        r: 279.4258728027344
        t: 163.0330810546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 3
      text: "O(k \xB7  n  \xB7 d 2 )"
    - bbox:
        b: 172.9857177734375
        coord_origin: TOPLEFT
        l: 351.5422058105469
        r: 370.6560363769531
        t: 163.0330810546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 3
      text: O(1)
    - bbox:
        b: 172.9857177734375
        coord_origin: TOPLEFT
        l: 418.2972106933594
        r: 463.80975341796875
        t: 163.0330810546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 3
      text: O(logk(n))
    - bbox:
        b: 183.16741943359375
        coord_origin: TOPLEFT
        l: 124.96548461914062
        r: 227.27142333984375
        t: 174.60955810546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: Self-Attention (restricted)
    - bbox:
        b: 183.89471435546875
        coord_origin: TOPLEFT
        l: 261.13623046875
        r: 276.74176025390625
        t: 173.94207763671875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 4
      text: "O(r  \xB7  n  \xB7 d)"
    - bbox:
        b: 183.89471435546875
        coord_origin: TOPLEFT
        l: 351.5422058105469
        r: 370.6560363769531
        t: 173.94207763671875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 4
      text: O(1)
    - bbox:
        b: 183.89471435546875
        coord_origin: TOPLEFT
        l: 426.1212158203125
        r: 455.9952392578125
        t: 173.94207763671875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 4
      text: O(n/r)
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 604.8668060302734
      coord_origin: BOTTOMLEFT
      l: 117.51346588134766
      r: 493.74737548828125
      t: 679.4528503417969
    charspan:
    - 0
    - 0
    page_no: 6
  references: []
  self_ref: '#/tables/0'
- annotations: []
  captions:
  - $ref: '#/texts/145'
  children:
  - $ref: '#/texts/145'
  content_layer: body
  data:
    grid:
    - - bbox:
          b: 113.84765625
          coord_origin: TOPLEFT
          l: 136.79055786132812
          r: 162.47413635253906
          t: 106.95355224609375
        col_span: 1
        column_header: true
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: Model
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 0
        text: ''
      - bbox:
          b: 107.630859375
          coord_origin: TOPLEFT
          l: 311.19036865234375
          r: 474.845947265625
          t: 98.66455078125
        col_span: 2
        column_header: true
        end_col_offset_idx: 4
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: BLEU Training Cost (FLOPs)
      - bbox:
          b: 107.630859375
          coord_origin: TOPLEFT
          l: 311.19036865234375
          r: 474.845947265625
          t: 98.66455078125
        col_span: 2
        column_header: true
        end_col_offset_idx: 4
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: BLEU Training Cost (FLOPs)
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 1
        text: ''
      - bbox:
          b: 121.48358154296875
          coord_origin: TOPLEFT
          l: 288.83953857421875
          r: 468.74420166015625
          t: 114.78875732421875
        col_span: 1
        column_header: true
        end_col_offset_idx: 4
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 1
        text: EN-DE EN-FR EN-DE EN-FR
    - - bbox:
          b: 134.85284423828125
          coord_origin: TOPLEFT
          l: 136.84036254882812
          r: 314.2443542480469
          t: 125.83673095703125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: ByteNet [18] 23.75
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 2
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 2
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 2
        text: ''
    - - bbox:
          b: 146.22589111328125
          coord_origin: TOPLEFT
          l: 136.83041381835938
          r: 353.4073486328125
          t: 137.26953125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: Deep-Att + PosUnk [39] 39.2
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 3
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 3
        text: ''
      - bbox:
          b: 144.073974609375
          coord_origin: TOPLEFT
          l: 436.150634765625
          r: 439.43829345703125
          t: 137.43890380859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 3
        text: "1 . 0  \xB7  10 20"
    - - bbox:
          b: 157.0001220703125
          coord_origin: TOPLEFT
          l: 136.9897918701172
          r: 355.89794921875
          t: 148.64154052734375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: GNMT + RL [38] 24.6 39.92
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 4
        text: ''
      - bbox:
          b: 155.4559326171875
          coord_origin: TOPLEFT
          l: 383.7431335449219
          r: 387.71820068359375
          t: 148.82086181640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 4
        text: "2 . 3  \xB7  10 19"
      - bbox:
          b: 155.4559326171875
          coord_origin: TOPLEFT
          l: 436.1506652832031
          r: 439.4383239746094
          t: 148.82086181640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 4
        text: "1 . 4  \xB7  10 20"
    - - bbox:
          b: 168.38311767578125
          coord_origin: TOPLEFT
          l: 136.94996643066406
          r: 355.8282165527344
          t: 159.98468017578125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: ConvS2S [9] 25.16 40.46
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 5
        text: ''
      - bbox:
          b: 167.04815673828125
          coord_origin: TOPLEFT
          l: 383.6634521484375
          r: 387.7979431152344
          t: 160.203857421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 5
        text: "9 . 6  \xB7  10 18"
      - bbox:
          b: 166.83892822265625
          coord_origin: TOPLEFT
          l: 436.15069580078125
          r: 439.4383544921875
          t: 160.203857421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 5
        text: "1 . 5  \xB7  10 20"
    - - bbox:
          b: 179.76507568359375
          coord_origin: TOPLEFT
          l: 136.7906036376953
          r: 355.8282470703125
          t: 171.36663818359375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 7
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 6
        text: MoE [32] 26.03 40.56
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 6
        text: ''
      - bbox:
          b: 178.22088623046875
          coord_origin: TOPLEFT
          l: 383.7431945800781
          r: 387.71826171875
          t: 171.5858154296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 6
        text: "2 . 0  \xB7  10 19"
      - bbox:
          b: 178.22088623046875
          coord_origin: TOPLEFT
          l: 436.1517028808594
          r: 439.4393615722656
          t: 171.5858154296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 6
        text: "1 . 2  \xB7  10 20"
    - - bbox:
          b: 193.01092529296875
          coord_origin: TOPLEFT
          l: 136.83041381835938
          r: 353.37744140625
          t: 184.0545654296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 8
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 7
        text: Deep-Att + PosUnk Ensemble [39] 40.4
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 7
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 7
        text: ''
      - bbox:
          b: 191.0682373046875
          coord_origin: TOPLEFT
          l: 435.6824035644531
          r: 439.81689453125
          t: 184.22393798828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 7
        text: "8 . 0  \xB7  10 20"
    - - bbox:
          b: 203.78515625
          coord_origin: TOPLEFT
          l: 136.9897918701172
          r: 355.82818603515625
          t: 195.42657470703125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 9
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 8
        text: GNMT + RL Ensemble [38] 26.30 41.16
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 8
        text: ''
      - bbox:
          b: 202.240966796875
          coord_origin: TOPLEFT
          l: 384.13165283203125
          r: 387.4193115234375
          t: 195.60589599609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 8
        text: "1 . 8  \xB7  10 20"
      - bbox:
          b: 202.240966796875
          coord_origin: TOPLEFT
          l: 436.1506652832031
          r: 439.4383239746094
          t: 195.60589599609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 8
        text: "1 . 1  \xB7  10 21"
    - - bbox:
          b: 215.16717529296875
          coord_origin: TOPLEFT
          l: 136.94996643066406
          r: 314.543212890625
          t: 206.80859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 10
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 9
        text: ConvS2S Ensemble [9] 26.36
      - bbox:
          b: 213.7425537109375
          coord_origin: TOPLEFT
          l: 333.9253234863281
          r: 355.88287353515625
          t: 206.76873779296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 9
        text: '41.29'
      - bbox:
          b: 213.83221435546875
          coord_origin: TOPLEFT
          l: 383.80291748046875
          r: 388.0768737792969
          t: 206.88824462890625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 9
        text: "7 . 7  \xB7  10 19"
      - bbox:
          b: 213.62298583984375
          coord_origin: TOPLEFT
          l: 436.15069580078125
          r: 439.4383544921875
          t: 206.9879150390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 9
        text: "1 . 2  \xB7  10 21"
    - - bbox:
          b: 228.51239013671875
          coord_origin: TOPLEFT
          l: 136.84036254882812
          r: 352.600341796875
          t: 219.95452880859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 11
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 10
        text: Transformer (base model) 27.3 38.1
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 10
        text: ''
      - bbox:
          b: 226.85858154296875
          coord_origin: TOPLEFT
          l: 407.81719970703125
          r: 412.5793151855469
          t: 220.23345947265625
        col_span: 2
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 10
        text: "3 . 3 \xB7  10 18"
      - bbox:
          b: 226.85858154296875
          coord_origin: TOPLEFT
          l: 407.81719970703125
          r: 412.5793151855469
          t: 220.23345947265625
        col_span: 2
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 10
        text: "3 . 3 \xB7  10 18"
    - - bbox:
          b: 240.3028564453125
          coord_origin: TOPLEFT
          l: 136.84034729003906
          r: 207.4951171875
          t: 231.3365478515625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 12
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 11
        text: Transformer (big)
      - bbox:
          b: 238.26055908203125
          coord_origin: TOPLEFT
          l: 295.1103515625
          r: 353.381591796875
          t: 231.2867431640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 11
        text: 28.4 41.8
      - bbox:
          b: 238.1409912109375
          coord_origin: TOPLEFT
          l: 410.6211242675781
          r: 414.59619140625
          t: 231.50592041015625
        col_span: 2
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 11
        text: "2 . 3  \xB7  10 19"
      - bbox:
          b: 238.1409912109375
          coord_origin: TOPLEFT
          l: 410.6211242675781
          r: 414.59619140625
          t: 231.50592041015625
        col_span: 2
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 11
        text: "2 . 3  \xB7  10 19"
    num_cols: 4
    num_rows: 12
    table_cells:
    - bbox:
        b: 113.84765625
        coord_origin: TOPLEFT
        l: 136.79055786132812
        r: 162.47413635253906
        t: 106.95355224609375
      col_span: 1
      column_header: true
      end_col_offset_idx: 1
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 0
      text: Model
    - bbox:
        b: 107.630859375
        coord_origin: TOPLEFT
        l: 311.19036865234375
        r: 474.845947265625
        t: 98.66455078125
      col_span: 2
      column_header: true
      end_col_offset_idx: 4
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 0
      text: BLEU Training Cost (FLOPs)
    - bbox:
        b: 121.48358154296875
        coord_origin: TOPLEFT
        l: 288.83953857421875
        r: 468.74420166015625
        t: 114.78875732421875
      col_span: 1
      column_header: true
      end_col_offset_idx: 4
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 1
      text: EN-DE EN-FR EN-DE EN-FR
    - bbox:
        b: 134.85284423828125
        coord_origin: TOPLEFT
        l: 136.84036254882812
        r: 314.2443542480469
        t: 125.83673095703125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: ByteNet [18] 23.75
    - bbox:
        b: 146.22589111328125
        coord_origin: TOPLEFT
        l: 136.83041381835938
        r: 353.4073486328125
        t: 137.26953125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: Deep-Att + PosUnk [39] 39.2
    - bbox:
        b: 144.073974609375
        coord_origin: TOPLEFT
        l: 436.150634765625
        r: 439.43829345703125
        t: 137.43890380859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 3
      text: "1 . 0  \xB7  10 20"
    - bbox:
        b: 157.0001220703125
        coord_origin: TOPLEFT
        l: 136.9897918701172
        r: 355.89794921875
        t: 148.64154052734375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: GNMT + RL [38] 24.6 39.92
    - bbox:
        b: 155.4559326171875
        coord_origin: TOPLEFT
        l: 383.7431335449219
        r: 387.71820068359375
        t: 148.82086181640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 4
      text: "2 . 3  \xB7  10 19"
    - bbox:
        b: 155.4559326171875
        coord_origin: TOPLEFT
        l: 436.1506652832031
        r: 439.4383239746094
        t: 148.82086181640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 4
      text: "1 . 4  \xB7  10 20"
    - bbox:
        b: 168.38311767578125
        coord_origin: TOPLEFT
        l: 136.94996643066406
        r: 355.8282165527344
        t: 159.98468017578125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 6
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 5
      text: ConvS2S [9] 25.16 40.46
    - bbox:
        b: 167.04815673828125
        coord_origin: TOPLEFT
        l: 383.6634521484375
        r: 387.7979431152344
        t: 160.203857421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 5
      text: "9 . 6  \xB7  10 18"
    - bbox:
        b: 166.83892822265625
        coord_origin: TOPLEFT
        l: 436.15069580078125
        r: 439.4383544921875
        t: 160.203857421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 5
      text: "1 . 5  \xB7  10 20"
    - bbox:
        b: 179.76507568359375
        coord_origin: TOPLEFT
        l: 136.7906036376953
        r: 355.8282470703125
        t: 171.36663818359375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 7
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 6
      text: MoE [32] 26.03 40.56
    - bbox:
        b: 178.22088623046875
        coord_origin: TOPLEFT
        l: 383.7431945800781
        r: 387.71826171875
        t: 171.5858154296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 6
      text: "2 . 0  \xB7  10 19"
    - bbox:
        b: 178.22088623046875
        coord_origin: TOPLEFT
        l: 436.1517028808594
        r: 439.4393615722656
        t: 171.5858154296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 6
      text: "1 . 2  \xB7  10 20"
    - bbox:
        b: 193.01092529296875
        coord_origin: TOPLEFT
        l: 136.83041381835938
        r: 353.37744140625
        t: 184.0545654296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 8
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 7
      text: Deep-Att + PosUnk Ensemble [39] 40.4
    - bbox:
        b: 191.0682373046875
        coord_origin: TOPLEFT
        l: 435.6824035644531
        r: 439.81689453125
        t: 184.22393798828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 7
      text: "8 . 0  \xB7  10 20"
    - bbox:
        b: 203.78515625
        coord_origin: TOPLEFT
        l: 136.9897918701172
        r: 355.82818603515625
        t: 195.42657470703125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 9
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 8
      text: GNMT + RL Ensemble [38] 26.30 41.16
    - bbox:
        b: 202.240966796875
        coord_origin: TOPLEFT
        l: 384.13165283203125
        r: 387.4193115234375
        t: 195.60589599609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 8
      text: "1 . 8  \xB7  10 20"
    - bbox:
        b: 202.240966796875
        coord_origin: TOPLEFT
        l: 436.1506652832031
        r: 439.4383239746094
        t: 195.60589599609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 8
      text: "1 . 1  \xB7  10 21"
    - bbox:
        b: 215.16717529296875
        coord_origin: TOPLEFT
        l: 136.94996643066406
        r: 314.543212890625
        t: 206.80859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 10
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 9
      text: ConvS2S Ensemble [9] 26.36
    - bbox:
        b: 213.7425537109375
        coord_origin: TOPLEFT
        l: 333.9253234863281
        r: 355.88287353515625
        t: 206.76873779296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 9
      text: '41.29'
    - bbox:
        b: 213.83221435546875
        coord_origin: TOPLEFT
        l: 383.80291748046875
        r: 388.0768737792969
        t: 206.88824462890625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 9
      text: "7 . 7  \xB7  10 19"
    - bbox:
        b: 213.62298583984375
        coord_origin: TOPLEFT
        l: 436.15069580078125
        r: 439.4383544921875
        t: 206.9879150390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 9
      text: "1 . 2  \xB7  10 21"
    - bbox:
        b: 228.51239013671875
        coord_origin: TOPLEFT
        l: 136.84036254882812
        r: 352.600341796875
        t: 219.95452880859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 11
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 10
      text: Transformer (base model) 27.3 38.1
    - bbox:
        b: 226.85858154296875
        coord_origin: TOPLEFT
        l: 407.81719970703125
        r: 412.5793151855469
        t: 220.23345947265625
      col_span: 2
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 10
      text: "3 . 3 \xB7  10 18"
    - bbox:
        b: 240.3028564453125
        coord_origin: TOPLEFT
        l: 136.84034729003906
        r: 207.4951171875
        t: 231.3365478515625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 12
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 11
      text: Transformer (big)
    - bbox:
        b: 238.26055908203125
        coord_origin: TOPLEFT
        l: 295.1103515625
        r: 353.381591796875
        t: 231.2867431640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 11
      text: 28.4 41.8
    - bbox:
        b: 238.1409912109375
        coord_origin: TOPLEFT
        l: 410.6211242675781
        r: 414.59619140625
        t: 231.50592041015625
      col_span: 2
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 11
      text: "2 . 3  \xB7  10 19"
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 547.9623870849609
      coord_origin: BOTTOMLEFT
      l: 130.73477172851562
      r: 482.0496826171875
      t: 696.6281204223633
    charspan:
    - 0
    - 0
    page_no: 8
  references: []
  self_ref: '#/tables/1'
- annotations: []
  captions:
  - $ref: '#/texts/158'
  children:
  - $ref: '#/texts/158'
  content_layer: body
  data:
    grid:
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: ''
      - bbox:
          b: 144.59161376953125
          coord_origin: TOPLEFT
          l: 146.51553344726562
          r: 172.3087158203125
          t: 137.57794189453125
        col_span: 1
        column_header: true
        end_col_offset_idx: 2
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 0
        text: N d
      - bbox:
          b: 144.59161376953125
          coord_origin: TOPLEFT
          l: 207.53050231933594
          r: 212.272705078125
          t: 137.57794189453125
        col_span: 1
        column_header: true
        end_col_offset_idx: 3
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: d ff
      - bbox:
          b: 144.59161376953125
          coord_origin: TOPLEFT
          l: 236.78594970703125
          r: 263.615234375
          t: 137.57794189453125
        col_span: 1
        column_header: true
        end_col_offset_idx: 4
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 0
        text: h d k
      - bbox:
          b: 144.49200439453125
          coord_origin: TOPLEFT
          l: 310.24151611328125
          r: 317.3548278808594
          t: 137.68756103515625
        col_span: 1
        column_header: true
        end_col_offset_idx: 5
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 0
        text: Pd Pdrop
      - bbox:
          b: 146.04974365234375
          coord_origin: TOPLEFT
          l: 346.0562438964844
          r: 355.4690856933594
          t: 140.1981201171875
        col_span: 1
        column_header: true
        end_col_offset_idx: 6
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 0
        text: "\u03F5ls"
      - bbox:
          b: 146.04974365234375
          coord_origin: TOPLEFT
          l: 172.46958923339844
          r: 389.2510070800781
          t: 132.23358154296875
        col_span: 1
        column_header: true
        end_col_offset_idx: 7
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 0
        text: model  d v  train  steps
      - bbox:
          b: 152.17340087890625
          coord_origin: TOPLEFT
          l: 403.7712097167969
          r: 458.9341125488281
          t: 143.61553955078125
        col_span: 1
        column_header: true
        end_col_offset_idx: 8
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 0
        text: (dev) (dev)
      - bbox:
          b: 141.18994140625
          coord_origin: TOPLEFT
          l: 405.2554016113281
          r: 502.35089111328125
          t: 132.44281005859375
        col_span: 1
        column_header: true
        end_col_offset_idx: 9
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 0
        text: "PPL BLEU params \xD710 6"
    - - bbox:
          b: 163.14764404296875
          coord_origin: TOPLEFT
          l: 116.49788665771484
          r: 133.9722900390625
          t: 156.2535400390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: base
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 1
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 1
        text: ''
      - bbox:
          b: 163.1875
          coord_origin: TOPLEFT
          l: 148.52073669433594
          r: 391.3490905761719
          t: 156.2037353515625
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 1
        text: 6 512 2048 8 64 64 0.1 0.1 100K
      - bbox:
          b: 163.2672119140625
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 492.7325134277344
          t: 156.2037353515625
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 1
        text: 4.92 25.8 65
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 1
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 2
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 2
        text: ''
      - bbox:
          b: 175.82452392578125
          coord_origin: TOPLEFT
          l: 237.7228546142578
          r: 297.637939453125
          t: 168.84075927734375
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 2
        text: 1 512 512
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 2
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 2
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 2
        text: ''
      - bbox:
          b: 175.90423583984375
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 457.33538818359375
          t: 168.84075927734375
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 2
        text: 5.29 24.9
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 2
        text: ''
    - - bbox:
          b: 193.81243896484375
          coord_origin: TOPLEFT
          l: 118.88420104980469
          r: 131.75588989257812
          t: 185.32427978515625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: (A)
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 3
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 3
        text: ''
      - bbox:
          b: 186.7344970703125
          coord_origin: TOPLEFT
          l: 236.73655700683594
          r: 297.33905029296875
          t: 179.8702392578125
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 3
        text: 4 128 128
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 3
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 3
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 3
        text: ''
      - bbox:
          b: 186.7344970703125
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 457.1261901855469
          t: 179.750732421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 3
        text: 5.00 25.5
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 3
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 4
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 4
        text: ''
      - bbox:
          b: 197.64349365234375
          coord_origin: TOPLEFT
          l: 235.23284912109375
          r: 295.1479187011719
          t: 190.6995849609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 4
        text: 16 32 32
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 4
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 4
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 4
        text: ''
      - bbox:
          b: 197.72320556640625
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 457.1959228515625
          t: 190.65972900390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 4
        text: 4.91 25.8
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 4
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 5
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 5
        text: ''
      - bbox:
          b: 208.552490234375
          coord_origin: TOPLEFT
          l: 234.55538940429688
          r: 295.07818603515625
          t: 201.60858154296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 5
        text: 32 16 16
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 5
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 5
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 5
        text: ''
      - bbox:
          b: 208.552490234375
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 457.46490478515625
          t: 201.5687255859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 5
        text: 5.01 25.4
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 5
        text: ''
    - - bbox:
          b: 228.2684326171875
          coord_origin: TOPLEFT
          l: 119.158203125
          r: 131.48193359375
          t: 219.7802734375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 7
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 6
        text: (B)
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 6
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 6
        text: ''
      - bbox:
          b: 221.18951416015625
          coord_origin: TOPLEFT
          l: 259.6408386230469
          r: 268.1788024902344
          t: 214.24560546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 6
        text: '16'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 6
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 6
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 6
        text: ''
      - bbox:
          b: 221.18951416015625
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 492.80224609375
          t: 214.20574951171875
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 6
        text: 5.16 25.1 58
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 6
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 7
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 7
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 7
        text: ''
      - bbox:
          b: 232.0985107421875
          coord_origin: TOPLEFT
          l: 258.9634094238281
          r: 268.24853515625
          t: 225.2342529296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 7
        text: '32'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 7
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 7
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 7
        text: ''
      - bbox:
          b: 232.0985107421875
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 493.111083984375
          t: 225.11474609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 7
        text: 5.01 25.4 60
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 7
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 8
        text: ''
      - bbox:
          b: 244.60699462890625
          coord_origin: TOPLEFT
          l: 148.4808807373047
          r: 152.91424560546875
          t: 237.87225341796875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 8
        text: '2'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 8
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 8
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 8
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 8
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 8
        text: ''
      - bbox:
          b: 244.73651123046875
          coord_origin: TOPLEFT
          l: 405.30072021484375
          r: 493.0314025878906
          t: 237.7926025390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 8
        text: 6.11 23.7 36
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 8
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 9
        text: ''
      - bbox:
          b: 255.5159912109375
          coord_origin: TOPLEFT
          l: 148.3015594482422
          r: 152.8843536376953
          t: 248.78125
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 9
        text: '4'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 9
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 9
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 9
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 9
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 9
        text: ''
      - bbox:
          b: 255.7252197265625
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 493.111083984375
          t: 248.6617431640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 9
        text: 5.19 25.3 50
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 9
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 10
        text: ''
      - bbox:
          b: 266.55450439453125
          coord_origin: TOPLEFT
          l: 148.7399139404297
          r: 152.6153564453125
          t: 259.69024658203125
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 10
        text: '8'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 10
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 10
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 10
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 10
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 10
        text: ''
      - bbox:
          b: 266.55450439453125
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 493.111083984375
          t: 259.57073974609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 10
        text: 4.88 25.5 80
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 10
        text: ''
    - - bbox:
          b: 279.08740234375
          coord_origin: TOPLEFT
          l: 119.158203125
          r: 131.48193359375
          t: 270.5992431640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 12
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 11
        text: (C)
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 11
        text: ''
      - bbox:
          b: 277.4635009765625
          coord_origin: TOPLEFT
          l: 171.55886840820312
          r: 295.1449279785156
          t: 270.479736328125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 11
        text: 256 32 32
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 11
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 11
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 11
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 11
        text: ''
      - bbox:
          b: 277.4635009765625
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 492.80224609375
          t: 270.479736328125
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 11
        text: 5.75 24.5 28
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 11
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 12
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 12
        text: ''
      - bbox:
          b: 288.3725280761719
          coord_origin: TOPLEFT
          l: 169.87484741210938
          r: 297.33636474609375
          t: 281.50830078125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 12
        text: 1024 128 128
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 12
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 12
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 12
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 12
        text: ''
      - bbox:
          b: 288.3725280761719
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 495.29290771484375
          t: 281.4285888671875
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 12
        text: 4.66 26.0 168
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 12
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 13
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 13
        text: ''
      - bbox:
          b: 299.2815246582031
          coord_origin: TOPLEFT
          l: 203.3518524169922
          r: 221.89224243164062
          t: 292.41729736328125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 13
        text: '1024'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 13
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 13
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 13
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 13
        text: ''
      - bbox:
          b: 299.2815246582031
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 492.6727294921875
          t: 292.2977294921875
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 13
        text: 5.12 25.4 53
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 13
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 14
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 14
        text: ''
      - bbox:
          b: 310.2702331542969
          coord_origin: TOPLEFT
          l: 202.3655548095703
          r: 221.85240173339844
          t: 303.24658203125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 14
        text: '4096'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 14
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 14
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 14
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 14
        text: ''
      - bbox:
          b: 310.2702331542969
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 493.111083984375
          t: 303.20672607421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 14
        text: 4.75 26.2 90
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 14
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 15
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 15
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 15
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 15
        text: ''
      - bbox:
          b: 322.8285217285156
          coord_origin: TOPLEFT
          l: 315.35211181640625
          r: 327.3271484375
          t: 315.96429443359375
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 15
        text: '0.0'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 15
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 15
        text: ''
      - bbox:
          b: 322.8285217285156
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 457.425048828125
          t: 315.8447265625
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 15
        text: 5.77 24.6
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 15
        text: ''
    - - bbox:
          b: 340.81640625
          coord_origin: TOPLEFT
          l: 118.88420104980469
          r: 131.75588989257812
          t: 332.3282775878906
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 17
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 16
        text: (D)
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 16
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 16
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 16
        text: ''
      - bbox:
          b: 333.7375183105469
          coord_origin: TOPLEFT
          l: 315.35211181640625
          r: 327.31719970703125
          t: 326.873291015625
        col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 16
        text: '0.2'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 16
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 16
        text: ''
      - bbox:
          b: 333.8172302246094
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 457.1261901855469
          t: 326.75372314453125
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 16
        text: 4.95 25.5
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 16
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 17
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 17
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 17
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 17
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 17
        text: ''
      - bbox:
          b: 344.6465148925781
          coord_origin: TOPLEFT
          l: 345.0310974121094
          r: 357.0061340332031
          t: 337.78228759765625
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 17
        text: '0.0'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 17
        text: ''
      - bbox:
          b: 344.6465148925781
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 457.06640625
          t: 337.6627197265625
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 17
        text: 4.67 25.3
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 17
        text: ''
    - - col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 18
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 18
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 18
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 18
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 18
        text: ''
      - bbox:
          b: 355.5555114746094
          coord_origin: TOPLEFT
          l: 345.0310974121094
          r: 356.9961853027344
          t: 348.6912841796875
        col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 18
        text: '0.2'
      - col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 18
        text: ''
      - bbox:
          b: 355.5555114746094
          coord_origin: TOPLEFT
          l: 405.28082275390625
          r: 457.23577880859375
          t: 348.57171630859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 18
        text: 5.47 25.7
      - col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 18
        text: ''
    - - bbox:
          b: 369.8174133300781
          coord_origin: TOPLEFT
          l: 119.43720245361328
          r: 131.20303344726562
          t: 361.32928466796875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 20
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 19
        text: (E)
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
      - bbox:
          b: 368.2732238769531
          coord_origin: TOPLEFT
          l: 405.08154296875
          r: 457.23577880859375
          t: 361.209716796875
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 19
        text: 4.92 25.7
      - bbox:
          b: 370.22589111328125
          coord_origin: TOPLEFT
          l: 178.68382263183594
          r: 345.37799072265625
          t: 361.2595520019531
        col_span: 8
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 19
        text: positional embedding instead of sinusoids
    - - bbox:
          b: 382.8628845214844
          coord_origin: TOPLEFT
          l: 118.98388671875
          r: 131.3873291015625
          t: 373.89654541015625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 21
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 20
        text: big
      - col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 20
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 20
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 4
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 3
        start_row_offset_idx: 20
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 5
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 4
        start_row_offset_idx: 20
        text: ''
      - col_span: 1
        column_header: false
        end_col_offset_idx: 6
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 5
        start_row_offset_idx: 20
        text: ''
      - bbox:
          b: 380.91021728515625
          coord_origin: TOPLEFT
          l: 148.52073669433594
          r: 391.3490905761719
          t: 373.8865661621094
        col_span: 1
        column_header: false
        end_col_offset_idx: 7
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 6
        start_row_offset_idx: 20
        text: 6 1024 4096 16 0.3 300K
      - bbox:
          b: 380.83050537109375
          coord_origin: TOPLEFT
          l: 405.15130615234375
          r: 457.49481201171875
          t: 373.8467102050781
        col_span: 1
        column_header: false
        end_col_offset_idx: 8
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 7
        start_row_offset_idx: 20
        text: 4.33 26.4
      - bbox:
          b: 380.83050537109375
          coord_origin: TOPLEFT
          l: 481.2008972167969
          r: 495.16845703125
          t: 373.9662780761719
        col_span: 1
        column_header: false
        end_col_offset_idx: 9
        end_row_offset_idx: 21
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 8
        start_row_offset_idx: 20
        text: '213'
    num_cols: 9
    num_rows: 21
    table_cells:
    - bbox:
        b: 144.59161376953125
        coord_origin: TOPLEFT
        l: 146.51553344726562
        r: 172.3087158203125
        t: 137.57794189453125
      col_span: 1
      column_header: true
      end_col_offset_idx: 2
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 0
      text: N d
    - bbox:
        b: 146.04974365234375
        coord_origin: TOPLEFT
        l: 172.46958923339844
        r: 389.2510070800781
        t: 132.23358154296875
      col_span: 1
      column_header: true
      end_col_offset_idx: 7
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 0
      text: model  d v  train  steps
    - bbox:
        b: 144.59161376953125
        coord_origin: TOPLEFT
        l: 207.53050231933594
        r: 212.272705078125
        t: 137.57794189453125
      col_span: 1
      column_header: true
      end_col_offset_idx: 3
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 0
      text: d ff
    - bbox:
        b: 144.59161376953125
        coord_origin: TOPLEFT
        l: 236.78594970703125
        r: 263.615234375
        t: 137.57794189453125
      col_span: 1
      column_header: true
      end_col_offset_idx: 4
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 0
      text: h d k
    - bbox:
        b: 144.49200439453125
        coord_origin: TOPLEFT
        l: 310.24151611328125
        r: 317.3548278808594
        t: 137.68756103515625
      col_span: 1
      column_header: true
      end_col_offset_idx: 5
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 0
      text: Pd Pdrop
    - bbox:
        b: 146.04974365234375
        coord_origin: TOPLEFT
        l: 346.0562438964844
        r: 355.4690856933594
        t: 140.1981201171875
      col_span: 1
      column_header: true
      end_col_offset_idx: 6
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 0
      text: "\u03F5ls"
    - bbox:
        b: 141.18994140625
        coord_origin: TOPLEFT
        l: 405.2554016113281
        r: 502.35089111328125
        t: 132.44281005859375
      col_span: 1
      column_header: true
      end_col_offset_idx: 9
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 8
      start_row_offset_idx: 0
      text: "PPL BLEU params \xD710 6"
    - bbox:
        b: 152.17340087890625
        coord_origin: TOPLEFT
        l: 403.7712097167969
        r: 458.9341125488281
        t: 143.61553955078125
      col_span: 1
      column_header: true
      end_col_offset_idx: 8
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 0
      text: (dev) (dev)
    - bbox:
        b: 163.14764404296875
        coord_origin: TOPLEFT
        l: 116.49788665771484
        r: 133.9722900390625
        t: 156.2535400390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: base
    - bbox:
        b: 163.1875
        coord_origin: TOPLEFT
        l: 148.52073669433594
        r: 391.3490905761719
        t: 156.2037353515625
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 1
      text: 6 512 2048 8 64 64 0.1 0.1 100K
    - bbox:
        b: 163.2672119140625
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 492.7325134277344
        t: 156.2037353515625
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 1
      text: 4.92 25.8 65
    - bbox:
        b: 193.81243896484375
        coord_origin: TOPLEFT
        l: 118.88420104980469
        r: 131.75588989257812
        t: 185.32427978515625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: (A)
    - bbox:
        b: 175.82452392578125
        coord_origin: TOPLEFT
        l: 237.7228546142578
        r: 297.637939453125
        t: 168.84075927734375
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 2
      text: 1 512 512
    - bbox:
        b: 175.90423583984375
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 457.33538818359375
        t: 168.84075927734375
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 2
      text: 5.29 24.9
    - bbox:
        b: 186.7344970703125
        coord_origin: TOPLEFT
        l: 236.73655700683594
        r: 297.33905029296875
        t: 179.8702392578125
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 3
      text: 4 128 128
    - bbox:
        b: 186.7344970703125
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 457.1261901855469
        t: 179.750732421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 3
      text: 5.00 25.5
    - bbox:
        b: 197.64349365234375
        coord_origin: TOPLEFT
        l: 235.23284912109375
        r: 295.1479187011719
        t: 190.6995849609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 4
      text: 16 32 32
    - bbox:
        b: 197.72320556640625
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 457.1959228515625
        t: 190.65972900390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 4
      text: 4.91 25.8
    - bbox:
        b: 208.552490234375
        coord_origin: TOPLEFT
        l: 234.55538940429688
        r: 295.07818603515625
        t: 201.60858154296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 5
      text: 32 16 16
    - bbox:
        b: 208.552490234375
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 457.46490478515625
        t: 201.5687255859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 5
      text: 5.01 25.4
    - bbox:
        b: 228.2684326171875
        coord_origin: TOPLEFT
        l: 119.158203125
        r: 131.48193359375
        t: 219.7802734375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 7
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 6
      text: (B)
    - bbox:
        b: 221.18951416015625
        coord_origin: TOPLEFT
        l: 259.6408386230469
        r: 268.1788024902344
        t: 214.24560546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 6
      text: '16'
    - bbox:
        b: 221.18951416015625
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 492.80224609375
        t: 214.20574951171875
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 6
      text: 5.16 25.1 58
    - bbox:
        b: 232.0985107421875
        coord_origin: TOPLEFT
        l: 258.9634094238281
        r: 268.24853515625
        t: 225.2342529296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 4
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 3
      start_row_offset_idx: 7
      text: '32'
    - bbox:
        b: 232.0985107421875
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 493.111083984375
        t: 225.11474609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 7
      text: 5.01 25.4 60
    - bbox:
        b: 279.08740234375
        coord_origin: TOPLEFT
        l: 119.158203125
        r: 131.48193359375
        t: 270.5992431640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 12
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 11
      text: (C)
    - bbox:
        b: 244.60699462890625
        coord_origin: TOPLEFT
        l: 148.4808807373047
        r: 152.91424560546875
        t: 237.87225341796875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 8
      text: '2'
    - bbox:
        b: 244.73651123046875
        coord_origin: TOPLEFT
        l: 405.30072021484375
        r: 493.0314025878906
        t: 237.7926025390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 8
      text: 6.11 23.7 36
    - bbox:
        b: 255.5159912109375
        coord_origin: TOPLEFT
        l: 148.3015594482422
        r: 152.8843536376953
        t: 248.78125
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 9
      text: '4'
    - bbox:
        b: 255.7252197265625
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 493.111083984375
        t: 248.6617431640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 9
      text: 5.19 25.3 50
    - bbox:
        b: 266.55450439453125
        coord_origin: TOPLEFT
        l: 148.7399139404297
        r: 152.6153564453125
        t: 259.69024658203125
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 10
      text: '8'
    - bbox:
        b: 266.55450439453125
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 493.111083984375
        t: 259.57073974609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 10
      text: 4.88 25.5 80
    - bbox:
        b: 277.4635009765625
        coord_origin: TOPLEFT
        l: 171.55886840820312
        r: 295.1449279785156
        t: 270.479736328125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 11
      text: 256 32 32
    - bbox:
        b: 277.4635009765625
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 492.80224609375
        t: 270.479736328125
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 11
      text: 5.75 24.5 28
    - bbox:
        b: 288.3725280761719
        coord_origin: TOPLEFT
        l: 169.87484741210938
        r: 297.33636474609375
        t: 281.50830078125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 13
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 12
      text: 1024 128 128
    - bbox:
        b: 288.3725280761719
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 495.29290771484375
        t: 281.4285888671875
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 13
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 12
      text: 4.66 26.0 168
    - bbox:
        b: 299.2815246582031
        coord_origin: TOPLEFT
        l: 203.3518524169922
        r: 221.89224243164062
        t: 292.41729736328125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 14
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 13
      text: '1024'
    - bbox:
        b: 299.2815246582031
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 492.6727294921875
        t: 292.2977294921875
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 14
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 13
      text: 5.12 25.4 53
    - bbox:
        b: 310.2702331542969
        coord_origin: TOPLEFT
        l: 202.3655548095703
        r: 221.85240173339844
        t: 303.24658203125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 15
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 14
      text: '4096'
    - bbox:
        b: 310.2702331542969
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 493.111083984375
        t: 303.20672607421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 15
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 14
      text: 4.75 26.2 90
    - bbox:
        b: 340.81640625
        coord_origin: TOPLEFT
        l: 118.88420104980469
        r: 131.75588989257812
        t: 332.3282775878906
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 17
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 16
      text: (D)
    - bbox:
        b: 322.8285217285156
        coord_origin: TOPLEFT
        l: 315.35211181640625
        r: 327.3271484375
        t: 315.96429443359375
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 16
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 15
      text: '0.0'
    - bbox:
        b: 322.8285217285156
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 457.425048828125
        t: 315.8447265625
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 16
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 15
      text: 5.77 24.6
    - bbox:
        b: 333.7375183105469
        coord_origin: TOPLEFT
        l: 315.35211181640625
        r: 327.31719970703125
        t: 326.873291015625
      col_span: 1
      column_header: false
      end_col_offset_idx: 5
      end_row_offset_idx: 17
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 4
      start_row_offset_idx: 16
      text: '0.2'
    - bbox:
        b: 333.8172302246094
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 457.1261901855469
        t: 326.75372314453125
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 17
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 16
      text: 4.95 25.5
    - bbox:
        b: 344.6465148925781
        coord_origin: TOPLEFT
        l: 345.0310974121094
        r: 357.0061340332031
        t: 337.78228759765625
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 18
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 17
      text: '0.0'
    - bbox:
        b: 344.6465148925781
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 457.06640625
        t: 337.6627197265625
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 18
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 17
      text: 4.67 25.3
    - bbox:
        b: 355.5555114746094
        coord_origin: TOPLEFT
        l: 345.0310974121094
        r: 356.9961853027344
        t: 348.6912841796875
      col_span: 1
      column_header: false
      end_col_offset_idx: 6
      end_row_offset_idx: 19
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 5
      start_row_offset_idx: 18
      text: '0.2'
    - bbox:
        b: 355.5555114746094
        coord_origin: TOPLEFT
        l: 405.28082275390625
        r: 457.23577880859375
        t: 348.57171630859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 19
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 18
      text: 5.47 25.7
    - bbox:
        b: 369.8174133300781
        coord_origin: TOPLEFT
        l: 119.43720245361328
        r: 131.20303344726562
        t: 361.32928466796875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 20
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 19
      text: (E)
    - bbox:
        b: 370.22589111328125
        coord_origin: TOPLEFT
        l: 178.68382263183594
        r: 345.37799072265625
        t: 361.2595520019531
      col_span: 8
      column_header: false
      end_col_offset_idx: 9
      end_row_offset_idx: 20
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 19
      text: positional embedding instead of sinusoids
    - bbox:
        b: 368.2732238769531
        coord_origin: TOPLEFT
        l: 405.08154296875
        r: 457.23577880859375
        t: 361.209716796875
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 20
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 19
      text: 4.92 25.7
    - bbox:
        b: 382.8628845214844
        coord_origin: TOPLEFT
        l: 118.98388671875
        r: 131.3873291015625
        t: 373.89654541015625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 21
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 20
      text: big
    - bbox:
        b: 380.91021728515625
        coord_origin: TOPLEFT
        l: 148.52073669433594
        r: 391.3490905761719
        t: 373.8865661621094
      col_span: 1
      column_header: false
      end_col_offset_idx: 7
      end_row_offset_idx: 21
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 6
      start_row_offset_idx: 20
      text: 6 1024 4096 16 0.3 300K
    - bbox:
        b: 380.83050537109375
        coord_origin: TOPLEFT
        l: 405.15130615234375
        r: 457.49481201171875
        t: 373.8467102050781
      col_span: 1
      column_header: false
      end_col_offset_idx: 8
      end_row_offset_idx: 21
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 7
      start_row_offset_idx: 20
      text: 4.33 26.4
    - bbox:
        b: 380.83050537109375
        coord_origin: TOPLEFT
        l: 481.2008972167969
        r: 495.16845703125
        t: 373.9662780761719
      col_span: 1
      column_header: false
      end_col_offset_idx: 9
      end_row_offset_idx: 21
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 8
      start_row_offset_idx: 20
      text: '213'
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 407.0993347167969
      coord_origin: BOTTOMLEFT
      l: 107.06885528564453
      r: 509.013916015625
      t: 662.7780609130859
    charspan:
    - 0
    - 0
    page_no: 9
  references: []
  self_ref: '#/tables/2'
- annotations: []
  captions:
  - $ref: '#/texts/167'
  children:
  - $ref: '#/texts/167'
  content_layer: body
  data:
    grid:
    - - bbox:
          b: 101.91253662109375
          coord_origin: TOPLEFT
          l: 206.9174041748047
          r: 234.7728271484375
          t: 95.04827880859375
        col_span: 1
        column_header: true
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: Parser
      - bbox:
          b: 103.82537841796875
          coord_origin: TOPLEFT
          l: 334.3928527832031
          r: 370.8260803222656
          t: 94.89886474609375
        col_span: 1
        column_header: true
        end_col_offset_idx: 2
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 0
        text: Training
      - bbox:
          b: 102.7294921875
          coord_origin: TOPLEFT
          l: 414.6662902832031
          r: 460.3946228027344
          t: 94.888916015625
        col_span: 1
        column_header: true
        end_col_offset_idx: 3
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 0
        text: WSJ 23 F1
    - - bbox:
          b: 115.25189208984375
          coord_origin: TOPLEFT
          l: 151.1864013671875
          r: 289.72625732421875
          t: 106.28558349609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: Vinyals & Kaiser el al. (2014) [37]
      - bbox:
          b: 115.25189208984375
          coord_origin: TOPLEFT
          l: 302.6078186035156
          r: 402.323486328125
          t: 106.28558349609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 1
        text: WSJ only, discriminative
      - bbox:
          b: 113.21954345703125
          coord_origin: TOPLEFT
          l: 429.5658874511719
          r: 445.76507568359375
          t: 106.35528564453125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 1
        text: '88.3'
    - - bbox:
          b: 125.75244140625
          coord_origin: TOPLEFT
          l: 172.7454071044922
          r: 268.16717529296875
          t: 117.18463134765625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: Petrov et al. (2006) [29]
      - bbox:
          b: 126.160888671875
          coord_origin: TOPLEFT
          l: 302.6078186035156
          r: 402.323486328125
          t: 117.194580078125
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 2
        text: WSJ only, discriminative
      - bbox:
          b: 124.208251953125
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 446.1636047363281
          t: 117.2642822265625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 2
        text: '90.4'
    - - bbox:
          b: 136.66143798828125
          coord_origin: TOPLEFT
          l: 177.58265686035156
          r: 263.2610168457031
          t: 128.10357666015625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: Zhu et al. (2013) [40]
      - bbox:
          b: 137.06988525390625
          coord_origin: TOPLEFT
          l: 302.6078186035156
          r: 402.323486328125
          t: 128.10357666015625
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 3
        text: WSJ only, discriminative
      - bbox:
          b: 135.11724853515625
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 446.1636047363281
          t: 128.17327880859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 3
        text: '90.4'
    - - bbox:
          b: 147.9788818359375
          coord_origin: TOPLEFT
          l: 178.2104034423828
          r: 262.70318603515625
          t: 139.00262451171875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: Dyer et al. (2016) [8]
      - bbox:
          b: 147.9788818359375
          coord_origin: TOPLEFT
          l: 302.6078186035156
          r: 402.323486328125
          t: 139.0125732421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 4
        text: WSJ only, discriminative
      - bbox:
          b: 146.0262451171875
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.9344482421875
          t: 139.082275390625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 4
        text: '91.7'
    - - bbox:
          b: 158.88787841796875
          coord_origin: TOPLEFT
          l: 176.068359375
          r: 265.2535400390625
          t: 149.92156982421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: Transformer (4 layers)
      - bbox:
          b: 158.88787841796875
          coord_origin: TOPLEFT
          l: 302.6078186035156
          r: 402.323486328125
          t: 149.92156982421875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 5
        text: WSJ only, discriminative
      - bbox:
          b: 156.93524169921875
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.76507568359375
          t: 149.99127197265625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 5
        text: '91.3'
    - - bbox:
          b: 169.388427734375
          coord_origin: TOPLEFT
          l: 177.58265686035156
          r: 263.2610168457031
          t: 160.83056640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 7
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 6
        text: Zhu et al. (2013) [40]
      - bbox:
          b: 169.78692626953125
          coord_origin: TOPLEFT
          l: 320.6750793457031
          r: 384.82427978515625
          t: 160.83056640625
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 6
        text: semi-supervised
      - bbox:
          b: 167.84423828125
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.76507568359375
          t: 160.9002685546875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 6
        text: '91.3'
    - - bbox:
          b: 180.70684814453125
          coord_origin: TOPLEFT
          l: 163.4602813720703
          r: 277.48223876953125
          t: 171.81024169921875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 8
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 7
        text: Huang & Harper (2009) [14]
      - bbox:
          b: 180.6968994140625
          coord_origin: TOPLEFT
          l: 320.6750793457031
          r: 384.82427978515625
          t: 171.74053955078125
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 7
        text: semi-supervised
      - bbox:
          b: 178.75421142578125
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.76507568359375
          t: 171.81024169921875
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 7
        text: '91.3'
    - - bbox:
          b: 191.6158447265625
          coord_origin: TOPLEFT
          l: 164.9545440673828
          r: 275.91796875
          t: 182.63958740234375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 9
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 8
        text: McClosky et al. (2006) [26]
      - bbox:
          b: 191.60589599609375
          coord_origin: TOPLEFT
          l: 320.6750793457031
          r: 384.82427978515625
          t: 182.6495361328125
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 8
        text: semi-supervised
      - bbox:
          b: 189.6632080078125
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.3865051269531
          t: 182.71923828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 8
        text: '92.1'
    - - bbox:
          b: 202.52484130859375
          coord_origin: TOPLEFT
          l: 151.18641662597656
          r: 289.7262878417969
          t: 193.55853271484375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 10
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 9
        text: Vinyals & Kaiser el al. (2014) [37]
      - bbox:
          b: 202.514892578125
          coord_origin: TOPLEFT
          l: 320.6750793457031
          r: 384.82427978515625
          t: 193.55853271484375
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 9
        text: semi-supervised
      - bbox:
          b: 200.57220458984375
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.3865051269531
          t: 193.62823486328125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 9
        text: '92.1'
    - - bbox:
          b: 213.433837890625
          coord_origin: TOPLEFT
          l: 176.068359375
          r: 265.2535400390625
          t: 204.467529296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 11
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 10
        text: Transformer (4 layers)
      - bbox:
          b: 213.42388916015625
          coord_origin: TOPLEFT
          l: 320.6750793457031
          r: 384.82427978515625
          t: 204.467529296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 10
        text: semi-supervised
      - bbox:
          b: 211.481201171875
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.9344482421875
          t: 204.5372314453125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 10
        text: '92.7'
    - - bbox:
          b: 224.3428955078125
          coord_origin: TOPLEFT
          l: 172.63055419921875
          r: 268.2416076660156
          t: 215.3267822265625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 12
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 11
        text: Luong et al. (2015) [23]
      - bbox:
          b: 222.27069091796875
          coord_origin: TOPLEFT
          l: 332.4953918457031
          r: 372.79412841796875
          t: 215.3765869140625
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 11
        text: multi-task
      - bbox:
          b: 222.3902587890625
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 446.20343017578125
          t: 215.4462890625
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 11
        text: '93.0'
    - - bbox:
          b: 235.25189208984375
          coord_origin: TOPLEFT
          l: 178.2104034423828
          r: 262.70318603515625
          t: 226.275634765625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 13
        row_header: true
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 12
        text: Dyer et al. (2016) [8]
      - bbox:
          b: 235.25189208984375
          coord_origin: TOPLEFT
          l: 332.2709655761719
          r: 372.88848876953125
          t: 226.28558349609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 2
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 1
        start_row_offset_idx: 12
        text: generative
      - bbox:
          b: 233.29925537109375
          coord_origin: TOPLEFT
          l: 429.306884765625
          r: 445.76507568359375
          t: 226.35528564453125
        col_span: 1
        column_header: false
        end_col_offset_idx: 3
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 2
        start_row_offset_idx: 12
        text: '93.3'
    num_cols: 3
    num_rows: 13
    table_cells:
    - bbox:
        b: 101.91253662109375
        coord_origin: TOPLEFT
        l: 206.9174041748047
        r: 234.7728271484375
        t: 95.04827880859375
      col_span: 1
      column_header: true
      end_col_offset_idx: 1
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 0
      text: Parser
    - bbox:
        b: 103.82537841796875
        coord_origin: TOPLEFT
        l: 334.3928527832031
        r: 370.8260803222656
        t: 94.89886474609375
      col_span: 1
      column_header: true
      end_col_offset_idx: 2
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 0
      text: Training
    - bbox:
        b: 102.7294921875
        coord_origin: TOPLEFT
        l: 414.6662902832031
        r: 460.3946228027344
        t: 94.888916015625
      col_span: 1
      column_header: true
      end_col_offset_idx: 3
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 0
      text: WSJ 23 F1
    - bbox:
        b: 115.25189208984375
        coord_origin: TOPLEFT
        l: 151.1864013671875
        r: 289.72625732421875
        t: 106.28558349609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: Vinyals & Kaiser el al. (2014) [37]
    - bbox:
        b: 115.25189208984375
        coord_origin: TOPLEFT
        l: 302.6078186035156
        r: 402.323486328125
        t: 106.28558349609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 1
      text: WSJ only, discriminative
    - bbox:
        b: 113.21954345703125
        coord_origin: TOPLEFT
        l: 429.5658874511719
        r: 445.76507568359375
        t: 106.35528564453125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 1
      text: '88.3'
    - bbox:
        b: 125.75244140625
        coord_origin: TOPLEFT
        l: 172.7454071044922
        r: 268.16717529296875
        t: 117.18463134765625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: Petrov et al. (2006) [29]
    - bbox:
        b: 126.160888671875
        coord_origin: TOPLEFT
        l: 302.6078186035156
        r: 402.323486328125
        t: 117.194580078125
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 2
      text: WSJ only, discriminative
    - bbox:
        b: 124.208251953125
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 446.1636047363281
        t: 117.2642822265625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 2
      text: '90.4'
    - bbox:
        b: 136.66143798828125
        coord_origin: TOPLEFT
        l: 177.58265686035156
        r: 263.2610168457031
        t: 128.10357666015625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: Zhu et al. (2013) [40]
    - bbox:
        b: 137.06988525390625
        coord_origin: TOPLEFT
        l: 302.6078186035156
        r: 402.323486328125
        t: 128.10357666015625
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 3
      text: WSJ only, discriminative
    - bbox:
        b: 135.11724853515625
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 446.1636047363281
        t: 128.17327880859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 3
      text: '90.4'
    - bbox:
        b: 147.9788818359375
        coord_origin: TOPLEFT
        l: 178.2104034423828
        r: 262.70318603515625
        t: 139.00262451171875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: Dyer et al. (2016) [8]
    - bbox:
        b: 147.9788818359375
        coord_origin: TOPLEFT
        l: 302.6078186035156
        r: 402.323486328125
        t: 139.0125732421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 4
      text: WSJ only, discriminative
    - bbox:
        b: 146.0262451171875
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.9344482421875
        t: 139.082275390625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 4
      text: '91.7'
    - bbox:
        b: 158.88787841796875
        coord_origin: TOPLEFT
        l: 176.068359375
        r: 265.2535400390625
        t: 149.92156982421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 6
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 5
      text: Transformer (4 layers)
    - bbox:
        b: 158.88787841796875
        coord_origin: TOPLEFT
        l: 302.6078186035156
        r: 402.323486328125
        t: 149.92156982421875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 5
      text: WSJ only, discriminative
    - bbox:
        b: 156.93524169921875
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.76507568359375
        t: 149.99127197265625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 5
      text: '91.3'
    - bbox:
        b: 169.388427734375
        coord_origin: TOPLEFT
        l: 177.58265686035156
        r: 263.2610168457031
        t: 160.83056640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 7
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 6
      text: Zhu et al. (2013) [40]
    - bbox:
        b: 169.78692626953125
        coord_origin: TOPLEFT
        l: 320.6750793457031
        r: 384.82427978515625
        t: 160.83056640625
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 6
      text: semi-supervised
    - bbox:
        b: 167.84423828125
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.76507568359375
        t: 160.9002685546875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 6
      text: '91.3'
    - bbox:
        b: 180.70684814453125
        coord_origin: TOPLEFT
        l: 163.4602813720703
        r: 277.48223876953125
        t: 171.81024169921875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 8
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 7
      text: Huang & Harper (2009) [14]
    - bbox:
        b: 180.6968994140625
        coord_origin: TOPLEFT
        l: 320.6750793457031
        r: 384.82427978515625
        t: 171.74053955078125
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 7
      text: semi-supervised
    - bbox:
        b: 178.75421142578125
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.76507568359375
        t: 171.81024169921875
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 7
      text: '91.3'
    - bbox:
        b: 191.6158447265625
        coord_origin: TOPLEFT
        l: 164.9545440673828
        r: 275.91796875
        t: 182.63958740234375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 9
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 8
      text: McClosky et al. (2006) [26]
    - bbox:
        b: 191.60589599609375
        coord_origin: TOPLEFT
        l: 320.6750793457031
        r: 384.82427978515625
        t: 182.6495361328125
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 8
      text: semi-supervised
    - bbox:
        b: 189.6632080078125
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.3865051269531
        t: 182.71923828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 8
      text: '92.1'
    - bbox:
        b: 202.52484130859375
        coord_origin: TOPLEFT
        l: 151.18641662597656
        r: 289.7262878417969
        t: 193.55853271484375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 10
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 9
      text: Vinyals & Kaiser el al. (2014) [37]
    - bbox:
        b: 202.514892578125
        coord_origin: TOPLEFT
        l: 320.6750793457031
        r: 384.82427978515625
        t: 193.55853271484375
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 9
      text: semi-supervised
    - bbox:
        b: 200.57220458984375
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.3865051269531
        t: 193.62823486328125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 9
      text: '92.1'
    - bbox:
        b: 213.433837890625
        coord_origin: TOPLEFT
        l: 176.068359375
        r: 265.2535400390625
        t: 204.467529296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 11
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 10
      text: Transformer (4 layers)
    - bbox:
        b: 213.42388916015625
        coord_origin: TOPLEFT
        l: 320.6750793457031
        r: 384.82427978515625
        t: 204.467529296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 10
      text: semi-supervised
    - bbox:
        b: 211.481201171875
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.9344482421875
        t: 204.5372314453125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 10
      text: '92.7'
    - bbox:
        b: 224.3428955078125
        coord_origin: TOPLEFT
        l: 172.63055419921875
        r: 268.2416076660156
        t: 215.3267822265625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 12
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 11
      text: Luong et al. (2015) [23]
    - bbox:
        b: 222.27069091796875
        coord_origin: TOPLEFT
        l: 332.4953918457031
        r: 372.79412841796875
        t: 215.3765869140625
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 11
      text: multi-task
    - bbox:
        b: 222.3902587890625
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 446.20343017578125
        t: 215.4462890625
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 11
      text: '93.0'
    - bbox:
        b: 235.25189208984375
        coord_origin: TOPLEFT
        l: 178.2104034423828
        r: 262.70318603515625
        t: 226.275634765625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 13
      row_header: true
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 12
      text: Dyer et al. (2016) [8]
    - bbox:
        b: 235.25189208984375
        coord_origin: TOPLEFT
        l: 332.2709655761719
        r: 372.88848876953125
        t: 226.28558349609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 2
      end_row_offset_idx: 13
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 1
      start_row_offset_idx: 12
      text: generative
    - bbox:
        b: 233.29925537109375
        coord_origin: TOPLEFT
        l: 429.306884765625
        r: 445.76507568359375
        t: 226.35528564453125
      col_span: 1
      column_header: false
      end_col_offset_idx: 3
      end_row_offset_idx: 13
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 2
      start_row_offset_idx: 12
      text: '93.3'
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 555.0657043457031
      coord_origin: BOTTOMLEFT
      l: 144.0305633544922
      r: 467.5986633300781
      t: 699.1512298583984
    charspan:
    - 0
    - 0
    page_no: 10
  references: []
  self_ref: '#/tables/3'
- annotations: []
  captions: []
  children: []
  content_layer: body
  data:
    grid:
    - - bbox:
          b: 84.1248779296875
          coord_origin: TOPLEFT
          l: 113.85771179199219
          r: 504.68341064453125
          t: 75.1087646484375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: '[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares,
          Holger Schwenk, and Yoshua Bengio. Learning phrase representations using
          rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078,
          2014.'
    - - bbox:
          b: 125.74383544921875
          coord_origin: TOPLEFT
          l: 113.85771179199219
          r: 503.8177185058594
          t: 116.76763916015625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: '[6] Francois Chollet. Xception: Deep learning with depthwise separable
          convolutions. arXiv preprint arXiv:1610.02357, 2016.'
    - - bbox:
          b: 156.453857421875
          coord_origin: TOPLEFT
          l: 113.85770416259766
          r: 503.856689453125
          t: 147.487548828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: "[7] Junyoung Chung, \xC7aglar G\xFCl\xE7ehre, Kyunghyun Cho, and Yoshua\
          \ Bengio. Empirical evaluation of gated recurrent neural networks on sequence\
          \ modeling. CoRR, abs/1412.3555, 2014."
    - - bbox:
          b: 187.16387939453125
          coord_origin: TOPLEFT
          l: 113.85768127441406
          r: 503.7891845703125
          t: 178.19757080078125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: '[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.
          Recurrent neural network grammars. In Proc. of NAACL, 2016.'
    - - bbox:
          b: 217.87384033203125
          coord_origin: TOPLEFT
          l: 113.85769653320312
          r: 505.1693115234375
          t: 208.90753173828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and\
          \ Yann N. Dauphin. Convolu\x02 tional sequence to sequence learning. arXiv\
          \ preprint arXiv:1705.03122v2, 2017."
    - - bbox:
          b: 248.5838623046875
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 429.93280029296875
          t: 239.6175537109375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: '[10] Alex Graves. Generating sequences with recurrent neural networks.  arXiv
          preprint arXiv:1308.0850, 2013.'
    - - bbox:
          b: 279.29388427734375
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 505.160888671875
          t: 270.32757568359375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 6
        text: "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\
          \ learning for im\x02 age recognition. In Proceedings of the IEEE Conference\
          \ on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016."
    - - bbox:
          b: 320.9139099121094
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 503.85003662109375
          t: 311.94757080078125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 7
        text: "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\xFCrgen\
          \ Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning\
          \ long-term dependencies, 2001."
    - - bbox:
          b: 351.6239013671875
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 504.6865539550781
          t: 342.6575622558594
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 8
        text: "[13] Sepp Hochreiter and J\xFCrgen Schmidhuber. Long short-term memory.\
          \ Neural computation , 9(8):1735\u20131780, 1997."
    - - bbox:
          b: 382.3338928222656
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 503.58587646484375
          t: 373.3675537109375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 9
        text: "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars\
          \ with latent annotations across languages. In Proceedings of the 2009 Conference\
          \ on Empirical Methods in Natural Language Processing, pages 832\u2013841.\
          \ ACL, August 2009."
    - - bbox:
          b: 423.952880859375
          coord_origin: TOPLEFT
          l: 108.87671661376953
          r: 503.697265625
          t: 414.93670654296875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 10
        text: '[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer,
          and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint
          arXiv:1602.02410, 2016.'
    - - bbox:
          b: 454.66290283203125
          coord_origin: TOPLEFT
          l: 108.87672424316406
          r: 504.0049743652344
          t: 445.68658447265625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 11
        text: "[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace\
          \ attention? In Advances in Neural Information Processing Systems, (NIPS),\
          \ 2016."
    - - bbox:
          b: 485.3728942871094
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 503.6877746582031
          t: 476.40655517578125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 12
        text: "[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms.\
          \ In International Conference on Learning Representations (ICLR), 2016."
    - - bbox:
          b: 516.0828857421875
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 505.1846923828125
          t: 507.1165466308594
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 13
        text: "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den\
          \ Oord, Alex Graves, and Ko\x02 ray Kavukcuoglu. Neural machine translation\
          \ in linear time. arXiv preprint arXiv:1610.10099v2 2017."
    - - bbox:
          b: 557.7018737792969
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 505.07073974609375
          t: 548.7355346679688
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 14
        text: '[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured
          attention networks. In International Conference on Learning Representations,
          2017.'
    - - bbox:
          b: 588.4128875732422
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 505.06549072265625
          t: 579.3967437744141
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 15
        text: '[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
          In ICLR, 2015.'
    - - bbox:
          b: 608.2138824462891
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 504.17840576171875
          t: 599.2475433349609
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 17
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 16
        text: '[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for
          LSTM networks. arXiv preprint arXiv:1703.10722, 2017.'
    - - bbox:
          b: 638.9238739013672
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 503.85009765625
          t: 629.9575347900391
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 18
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 17
        text: '[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing
          Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence
          embedding. arXiv preprint arXiv:1703.03130, 2017.'
    - - bbox:
          b: 680.5428771972656
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 504.3004150390625
          t: 671.5765380859375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 19
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 18
        text: '[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and
          Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint
          arXiv:1511.06114, 2015.'
    - - bbox:
          b: 711.2528839111328
          coord_origin: TOPLEFT
          l: 108.87672424316406
          r: 505.18463134765625
          t: 702.2865447998047
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 20
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 19
        text: "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective\
          \ approaches to attention\x02 based neural machine translation. arXiv preprint\
          \ arXiv:1508.04025, 2015."
    num_cols: 1
    num_rows: 20
    table_cells:
    - bbox:
        b: 84.1248779296875
        coord_origin: TOPLEFT
        l: 113.85771179199219
        r: 504.68341064453125
        t: 75.1087646484375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 0
      text: '[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares,
        Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn
        encoder-decoder for statistical machine translation. CoRR, abs/1406.1078,
        2014.'
    - bbox:
        b: 125.74383544921875
        coord_origin: TOPLEFT
        l: 113.85771179199219
        r: 503.8177185058594
        t: 116.76763916015625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: '[6] Francois Chollet. Xception: Deep learning with depthwise separable
        convolutions. arXiv preprint arXiv:1610.02357, 2016.'
    - bbox:
        b: 156.453857421875
        coord_origin: TOPLEFT
        l: 113.85770416259766
        r: 503.856689453125
        t: 147.487548828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: "[7] Junyoung Chung, \xC7aglar G\xFCl\xE7ehre, Kyunghyun Cho, and Yoshua\
        \ Bengio. Empirical evaluation of gated recurrent neural networks on sequence\
        \ modeling. CoRR, abs/1412.3555, 2014."
    - bbox:
        b: 187.16387939453125
        coord_origin: TOPLEFT
        l: 113.85768127441406
        r: 503.7891845703125
        t: 178.19757080078125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: '[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.
        Recurrent neural network grammars. In Proc. of NAACL, 2016.'
    - bbox:
        b: 217.87384033203125
        coord_origin: TOPLEFT
        l: 113.85769653320312
        r: 505.1693115234375
        t: 208.90753173828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: "[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann\
        \ N. Dauphin. Convolu\x02 tional sequence to sequence learning. arXiv preprint\
        \ arXiv:1705.03122v2, 2017."
    - bbox:
        b: 248.5838623046875
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 429.93280029296875
        t: 239.6175537109375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 5
      text: '[10] Alex Graves. Generating sequences with recurrent neural networks.  arXiv
        preprint arXiv:1308.0850, 2013.'
    - bbox:
        b: 279.29388427734375
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 505.160888671875
        t: 270.32757568359375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 6
      text: "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\
        \ learning for im\x02 age recognition. In Proceedings of the IEEE Conference\
        \ on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016."
    - bbox:
        b: 320.9139099121094
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 503.85003662109375
        t: 311.94757080078125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 7
      text: "[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\xFCrgen Schmidhuber.\
        \ Gradient flow in recurrent nets: the difficulty of learning long-term dependencies,\
        \ 2001."
    - bbox:
        b: 351.6239013671875
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 504.6865539550781
        t: 342.6575622558594
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 8
      text: "[13] Sepp Hochreiter and J\xFCrgen Schmidhuber. Long short-term memory.\
        \ Neural computation , 9(8):1735\u20131780, 1997."
    - bbox:
        b: 382.3338928222656
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 503.58587646484375
        t: 373.3675537109375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 9
      text: "[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with\
        \ latent annotations across languages. In Proceedings of the 2009 Conference\
        \ on Empirical Methods in Natural Language Processing, pages 832\u2013841.\
        \ ACL, August 2009."
    - bbox:
        b: 423.952880859375
        coord_origin: TOPLEFT
        l: 108.87671661376953
        r: 503.697265625
        t: 414.93670654296875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 10
      text: '[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and
        Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410,
        2016.'
    - bbox:
        b: 454.66290283203125
        coord_origin: TOPLEFT
        l: 108.87672424316406
        r: 504.0049743652344
        t: 445.68658447265625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 11
      text: "[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention?\
        \ In Advances in Neural Information Processing Systems, (NIPS), 2016."
    - bbox:
        b: 485.3728942871094
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 503.6877746582031
        t: 476.40655517578125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 13
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 12
      text: "[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms.\
        \ In International Conference on Learning Representations (ICLR), 2016."
    - bbox:
        b: 516.0828857421875
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 505.1846923828125
        t: 507.1165466308594
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 14
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 13
      text: "[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den\
        \ Oord, Alex Graves, and Ko\x02 ray Kavukcuoglu. Neural machine translation\
        \ in linear time. arXiv preprint arXiv:1610.10099v2 2017."
    - bbox:
        b: 557.7018737792969
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 505.07073974609375
        t: 548.7355346679688
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 15
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 14
      text: '[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured
        attention networks. In International Conference on Learning Representations,
        2017.'
    - bbox:
        b: 588.4128875732422
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 505.06549072265625
        t: 579.3967437744141
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 16
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 15
      text: '[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
        In ICLR, 2015.'
    - bbox:
        b: 608.2138824462891
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 504.17840576171875
        t: 599.2475433349609
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 17
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 16
      text: '[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM
        networks. arXiv preprint arXiv:1703.10722, 2017.'
    - bbox:
        b: 638.9238739013672
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 503.85009765625
        t: 629.9575347900391
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 18
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 17
      text: '[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing
        Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence
        embedding. arXiv preprint arXiv:1703.03130, 2017.'
    - bbox:
        b: 680.5428771972656
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 504.3004150390625
        t: 671.5765380859375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 19
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 18
      text: '[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and
        Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114,
        2015.'
    - bbox:
        b: 711.2528839111328
        coord_origin: TOPLEFT
        l: 108.87672424316406
        r: 505.18463134765625
        t: 702.2865447998047
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 20
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 19
      text: "[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective\
        \ approaches to attention\x02 based neural machine translation. arXiv preprint\
        \ arXiv:1508.04025, 2015."
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 71.82159423828125
      coord_origin: BOTTOMLEFT
      l: 107.4990005493164
      r: 505.82586669921875
      t: 718.2976226806641
    charspan:
    - 0
    - 0
    page_no: 11
  references: []
  self_ref: '#/tables/4'
- annotations: []
  captions: []
  children: []
  content_layer: body
  data:
    grid:
    - - bbox:
          b: 84.1248779296875
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 503.9154052734375
          t: 75.1087646484375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 1
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 0
        text: "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\
          \ Building a large annotated corpus of english: The penn treebank. Computational\
          \ linguistics, 19(2):313\u2013330, 1993."
    - - bbox:
          b: 118.516845703125
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 503.84747314453125
          t: 109.54058837890625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 2
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 1
        text: "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training\
          \ for parsing. In Proceedings of the Human Language Technology Conference\
          \ of the NAACL, Main Conference , pages 152\u2013159. ACL, June 2006."
    - - bbox:
          b: 163.81787109375
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 503.85687255859375
          t: 154.8515625
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 3
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 2
        text: "[27] Ankur Parikh, Oscar T\xE4ckstr\xF6m, Dipanjan Das, and Jakob Uszkoreit.\
          \ A decomposable attention model. In Empirical Methods in Natural Language\
          \ Processing, 2016."
    - - bbox:
          b: 198.2098388671875
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 503.79803466796875
          t: 189.2435302734375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 4
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 3
        text: '[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced
          model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.'
    - - bbox:
          b: 232.60186767578125
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 504.68341064453125
          t: 223.63555908203125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 5
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 4
        text: "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning\
          \ accurate, compact, and interpretable tree annotation. In Proceedings of\
          \ the 21st International Conference on Computational Linguistics and 44th\
          \ Annual Meeting of the ACL, pages 433\u2013440. ACL, July 2006."
    - - bbox:
          b: 288.8118896484375
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 503.8177185058594
          t: 279.84552001953125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 6
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 5
        text: '[30] Ofir Press and Lior Wolf. Using the output embedding to improve
          language models. arXiv preprint arXiv:1608.05859, 2016.'
    - - bbox:
          b: 323.2038879394531
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 503.59716796875
          t: 314.237548828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 7
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 6
        text: '[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine
          translation of rare words with subword units. arXiv preprint arXiv:1508.07909,
          2015.'
    - - bbox:
          b: 357.59588623046875
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 504.7073974609375
          t: 348.6295471191406
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 8
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 7
        text: '[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,
          Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks:
          The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538,
          2017.'
    - - bbox:
          b: 402.8968811035156
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 505.1748046875
          t: 393.9305419921875
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 9
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 8
        text: "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever,\
          \ and Ruslan Salakhutdi\x02 nov. Dropout: a simple way to prevent neural\
          \ networks from overfitting. Journal of Machine Learning Research, 15(1):1929\u2013\
          1958, 2014."
    - - bbox:
          b: 448.1979064941406
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 504.09393310546875
          t: 439.2315673828125
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 10
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 9
        text: "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\
          \ End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M.\
          \ Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing\
          \ Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015."
    - - bbox:
          b: 504.4089050292969
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 503.78900146484375
          t: 495.3927307128906
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 11
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 10
        text: "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence\
          \ learning with neural networks. In Advances in Neural Information Processing\
          \ Systems, pages 3104\u20133112, 2014."
    - - bbox:
          b: 538.8008880615234
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 505.03900146484375
          t: 529.8245849609375
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 12
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 11
        text: '[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,
          and Zbigniew Wojna. Rethinking the inception architecture for computer vision.
          CoRR, abs/1512.00567, 2015.'
    - - bbox:
          b: 573.1928863525391
          coord_origin: TOPLEFT
          l: 108.87669372558594
          r: 503.85003662109375
          t: 564.2265472412109
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 13
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 12
        text: '[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar
          as a foreign language. In Advances in Neural Information Processing Systems,
          2015.'
    - - bbox:
          b: 607.5848846435547
          coord_origin: TOPLEFT
          l: 108.876708984375
          r: 503.69769287109375
          t: 598.6185455322266
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 14
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 13
        text: "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\
          \ Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et\
          \ al. Google\u2019s neural machine translation system: Bridging the gap\
          \ between human and machine translation. arXiv preprint arXiv:1609.08144,\
          \ 2016."
    - - bbox:
          b: 663.7948760986328
          coord_origin: TOPLEFT
          l: 108.87670135498047
          r: 503.870361328125
          t: 654.8285369873047
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 15
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 14
        text: '[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent
          models with fast-forward connections for neural machine translation. CoRR,
          abs/1606.04199, 2016.'
    - - bbox:
          b: 698.186882019043
          coord_origin: TOPLEFT
          l: 108.87667846679688
          r: 503.79925537109375
          t: 689.2205429077148
        col_span: 1
        column_header: false
        end_col_offset_idx: 1
        end_row_offset_idx: 16
        row_header: false
        row_section: false
        row_span: 1
        start_col_offset_idx: 0
        start_row_offset_idx: 15
        text: "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.\
          \ Fast and accurate shift-reduce constituent parsing. In Proceedings of\
          \ the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434\u2013\
          443. ACL, August 2013."
    num_cols: 1
    num_rows: 16
    table_cells:
    - bbox:
        b: 84.1248779296875
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 503.9154052734375
        t: 75.1087646484375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 1
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 0
      text: "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\
        \ Building a large annotated corpus of english: The penn treebank. Computational\
        \ linguistics, 19(2):313\u2013330, 1993."
    - bbox:
        b: 118.516845703125
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 503.84747314453125
        t: 109.54058837890625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 2
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 1
      text: "[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training\
        \ for parsing. In Proceedings of the Human Language Technology Conference\
        \ of the NAACL, Main Conference , pages 152\u2013159. ACL, June 2006."
    - bbox:
        b: 163.81787109375
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 503.85687255859375
        t: 154.8515625
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 3
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 2
      text: "[27] Ankur Parikh, Oscar T\xE4ckstr\xF6m, Dipanjan Das, and Jakob Uszkoreit.\
        \ A decomposable attention model. In Empirical Methods in Natural Language\
        \ Processing, 2016."
    - bbox:
        b: 198.2098388671875
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 503.79803466796875
        t: 189.2435302734375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 4
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 3
      text: '[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced
        model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.'
    - bbox:
        b: 232.60186767578125
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 504.68341064453125
        t: 223.63555908203125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 5
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 4
      text: "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning\
        \ accurate, compact, and interpretable tree annotation. In Proceedings of\
        \ the 21st International Conference on Computational Linguistics and 44th\
        \ Annual Meeting of the ACL, pages 433\u2013440. ACL, July 2006."
    - bbox:
        b: 288.8118896484375
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 503.8177185058594
        t: 279.84552001953125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 6
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 5
      text: '[30] Ofir Press and Lior Wolf. Using the output embedding to improve
        language models. arXiv preprint arXiv:1608.05859, 2016.'
    - bbox:
        b: 323.2038879394531
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 503.59716796875
        t: 314.237548828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 7
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 6
      text: '[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine
        translation of rare words with subword units. arXiv preprint arXiv:1508.07909,
        2015.'
    - bbox:
        b: 357.59588623046875
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 504.7073974609375
        t: 348.6295471191406
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 8
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 7
      text: '[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis,
        Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks:
        The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538,
        2017.'
    - bbox:
        b: 402.8968811035156
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 505.1748046875
        t: 393.9305419921875
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 9
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 8
      text: "[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever,\
        \ and Ruslan Salakhutdi\x02 nov. Dropout: a simple way to prevent neural networks\
        \ from overfitting. Journal of Machine Learning Research, 15(1):1929\u2013\
        1958, 2014."
    - bbox:
        b: 448.1979064941406
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 504.09393310546875
        t: 439.2315673828125
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 10
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 9
      text: "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\
        \ End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M.\
        \ Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing\
        \ Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015."
    - bbox:
        b: 504.4089050292969
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 503.78900146484375
        t: 495.3927307128906
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 11
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 10
      text: "[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence\
        \ learning with neural networks. In Advances in Neural Information Processing\
        \ Systems, pages 3104\u20133112, 2014."
    - bbox:
        b: 538.8008880615234
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 505.03900146484375
        t: 529.8245849609375
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 12
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 11
      text: '[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,
        and Zbigniew Wojna. Rethinking the inception architecture for computer vision.
        CoRR, abs/1512.00567, 2015.'
    - bbox:
        b: 573.1928863525391
        coord_origin: TOPLEFT
        l: 108.87669372558594
        r: 503.85003662109375
        t: 564.2265472412109
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 13
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 12
      text: '[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as
        a foreign language. In Advances in Neural Information Processing Systems,
        2015.'
    - bbox:
        b: 607.5848846435547
        coord_origin: TOPLEFT
        l: 108.876708984375
        r: 503.69769287109375
        t: 598.6185455322266
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 14
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 13
      text: "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi,\
        \ Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\
        \ Google\u2019s neural machine translation system: Bridging the gap between\
        \ human and machine translation. arXiv preprint arXiv:1609.08144, 2016."
    - bbox:
        b: 663.7948760986328
        coord_origin: TOPLEFT
        l: 108.87670135498047
        r: 503.870361328125
        t: 654.8285369873047
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 15
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 14
      text: '[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent
        models with fast-forward connections for neural machine translation. CoRR,
        abs/1606.04199, 2016.'
    - bbox:
        b: 698.186882019043
        coord_origin: TOPLEFT
        l: 108.87667846679688
        r: 503.79925537109375
        t: 689.2205429077148
      col_span: 1
      column_header: false
      end_col_offset_idx: 1
      end_row_offset_idx: 16
      row_header: false
      row_section: false
      row_span: 1
      start_col_offset_idx: 0
      start_row_offset_idx: 15
      text: "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.\
        \ Fast and accurate shift-reduce constituent parsing. In Proceedings of the\
        \ 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434\u2013\
        443. ACL, August 2013."
  footnotes: []
  label: table
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 72.07904052734375
      coord_origin: BOTTOMLEFT
      l: 106.64751434326172
      r: 506.0921325683594
      t: 719.6920166015625
    charspan:
    - 0
    - 0
    page_no: 12
  references: []
  self_ref: '#/tables/5'
texts:
- children: []
  content_layer: furniture
  label: page_header
  orig: arXiv:1706.03762v7 [cs.CL] 2 Aug 2023
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 237.72000122070312
      coord_origin: BOTTOMLEFT
      l: 18.119998931884766
      r: 36.29999923706055
      t: 576.4200439453125
    charspan:
    - 0
    - 37
    page_no: 1
  self_ref: '#/texts/0'
  text: arXiv:1706.03762v7 [cs.CL] 2 Aug 2023
- children: []
  content_layer: body
  label: text
  orig: Provided proper attribution is provided, Google hereby grants permission to
    reproduce the tables and figures in this paper solely for use in journalistic
    or scholarly works.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 679.5476684570312
      coord_origin: BOTTOMLEFT
      l: 124.3727798461914
      r: 487.91845703125
      t: 718.202392578125
    charspan:
    - 0
    - 173
    page_no: 1
  self_ref: '#/texts/1'
  text: Provided proper attribution is provided, Google hereby grants permission to
    reproduce the tables and figures in this paper solely for use in journalistic
    or scholarly works.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: Attention Is All You Need
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 629.664306640625
      coord_origin: BOTTOMLEFT
      l: 211.6429443359375
      r: 399.51458740234375
      t: 641.852783203125
    charspan:
    - 0
    - 25
    page_no: 1
  self_ref: '#/texts/2'
  text: Attention Is All You Need
- children: []
  content_layer: body
  label: text
  orig: "Ashish Vaswani \u2217 Google Brain avaswani@google.com"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 525.2254638671875
      coord_origin: BOTTOMLEFT
      l: 117.22894287109375
      r: 215.91847229003906
      t: 556.1891479492188
    charspan:
    - 0
    - 49
    page_no: 1
  self_ref: '#/texts/3'
  text: "Ashish Vaswani \u2217 Google Brain avaswani@google.com"
- children: []
  content_layer: body
  label: text
  orig: "Noam Shazeer \u2217 Google Brain noam@google.com"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 525.2254638671875
      coord_origin: BOTTOMLEFT
      l: 230.8015899658203
      r: 309.0478515625
      t: 556.1990966796875
    charspan:
    - 0
    - 43
    page_no: 1
  self_ref: '#/texts/4'
  text: "Noam Shazeer \u2217 Google Brain noam@google.com"
- children: []
  content_layer: body
  label: text
  orig: "Niki Parmar \u2217 Google Research nikip@google.com"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 525.2254638671875
      coord_origin: BOTTOMLEFT
      l: 323.8966064453125
      r: 407.36328125
      t: 556.1891479492188
    charspan:
    - 0
    - 46
    page_no: 1
  self_ref: '#/texts/5'
  text: "Niki Parmar \u2217 Google Research nikip@google.com"
- children: []
  content_layer: body
  label: text
  orig: "Jakob Uszkoreit \u2217 Google Research usz@google.com"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 525.2254638671875
      coord_origin: BOTTOMLEFT
      l: 422.2215881347656
      r: 496.5830383300781
      t: 556.1891479492188
    charspan:
    - 0
    - 48
    page_no: 1
  self_ref: '#/texts/6'
  text: "Jakob Uszkoreit \u2217 Google Research usz@google.com"
- children: []
  content_layer: body
  label: text
  orig: "Llion Jones \u2217 Google Research llion@google.com"
  parent:
    $ref: '#/groups/0'
  prov:
  - bbox:
      b: 475.22747802734375
      coord_origin: BOTTOMLEFT
      l: 127.44987487792969
      r: 210.458251953125
      t: 506.192138671875
    charspan:
    - 0
    - 46
    page_no: 1
  self_ref: '#/texts/7'
  text: "Llion Jones \u2217 Google Research llion@google.com"
- children: []
  content_layer: body
  label: text
  orig: "Aidan N. Gomez\u2217 \u2020 University of Toronto aidan@cs.toronto.edu"
  parent:
    $ref: '#/groups/0'
  prov:
  - bbox:
      b: 477.4491271972656
      coord_origin: BOTTOMLEFT
      l: 235.95492553710938
      r: 339.7054443359375
      t: 507.83251953125
    charspan:
    - 0
    - 60
    page_no: 1
  self_ref: '#/texts/8'
  text: "Aidan N. Gomez\u2217 \u2020 University of Toronto aidan@cs.toronto.edu"
- children: []
  content_layer: body
  label: text
  orig: "\u0141ukasz Kaiser \u2217 Google Brain lukaszkaiser@google.com"
  parent:
    $ref: '#/groups/0'
  prov:
  - bbox:
      b: 475.2284851074219
      coord_origin: BOTTOMLEFT
      l: 365.4168701171875
      r: 484.9581298828125
      t: 506.192138671875
    charspan:
    - 0
    - 52
    page_no: 1
  self_ref: '#/texts/9'
  text: "\u0141ukasz Kaiser \u2217 Google Brain lukaszkaiser@google.com"
- children: []
  content_layer: body
  label: text
  orig: "Illia Polosukhin\u2217 \u2021"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 449.18048095703125
      coord_origin: BOTTOMLEFT
      l: 269.0062561035156
      r: 346.3098449707031
      t: 457.8355407714844
    charspan:
    - 0
    - 19
    page_no: 1
  self_ref: '#/texts/10'
  text: "Illia Polosukhin\u2217 \u2021"
- children: []
  content_layer: body
  label: text
  orig: illia.polosukhin@gmail.com
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 436.1394958496094
      coord_origin: BOTTOMLEFT
      l: 238.79908752441406
      r: 373.7923583984375
      t: 444.5578918457031
    charspan:
    - 0
    - 26
    page_no: 1
  self_ref: '#/texts/11'
  text: illia.polosukhin@gmail.com
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: Abstract
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 397.0635681152344
      coord_origin: BOTTOMLEFT
      l: 283.8656005859375
      r: 328.2313232421875
      t: 405.46807861328125
    charspan:
    - 0
    - 8
    page_no: 1
  self_ref: '#/texts/12'
  text: Abstract
- children: []
  content_layer: body
  label: text
  orig: The dominant sequence transduction models are based on complex recurrent or
    convolutional neural networks that include an encoder and a decoder. The best
    performing models also connect the encoder and decoder through an attention mechanism.
    We propose a new simple network architecture, the Transformer, based solely on
    attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments
    on two machine translation tasks show these models to be superior in quality while
    being more parallelizable and requiring significantly less time to train. Our
    model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving
    over the existing best results, including ensembles, by over 2 BLEU. On the WMT
    2014 English-to-French translation task, our model establishes a new single-model
    state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs,
    a small fraction of the training costs of the best models from the literature.
    We show that the Transformer generalizes well to other tasks by applying it successfully
    to English constituency parsing both with large and limited training data.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 216.1461181640625
      coord_origin: BOTTOMLEFT
      l: 143.72975158691406
      r: 469.2992248535156
      t: 377.8394470214844
    charspan:
    - 0
    - 1138
    page_no: 1
  self_ref: '#/texts/13'
  text: The dominant sequence transduction models are based on complex recurrent or
    convolutional neural networks that include an encoder and a decoder. The best
    performing models also connect the encoder and decoder through an attention mechanism.
    We propose a new simple network architecture, the Transformer, based solely on
    attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments
    on two machine translation tasks show these models to be superior in quality while
    being more parallelizable and requiring significantly less time to train. Our
    model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving
    over the existing best results, including ensembles, by over 2 BLEU. On the WMT
    2014 English-to-French translation task, our model establishes a new single-model
    state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs,
    a small fraction of the training costs of the best models from the literature.
    We show that the Transformer generalizes well to other tasks by applying it successfully
    to English constituency parsing both with large and limited training data.
- children: []
  content_layer: body
  label: footnote
  orig: "\u2217 Equal contribution. Listing order is random. Jakob proposed replacing\
    \ RNNs with self-attention and started the effort to evaluate this idea. Ashish,\
    \ with Illia, designed and implemented the first Transformer models and has been\
    \ crucially involved in every aspect of this work. Noam proposed scaled dot-product\
    \ attention, multi-head attention and the parameter-free position representation\
    \ and became the other person involved in nearly every detail. Niki designed,\
    \ implemented, tuned and evaluated countless model variants in our original codebase\
    \ and tensor2tensor. Llion also experimented with novel model variants, was responsible\
    \ for our initial codebase, and efficient inference and visualizations. Lukasz\
    \ and Aidan spent countless long days designing various parts of and implementing\
    \ tensor2tensor, replacing our earlier codebase, greatly improving results and\
    \ massively accelerating our research."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 106.35433959960938
      coord_origin: BOTTOMLEFT
      l: 108.07908630371094
      r: 504.0872802734375
      t: 192.73358154296875
    charspan:
    - 0
    - 906
    page_no: 1
  self_ref: '#/texts/14'
  text: "\u2217 Equal contribution. Listing order is random. Jakob proposed replacing\
    \ RNNs with self-attention and started the effort to evaluate this idea. Ashish,\
    \ with Illia, designed and implemented the first Transformer models and has been\
    \ crucially involved in every aspect of this work. Noam proposed scaled dot-product\
    \ attention, multi-head attention and the parameter-free position representation\
    \ and became the other person involved in nearly every detail. Niki designed,\
    \ implemented, tuned and evaluated countless model variants in our original codebase\
    \ and tensor2tensor. Llion also experimented with novel model variants, was responsible\
    \ for our initial codebase, and efficient inference and visualizations. Lukasz\
    \ and Aidan spent countless long days designing various parts of and implementing\
    \ tensor2tensor, replacing our earlier codebase, greatly improving results and\
    \ massively accelerating our research."
- children: []
  content_layer: body
  label: footnote
  orig: "\u2020 Work performed while at Google Brain."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 93.5492935180664
      coord_origin: BOTTOMLEFT
      l: 120.76591491699219
      r: 266.732666015625
      t: 103.5122299194336
    charspan:
    - 0
    - 39
    page_no: 1
  self_ref: '#/texts/15'
  text: "\u2020 Work performed while at Google Brain."
- children: []
  content_layer: body
  label: footnote
  orig: "\u2021 Work performed while at Google Research."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 82.60029602050781
      coord_origin: BOTTOMLEFT
      l: 120.76591491699219
      r: 279.6711730957031
      t: 92.563232421875
    charspan:
    - 0
    - 42
    page_no: 1
  self_ref: '#/texts/16'
  text: "\u2021 Work performed while at Google Research."
- children: []
  content_layer: furniture
  label: page_footer
  orig: 31st Conference on Neural Information Processing Systems (NIPS 2017), Long
    Beach, CA, USA.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 50.17729187011719
      coord_origin: BOTTOMLEFT
      l: 108.38555145263672
      r: 459.5188903808594
      t: 58.24705123901367
    charspan:
    - 0
    - 90
    page_no: 1
  self_ref: '#/texts/17'
  text: 31st Conference on Neural Information Processing Systems (NIPS 2017), Long
    Beach, CA, USA.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 1 Introduction
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 709.881591796875
      coord_origin: BOTTOMLEFT
      l: 108.77708435058594
      r: 190.61044311523438
      t: 718.2980346679688
    charspan:
    - 0
    - 14
    page_no: 2
  self_ref: '#/texts/18'
  text: 1 Introduction
- children: []
  content_layer: body
  label: text
  orig: Recurrent neural networks, long short-term memory [13] and gated recurrent
    [7] neural networks in particular, have been firmly established as state of the
    art approaches in sequence modeling and transduction problems such as language
    modeling and machine translation [35 , 2 , 5]. Numerous efforts have since continued
    to push the boundaries of recurrent language models and encoder-decoder architectures
    [38, 24, 15].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 640.173828125
      coord_origin: BOTTOMLEFT
      l: 108.1321029663086
      r: 504.18682861328125
      t: 692.158447265625
    charspan:
    - 0
    - 418
    page_no: 2
  self_ref: '#/texts/19'
  text: Recurrent neural networks, long short-term memory [13] and gated recurrent
    [7] neural networks in particular, have been firmly established as state of the
    art approaches in sequence modeling and transduction problems such as language
    modeling and machine translation [35 , 2 , 5]. Numerous efforts have since continued
    to push the boundaries of recurrent language models and encoder-decoder architectures
    [38, 24, 15].
- children: []
  content_layer: body
  label: text
  orig: "Recurrent models typically factor computation along the symbol positions\
    \ of the input and output sequences. Aligning the positions to steps in computation\
    \ time, they generate a sequence of hidden states h t , as a function of the previous\
    \ hidden state ht \u2212 1 and the input for position t. This inherently sequential\
    \ nature precludes parallelization within training examples, which becomes critical\
    \ at longer sequence lengths, as memory constraints limit batching across examples.\
    \ Recent work has achieved significant improvements in computational efficiency\
    \ through factorization tricks [21] and conditional computation [32], while also\
    \ improving model performance in case of the latter. The fundamental constraint\
    \ of sequential computation, however, remains."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 546.8140869140625
      coord_origin: BOTTOMLEFT
      l: 108.1727523803711
      r: 504.1867980957031
      t: 632.1334228515625
    charspan:
    - 0
    - 759
    page_no: 2
  self_ref: '#/texts/20'
  text: "Recurrent models typically factor computation along the symbol positions\
    \ of the input and output sequences. Aligning the positions to steps in computation\
    \ time, they generate a sequence of hidden states h t , as a function of the previous\
    \ hidden state ht \u2212 1 and the input for position t. This inherently sequential\
    \ nature precludes parallelization within training examples, which becomes critical\
    \ at longer sequence lengths, as memory constraints limit batching across examples.\
    \ Recent work has achieved significant improvements in computational efficiency\
    \ through factorization tricks [21] and conditional computation [32], while also\
    \ improving model performance in case of the latter. The fundamental constraint\
    \ of sequential computation, however, remains."
- children: []
  content_layer: body
  label: text
  orig: Attention mechanisms have become an integral part of compelling sequence modeling
    and transduction models in various tasks, allowing modeling of dependencies without
    regard to their distance in the input or output sequences [2 , 19]. In all but
    a few cases [27], however, such attention mechanisms are used in conjunction with
    a recurrent network.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 497.6881103515625
      coord_origin: BOTTOMLEFT
      l: 107.78954315185547
      r: 505.17999267578125
      t: 539.3814697265625
    charspan:
    - 0
    - 347
    page_no: 2
  self_ref: '#/texts/21'
  text: Attention mechanisms have become an integral part of compelling sequence modeling
    and transduction models in various tasks, allowing modeling of dependencies without
    regard to their distance in the input or output sequences [2 , 19]. In all but
    a few cases [27], however, such attention mechanisms are used in conjunction with
    a recurrent network.
- children: []
  content_layer: body
  label: text
  orig: In this work we propose the Transformer, a model architecture eschewing recurrence
    and instead relying entirely on an attention mechanism to draw global dependencies
    between input and output. The Transformer allows for significantly more parallelization
    and can reach a new state of the art in translation quality after being trained
    for as little as twelve hours on eight P100 GPUs.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 448.5721130371094
      coord_origin: BOTTOMLEFT
      l: 107.86036682128906
      r: 505.0418395996094
      t: 490.26544189453125
    charspan:
    - 0
    - 383
    page_no: 2
  self_ref: '#/texts/22'
  text: In this work we propose the Transformer, a model architecture eschewing recurrence
    and instead relying entirely on an attention mechanism to draw global dependencies
    between input and output. The Transformer allows for significantly more parallelization
    and can reach a new state of the art in translation quality after being trained
    for as little as twelve hours on eight P100 GPUs.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 2 Background
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 417.273193359375
      coord_origin: BOTTOMLEFT
      l: 108.20323944091797
      r: 188.56610107421875
      t: 427.9491882324219
    charspan:
    - 0
    - 12
    page_no: 2
  self_ref: '#/texts/23'
  text: 2 Background
- children: []
  content_layer: body
  label: text
  orig: The goal of reducing sequential computation also forms the foundation of the
    Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional
    neural networks as basic building block, computing hidden representations in parallel
    for all input and output positions. In these models, the number of operations
    required to relate signals from two arbitrary input or output positions grows
    in the distance between positions, linearly for ConvS2S and logarithmically for
    ByteNet. This makes it more difficult to learn dependencies between distant positions
    [12]. In the Transformer this is reduced to a constant number of operations, albeit
    at the cost of reduced effective resolution due to averaging attention-weighted
    positions, an effect we counteract with Multi-Head Attention as described in section
    3.2.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 307.63848876953125
      coord_origin: BOTTOMLEFT
      l: 107.85833740234375
      r: 504.70428466796875
      t: 401.8454284667969
    charspan:
    - 0
    - 825
    page_no: 2
  self_ref: '#/texts/24'
  text: The goal of reducing sequential computation also forms the foundation of the
    Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional
    neural networks as basic building block, computing hidden representations in parallel
    for all input and output positions. In these models, the number of operations
    required to relate signals from two arbitrary input or output positions grows
    in the distance between positions, linearly for ConvS2S and logarithmically for
    ByteNet. This makes it more difficult to learn dependencies between distant positions
    [12]. In the Transformer this is reduced to a constant number of operations, albeit
    at the cost of reduced effective resolution due to averaging attention-weighted
    positions, an effect we counteract with Multi-Head Attention as described in section
    3.2.
- children: []
  content_layer: body
  label: text
  orig: Self-attention, sometimes called intra-attention is an attention mechanism
    relating different positions of a single sequence in order to compute a representation
    of the sequence. Self-attention has been used successfully in a variety of tasks
    including reading comprehension, abstractive summarization, textual entailment
    and learning task-independent sentence representations [4, 27, 28, 22].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 256.4901123046875
      coord_origin: BOTTOMLEFT
      l: 108.0895767211914
      r: 504.7021789550781
      t: 298.1834411621094
    charspan:
    - 0
    - 393
    page_no: 2
  self_ref: '#/texts/25'
  text: Self-attention, sometimes called intra-attention is an attention mechanism
    relating different positions of a single sequence in order to compute a representation
    of the sequence. Self-attention has been used successfully in a variety of tasks
    including reading comprehension, abstractive summarization, textual entailment
    and learning task-independent sentence representations [4, 27, 28, 22].
- children: []
  content_layer: body
  label: text
  orig: End-to-end memory networks are based on a recurrent attention mechanism instead
    of sequencealigned recurrence and have been shown to perform well on simple-language
    question answering and language modeling tasks [34].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 218.2831268310547
      coord_origin: BOTTOMLEFT
      l: 108.12194061279297
      r: 505.1658935546875
      t: 249.06846618652344
    charspan:
    - 0
    - 217
    page_no: 2
  self_ref: '#/texts/26'
  text: End-to-end memory networks are based on a recurrent attention mechanism instead
    of sequencealigned recurrence and have been shown to perform well on simple-language
    question answering and language modeling tasks [34].
- children: []
  content_layer: body
  label: text
  orig: To the best of our knowledge, however, the Transformer is the first transduction
    model relying entirely on self-attention to compute representations of its input
    and output without using sequencealigned RNNs or convolution. In the following
    sections, we will describe the Transformer, motivate self-attention and discuss
    its advantages over models such as [17, 18] and [9].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 169.16812133789062
      coord_origin: BOTTOMLEFT
      l: 107.86375427246094
      r: 505.17889404296875
      t: 210.86146545410156
    charspan:
    - 0
    - 373
    page_no: 2
  self_ref: '#/texts/27'
  text: To the best of our knowledge, however, the Transformer is the first transduction
    model relying entirely on self-attention to compute representations of its input
    and output without using sequencealigned RNNs or convolution. In the following
    sections, we will describe the Transformer, motivate self-attention and discuss
    its advantages over models such as [17, 18] and [9].
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3 Model Architecture
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 140.1645965576172
      coord_origin: BOTTOMLEFT
      l: 108.1912841796875
      r: 225.87826538085938
      t: 148.5810546875
    charspan:
    - 0
    - 20
    page_no: 2
  self_ref: '#/texts/28'
  text: 3 Model Architecture
- children: []
  content_layer: body
  label: text
  orig: Most competitive neural sequence transduction models have an encoder-decoder
    structure [5 , 2 , 35]. Here, the encoder maps an input sequence of symbol representations
    (x1, ..., x n ) to a sequence of continuous representations z = (z1, ..., z n
    ). Given z, the decoder then generates an output sequence (y1, ..., y m ) of symbols
    one element at a time. At each step the model is auto-regressive [10], consuming
    the previously generated symbols as additional input when generating the next.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 108.11894989013672
      r: 505.0591735839844
      t: 122.49027252197266
    charspan:
    - 0
    - 490
    page_no: 2
  self_ref: '#/texts/29'
  text: Most competitive neural sequence transduction models have an encoder-decoder
    structure [5 , 2 , 35]. Here, the encoder maps an input sequence of symbol representations
    (x1, ..., x n ) to a sequence of continuous representations z = (z1, ..., z n
    ). Given z, the decoder then generates an output sequence (y1, ..., y m ) of symbols
    one element at a time. At each step the model is auto-regressive [10], consuming
    the previously generated symbols as additional input when generating the next.
- children: []
  content_layer: furniture
  label: page_footer
  orig: '2'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 42.11199951171875
      coord_origin: BOTTOMLEFT
      l: 303.8078918457031
      r: 308.2412414550781
      t: 48.846717834472656
    charspan:
    - 0
    - 1
    page_no: 2
  self_ref: '#/texts/30'
  text: '2'
- children: []
  content_layer: body
  label: caption
  orig: 'Figure 1: The Transformer - model architecture.'
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 377.537109375
      coord_origin: BOTTOMLEFT
      l: 210.13055419921875
      r: 401.30291748046875
      t: 386.5034484863281
    charspan:
    - 0
    - 47
    page_no: 3
  self_ref: '#/texts/31'
  text: 'Figure 1: The Transformer - model architecture.'
- children: []
  content_layer: body
  label: text
  orig: Output
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 709.3333333333334
      coord_origin: BOTTOMLEFT
      l: 330.33333333333337
      r: 360.33333333333337
      t: 720.0
    charspan:
    - 0
    - 6
    page_no: 3
  self_ref: '#/texts/32'
  text: Output
- children: []
  content_layer: body
  label: text
  orig: Probabilities
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 699.3333333333334
      coord_origin: BOTTOMLEFT
      l: 321.0
      r: 369.66666666666663
      t: 709.3333333333334
    charspan:
    - 0
    - 13
    page_no: 3
  self_ref: '#/texts/33'
  text: Probabilities
- children: []
  content_layer: body
  label: text
  orig: Softmax
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 680.6666666666666
      coord_origin: BOTTOMLEFT
      l: 330.33333333333337
      r: 359.66666666666663
      t: 689.3333333333334
    charspan:
    - 0
    - 7
    page_no: 3
  self_ref: '#/texts/34'
  text: Softmax
- children: []
  content_layer: body
  label: text
  orig: Linear
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 661.0
      coord_origin: BOTTOMLEFT
      l: 334.0
      r: 355.33333333333337
      t: 667.6666666666666
    charspan:
    - 0
    - 6
    page_no: 3
  self_ref: '#/texts/35'
  text: Linear
- children: []
  content_layer: body
  label: text
  orig: Add & Norm
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 638.0
      coord_origin: BOTTOMLEFT
      l: 323.66666666666663
      r: 366.33333333333337
      t: 646.0
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/36'
  text: Add & Norm
- children: []
  content_layer: body
  label: text
  orig: Feed
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 624.6666666666667
      coord_origin: BOTTOMLEFT
      l: 335.66666666666663
      r: 353.66666666666663
      t: 632.6666666666667
    charspan:
    - 0
    - 4
    page_no: 3
  self_ref: '#/texts/37'
  text: Feed
- children: []
  content_layer: body
  label: text
  orig: Forward
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 615.3333333333333
      coord_origin: BOTTOMLEFT
      l: 330.33333333333337
      r: 359.0
      t: 624.0
    charspan:
    - 0
    - 7
    page_no: 3
  self_ref: '#/texts/38'
  text: Forward
- children: []
  content_layer: body
  label: text
  orig: Add & Norm
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 592.6666666666666
      coord_origin: BOTTOMLEFT
      l: 323.66666666666663
      r: 366.33333333333337
      t: 601.3333333333334
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/39'
  text: Add & Norm
- children: []
  content_layer: body
  label: text
  orig: Add & Norm
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 584.6666666666666
      coord_origin: BOTTOMLEFT
      l: 245.66666666666666
      r: 288.3333333333333
      t: 592.6666666666666
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/40'
  text: Add & Norm
- children: []
  content_layer: body
  label: text
  orig: Multi-Head
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 579.3333333333334
      coord_origin: BOTTOMLEFT
      l: 326.33333333333337
      r: 363.66666666666663
      t: 588.0
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/41'
  text: Multi-Head
- children: []
  content_layer: body
  label: text
  orig: Feed
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 571.3333333333334
      coord_origin: BOTTOMLEFT
      l: 257.6666666666667
      r: 276.3333333333333
      t: 580.0
    charspan:
    - 0
    - 4
    page_no: 3
  self_ref: '#/texts/42'
  text: Feed
- children: []
  content_layer: body
  label: text
  orig: Attention
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 570.0
      coord_origin: BOTTOMLEFT
      l: 329.66666666666663
      r: 360.33333333333337
      t: 578.6666666666666
    charspan:
    - 0
    - 9
    page_no: 3
  self_ref: '#/texts/43'
  text: Attention
- children: []
  content_layer: body
  label: text
  orig: Forward
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 562.6666666666666
      coord_origin: BOTTOMLEFT
      l: 252.33333333333331
      r: 281.0
      t: 570.6666666666666
    charspan:
    - 0
    - 7
    page_no: 3
  self_ref: '#/texts/44'
  text: Forward
- children: []
  content_layer: body
  label: text
  orig: Nx
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 562.0
      coord_origin: BOTTOMLEFT
      l: 395.66666666666663
      r: 407.66666666666663
      t: 570.6666666666666
    charspan:
    - 0
    - 2
    page_no: 3
  self_ref: '#/texts/45'
  text: Nx
- children: []
  content_layer: body
  label: text
  orig: Add & Norm
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 540.0
      coord_origin: BOTTOMLEFT
      l: 323.66666666666663
      r: 366.33333333333337
      t: 548.6666666666666
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/46'
  text: Add & Norm
- children: []
  content_layer: body
  label: text
  orig: Nx
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 534.6666666666666
      coord_origin: BOTTOMLEFT
      l: 202.33333333333334
      r: 215.0
      t: 543.3333333333334
    charspan:
    - 0
    - 2
    page_no: 3
  self_ref: '#/texts/47'
  text: Nx
- children: []
  content_layer: body
  label: text
  orig: Add & Norm
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 531.3333333333334
      coord_origin: BOTTOMLEFT
      l: 245.66666666666666
      r: 288.3333333333333
      t: 540.0
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/48'
  text: Add & Norm
- children: []
  content_layer: body
  label: text
  orig: Masked
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 526.6666666666666
      coord_origin: BOTTOMLEFT
      l: 331.66666666666663
      r: 359.0
      t: 535.3333333333334
    charspan:
    - 0
    - 6
    page_no: 3
  self_ref: '#/texts/49'
  text: Masked
- children: []
  content_layer: body
  label: text
  orig: Multi-Head
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 518.6666666666666
      coord_origin: BOTTOMLEFT
      l: 248.33333333333334
      r: 285.6666666666667
      t: 526.6666666666666
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/50'
  text: Multi-Head
- children: []
  content_layer: body
  label: text
  orig: Multi-Head
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 518.0
      coord_origin: BOTTOMLEFT
      l: 326.33333333333337
      r: 363.66666666666663
      t: 526.0
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/51'
  text: Multi-Head
- children: []
  content_layer: body
  label: text
  orig: Attention
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 509.33333333333337
      coord_origin: BOTTOMLEFT
      l: 251.0
      r: 282.3333333333333
      t: 518.0
    charspan:
    - 0
    - 9
    page_no: 3
  self_ref: '#/texts/52'
  text: Attention
- children: []
  content_layer: body
  label: text
  orig: Attention
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 508.66666666666663
      coord_origin: BOTTOMLEFT
      l: 330.33333333333337
      r: 361.66666666666663
      t: 517.3333333333334
    charspan:
    - 0
    - 9
    page_no: 3
  self_ref: '#/texts/53'
  text: Attention
- children: []
  content_layer: body
  label: text
  orig: Positional
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 472.0
      coord_origin: BOTTOMLEFT
      l: 195.66666666666666
      r: 235.0
      t: 482.0
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/54'
  text: Positional
- children: []
  content_layer: body
  label: text
  orig: Positional
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 470.6666666666667
      coord_origin: BOTTOMLEFT
      l: 377.66666666666663
      r: 417.0
      t: 480.66666666666663
    charspan:
    - 0
    - 10
    page_no: 3
  self_ref: '#/texts/55'
  text: Positional
- children: []
  content_layer: body
  label: text
  orig: Encoding
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 459.6666666666667
      coord_origin: BOTTOMLEFT
      l: 195.0
      r: 234.66666666666666
      t: 472.0
    charspan:
    - 0
    - 8
    page_no: 3
  self_ref: '#/texts/56'
  text: Encoding
- children: []
  content_layer: body
  label: text
  orig: Encoding
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 457.6666666666667
      coord_origin: BOTTOMLEFT
      l: 377.66666666666663
      r: 416.66666666666663
      t: 470.6666666666667
    charspan:
    - 0
    - 8
    page_no: 3
  self_ref: '#/texts/57'
  text: Encoding
- children: []
  content_layer: body
  label: text
  orig: Input
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 448.0
      coord_origin: BOTTOMLEFT
      l: 257.6666666666667
      r: 276.3333333333333
      t: 456.6666666666667
    charspan:
    - 0
    - 5
    page_no: 3
  self_ref: '#/texts/58'
  text: Input
- children: []
  content_layer: body
  label: text
  orig: Output
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 447.6666666666667
      coord_origin: BOTTOMLEFT
      l: 332.0
      r: 357.66666666666663
      t: 457.6666666666667
    charspan:
    - 0
    - 6
    page_no: 3
  self_ref: '#/texts/59'
  text: Output
- children: []
  content_layer: body
  label: text
  orig: Embedding
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 438.6666666666667
      coord_origin: BOTTOMLEFT
      l: 246.66666666666666
      r: 287.0
      t: 448.6666666666667
    charspan:
    - 0
    - 9
    page_no: 3
  self_ref: '#/texts/60'
  text: Embedding
- children: []
  content_layer: body
  label: text
  orig: Embedding
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 438.6666666666667
      coord_origin: BOTTOMLEFT
      l: 324.66666666666663
      r: 364.33333333333337
      t: 448.6666666666667
    charspan:
    - 0
    - 9
    page_no: 3
  self_ref: '#/texts/61'
  text: Embedding
- children: []
  content_layer: body
  label: text
  orig: Inputs
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 408.0
      coord_origin: BOTTOMLEFT
      l: 253.66666666666669
      r: 280.3333333333333
      t: 418.6666666666667
    charspan:
    - 0
    - 6
    page_no: 3
  self_ref: '#/texts/62'
  text: Inputs
- children: []
  content_layer: body
  label: text
  orig: Outputs
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 408.0
      coord_origin: BOTTOMLEFT
      l: 328.33333333333337
      r: 362.33333333333337
      t: 418.6666666666667
    charspan:
    - 0
    - 7
    page_no: 3
  self_ref: '#/texts/63'
  text: Outputs
- children: []
  content_layer: body
  label: text
  orig: (shifted right)
  parent:
    $ref: '#/pictures/0'
  prov:
  - bbox:
      b: 396.3333333333333
      coord_origin: BOTTOMLEFT
      l: 318.66666666666663
      r: 371.33333333333337
      t: 408.3333333333333
    charspan:
    - 0
    - 15
    page_no: 3
  self_ref: '#/texts/64'
  text: (shifted right)
- children: []
  content_layer: body
  label: text
  orig: The Transformer follows this overall architecture using stacked self-attention
    and point-wise, fully connected layers for both the encoder and decoder, shown
    in the left and right halves of Figure 1, respectively.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 323.6671142578125
      coord_origin: BOTTOMLEFT
      l: 107.86239624023438
      r: 504.68798828125
      t: 354.4514465332031
    charspan:
    - 0
    - 213
    page_no: 3
  self_ref: '#/texts/65'
  text: The Transformer follows this overall architecture using stacked self-attention
    and point-wise, fully connected layers for both the encoder and decoder, shown
    in the left and right halves of Figure 1, respectively.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.1 Encoder and Decoder Stacks
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 301.18267822265625
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 252.72666931152344
      t: 308.256103515625
    charspan:
    - 0
    - 30
    page_no: 3
  self_ref: '#/texts/66'
  text: 3.1 Encoder and Decoder Stacks
- children: []
  content_layer: body
  label: text
  orig: 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each
    layer has two sub-layers. The first is a multi-head self-attention mechanism,
    and the second is a simple, positionwise fully connected feed-forward network.
    We employ a residual connection [11] around each of the two sub-layers, followed
    by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x
    + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
    itself. To facilitate these residual connections, all sub-layers in the model,
    as well as the embedding layers, produce outputs of dimension dmodel = 512 .'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 213.76712036132812
      coord_origin: BOTTOMLEFT
      l: 107.85356140136719
      r: 505.1749572753906
      t: 288.1884460449219
    charspan:
    - 0
    - 629
    page_no: 3
  self_ref: '#/texts/67'
  text: 'Encoder: The encoder is composed of a stack of N = 6 identical layers. Each
    layer has two sub-layers. The first is a multi-head self-attention mechanism,
    and the second is a simple, positionwise fully connected feed-forward network.
    We employ a residual connection [11] around each of the two sub-layers, followed
    by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x
    + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
    itself. To facilitate these residual connections, all sub-layers in the model,
    as well as the embedding layers, produce outputs of dimension dmodel = 512 .'
- children: []
  content_layer: body
  label: text
  orig: 'Decoder: The decoder is also composed of a stack of N = 6 identical layers.
    In addition to the two sub-layers in each encoder layer, the decoder inserts a
    third sub-layer, which performs multi-head attention over the output of the encoder
    stack. Similar to the encoder, we employ residual connections around each of the
    sub-layers, followed by layer normalization. We also modify the self-attention
    sub-layer in the decoder stack to prevent positions from attending to subsequent
    positions. This masking, combined with fact that the output embeddings are offset
    by one position, ensures that the predictions for position i can depend only on
    the known outputs at positions less than i .'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 125.1921157836914
      coord_origin: BOTTOMLEFT
      l: 108.04981231689453
      r: 503.9056701660156
      t: 199.61245727539062
    charspan:
    - 0
    - 686
    page_no: 3
  self_ref: '#/texts/68'
  text: 'Decoder: The decoder is also composed of a stack of N = 6 identical layers.
    In addition to the two sub-layers in each encoder layer, the decoder inserts a
    third sub-layer, which performs multi-head attention over the output of the encoder
    stack. Similar to the encoder, we employ residual connections around each of the
    sub-layers, followed by layer normalization. We also modify the self-attention
    sub-layer in the decoder stack to prevent positions from attending to subsequent
    positions. This masking, combined with fact that the output embeddings are offset
    by one position, ensures that the predictions for position i can depend only on
    the known outputs at positions less than i .'
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.2 Attention
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 102.75747680664062
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 170.64483642578125
      t: 109.77114868164062
    charspan:
    - 0
    - 13
    page_no: 3
  self_ref: '#/texts/69'
  text: 3.2 Attention
- children: []
  content_layer: body
  label: text
  orig: An attention function can be described as mapping a query and a set of key-value
    pairs to an output, where the query, keys, values, and output are all vectors.
    The output is computed as a weighted sum
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 107.7901382446289
      r: 504.7015686035156
      t: 89.71345520019531
    charspan:
    - 0
    - 200
    page_no: 3
  self_ref: '#/texts/70'
  text: An attention function can be described as mapping a query and a set of key-value
    pairs to an output, where the query, keys, values, and output are all vectors.
    The output is computed as a weighted sum
- children: []
  content_layer: furniture
  label: page_footer
  orig: '3'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982486724853516
      coord_origin: BOTTOMLEFT
      l: 303.9374084472656
      r: 307.8128356933594
      t: 48.846717834472656
    charspan:
    - 0
    - 1
    page_no: 3
  self_ref: '#/texts/71'
  text: '3'
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: Scaled Dot-Product Attention
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 713.115478515625
      coord_origin: BOTTOMLEFT
      l: 148.20143127441406
      r: 266.0689392089844
      t: 720.0494384765625
    charspan:
    - 0
    - 28
    page_no: 4
  self_ref: '#/texts/72'
  text: Scaled Dot-Product Attention
- children: []
  content_layer: body
  label: text
  orig: MatMul
  parent:
    $ref: '#/pictures/1'
  prov:
  - bbox:
      b: 677.3333333333334
      coord_origin: BOTTOMLEFT
      l: 203.0
      r: 229.66666666666669
      t: 686.0
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/73'
  text: MatMul
- children: []
  content_layer: body
  label: text
  orig: SoftMax
  parent:
    $ref: '#/pictures/1'
  prov:
  - bbox:
      b: 657.0
      coord_origin: BOTTOMLEFT
      l: 183.66666666666666
      r: 213.33333333333334
      t: 665.0
    charspan:
    - 0
    - 7
    page_no: 4
  self_ref: '#/texts/74'
  text: SoftMax
- children: []
  content_layer: body
  label: text
  orig: Mask (opt.)
  parent:
    $ref: '#/pictures/1'
  prov:
  - bbox:
      b: 635.3333333333333
      coord_origin: BOTTOMLEFT
      l: 179.66666666666666
      r: 217.66666666666666
      t: 644.0
    charspan:
    - 0
    - 11
    page_no: 4
  self_ref: '#/texts/75'
  text: Mask (opt.)
- children: []
  content_layer: body
  label: text
  orig: Scale
  parent:
    $ref: '#/pictures/1'
  prov:
  - bbox:
      b: 615.3333333333333
      coord_origin: BOTTOMLEFT
      l: 188.33333333333334
      r: 209.0
      t: 624.0
    charspan:
    - 0
    - 5
    page_no: 4
  self_ref: '#/texts/76'
  text: Scale
- children: []
  content_layer: body
  label: text
  orig: MatMul
  parent:
    $ref: '#/pictures/1'
  prov:
  - bbox:
      b: 594.0
      coord_origin: BOTTOMLEFT
      l: 185.66666666666666
      r: 212.33333333333334
      t: 602.6666666666667
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/77'
  text: MatMul
- children: []
  content_layer: body
  label: caption
  orig: 'Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention
    consists of several attention layers running in parallel.'
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 496.91009521484375
      coord_origin: BOTTOMLEFT
      l: 108.12194061279297
      r: 503.78375244140625
      t: 516.7854614257812
    charspan:
    - 0
    - 133
    page_no: 4
  self_ref: '#/texts/78'
  text: 'Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention
    consists of several attention layers running in parallel.'
- children: []
  content_layer: body
  label: text
  orig: Multi-Head Attention
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 713.1553344726562
      coord_origin: BOTTOMLEFT
      l: 363.7055358886719
      r: 450.0513916015625
      t: 720.0494384765625
    charspan:
    - 0
    - 20
    page_no: 4
  self_ref: '#/texts/79'
  text: Multi-Head Attention
- children: []
  content_layer: body
  label: text
  orig: Linear
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 689.0
      coord_origin: BOTTOMLEFT
      l: 387.0
      r: 409.0
      t: 697.6666666666666
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/80'
  text: Linear
- children: []
  content_layer: body
  label: text
  orig: Concat
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 663.6666666666666
      coord_origin: BOTTOMLEFT
      l: 385.0
      r: 411.0
      t: 672.3333333333334
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/81'
  text: Concat
- children: []
  content_layer: body
  label: text
  orig: Scaled Dot-Product
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 633.6666666666667
      coord_origin: BOTTOMLEFT
      l: 364.3333333333333
      r: 432.3333333333333
      t: 642.3333333333333
    charspan:
    - 0
    - 18
    page_no: 4
  self_ref: '#/texts/82'
  text: Scaled Dot-Product
- children: []
  content_layer: body
  label: text
  orig: Attention
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 624.3333333333333
      coord_origin: BOTTOMLEFT
      l: 383.6666666666667
      r: 415.0
      t: 633.0
    charspan:
    - 0
    - 9
    page_no: 4
  self_ref: '#/texts/83'
  text: Attention
- children: []
  content_layer: body
  label: text
  orig: Linear
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 595.0
      coord_origin: BOTTOMLEFT
      l: 351.6666666666667
      r: 373.6666666666667
      t: 603.6666666666667
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/84'
  text: Linear
- children: []
  content_layer: body
  label: text
  orig: Linear
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 595.0
      coord_origin: BOTTOMLEFT
      l: 386.3333333333333
      r: 409.0
      t: 603.6666666666667
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/85'
  text: Linear
- children: []
  content_layer: body
  label: text
  orig: Linear
  parent:
    $ref: '#/pictures/2'
  prov:
  - bbox:
      b: 595.0
      coord_origin: BOTTOMLEFT
      l: 421.6666666666667
      r: 444.3333333333333
      t: 603.6666666666667
    charspan:
    - 0
    - 6
    page_no: 4
  self_ref: '#/texts/86'
  text: Linear
- children: []
  content_layer: body
  label: text
  orig: of the values, where the weight assigned to each value is computed by a compatibility
    function of the query with the corresponding key.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 454.1950988769531
      coord_origin: BOTTOMLEFT
      l: 108.23910522460938
      r: 503.79937744140625
      t: 474.0704345703125
    charspan:
    - 0
    - 135
    page_no: 4
  self_ref: '#/texts/87'
  text: of the values, where the weight assigned to each value is computed by a compatibility
    function of the query with the corresponding key.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.2.1 Scaled Dot-Product Attention
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 433.11767578125
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 263.7154235839844
      t: 440.19110107421875
    charspan:
    - 0
    - 34
    page_no: 4
  self_ref: '#/texts/88'
  text: 3.2.1 Scaled Dot-Product Attention
- children: []
  content_layer: body
  label: text
  orig: "We call our particular attention \"Scaled Dot-Product Attention\" (Figure\
    \ 2). The input consists of queries and keys of dimension dk, and values of dimension\
    \ d v . We compute the dot products of the query with all keys, divide each by\
    \ \u221Adk, and apply a softmax function to obtain the weights on the values."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 381.81048583984375
      coord_origin: BOTTOMLEFT
      l: 107.58280944824219
      r: 504.50469970703125
      t: 421.471435546875
    charspan:
    - 0
    - 303
    page_no: 4
  self_ref: '#/texts/89'
  text: "We call our particular attention \"Scaled Dot-Product Attention\" (Figure\
    \ 2). The input consists of queries and keys of dimension dk, and values of dimension\
    \ d v . We compute the dot products of the query with all keys, divide each by\
    \ \u221Adk, and apply a softmax function to obtain the weights on the values."
- children: []
  content_layer: body
  label: text
  orig: 'In practice, we compute the attention function on a set of queries simultaneously,
    packed together into a matrix Q. The keys and values are also packed together
    into matrices K and V . We compute the matrix of outputs as:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 341.5810852050781
      coord_origin: BOTTOMLEFT
      l: 108.1295166015625
      r: 504.190185546875
      t: 372.3554382324219
    charspan:
    - 0
    - 221
    page_no: 4
  self_ref: '#/texts/90'
  text: 'In practice, we compute the attention function on a set of queries simultaneously,
    packed together into a matrix Q. The keys and values are also packed together
    into matrices K and V . We compute the matrix of outputs as:'
- children: []
  content_layer: body
  label: formula
  orig: "Attention(Q, K, V ) = softmax( QK T \u221A\r\ndk )V (1)"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 301.9728698730469
      coord_origin: BOTTOMLEFT
      l: 220.28880310058594
      r: 504.1891784667969
      t: 326.29730224609375
    charspan:
    - 0
    - 48
    page_no: 4
  self_ref: '#/texts/91'
  text: ''
- children: []
  content_layer: body
  label: text
  orig: "The two most commonly used attention functions are additive attention [2],\
    \ and dot-product (multiplicative) attention. Dot-product attention is identical\
    \ to our algorithm, except for the scaling factor of \u221A 1 dk . Additive attention\
    \ computes the compatibility function using a feed-forward network with a single\
    \ hidden layer. While the two are similar in theoretical complexity, dot-product\
    \ attention is much faster and more space-efficient in practice, since it can\
    \ be implemented using highly optimized matrix multiplication code."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 225.96807861328125
      coord_origin: BOTTOMLEFT
      l: 107.86121368408203
      r: 505.17218017578125
      t: 291.7304382324219
    charspan:
    - 0
    - 532
    page_no: 4
  self_ref: '#/texts/92'
  text: "The two most commonly used attention functions are additive attention [2],\
    \ and dot-product (multiplicative) attention. Dot-product attention is identical\
    \ to our algorithm, except for the scaling factor of \u221A 1 dk . Additive attention\
    \ computes the compatibility function using a feed-forward network with a single\
    \ hidden layer. While the two are similar in theoretical complexity, dot-product\
    \ attention is much faster and more space-efficient in practice, since it can\
    \ be implemented using highly optimized matrix multiplication code."
- children: []
  content_layer: body
  label: text
  orig: "While for small values of d k the two mechanisms perform similarly, additive\
    \ attention outperforms dot product attention without scaling for larger values\
    \ of dk [3]. We suspect that for large values of d k , the dot products grow large\
    \ in magnitude, pushing the softmax function into regions where it has extremely\
    \ small gradients 4 . To counteract this effect, we scale the dot products by\
    \ \u221A 1 dk ."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 173.59388732910156
      coord_origin: BOTTOMLEFT
      l: 107.58210754394531
      r: 504.50048828125
      t: 218.64505004882812
    charspan:
    - 0
    - 399
    page_no: 4
  self_ref: '#/texts/93'
  text: "While for small values of d k the two mechanisms perform similarly, additive\
    \ attention outperforms dot product attention without scaling for larger values\
    \ of dk [3]. We suspect that for large values of d k , the dot products grow large\
    \ in magnitude, pushing the softmax function into regions where it has extremely\
    \ small gradients 4 . To counteract this effect, we scale the dot products by\
    \ \u221A 1 dk ."
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.2.2 Multi-Head Attention
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 153.4434814453125
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 230.42041015625
      t: 160.4571533203125
    charspan:
    - 0
    - 26
    page_no: 4
  self_ref: '#/texts/94'
  text: 3.2.2 Multi-Head Attention
- children: []
  content_layer: body
  label: text
  orig: Instead of performing a single attention function with dmodel-dimensional
    keys, values and queries, we found it beneficial to linearly project the queries,
    keys and values h times with different, learned linear projections to dk , d k
    and d v dimensions, respectively. On each of these projected versions of queries,
    keys and values we then perform the attention function in parallel, yielding d
    v -dimensional
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 100.05412292480469
      coord_origin: BOTTOMLEFT
      l: 107.85000610351562
      r: 504.68524169921875
      t: 141.85704040527344
    charspan:
    - 0
    - 410
    page_no: 4
  self_ref: '#/texts/95'
  text: Instead of performing a single attention function with dmodel-dimensional
    keys, values and queries, we found it beneficial to linearly project the queries,
    keys and values h times with different, learned linear projections to dk , d k
    and d v dimensions, respectively. On each of these projected versions of queries,
    keys and values we then perform the attention function in parallel, yielding d
    v -dimensional
- children: []
  content_layer: body
  label: footnote
  orig: "4 To illustrate why the dot products get large, assume that the components\
    \ of q and k are independent random variables with mean 0 and variance 1. Then\
    \ their dot product, q \xB7 k =\r\nP dk i=1 qiki, has mean 0 and variance dk ."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.20620727539062
      coord_origin: BOTTOMLEFT
      l: 107.94585418701172
      r: 503.9755554199219
      t: 91.1998519897461
    charspan:
    - 0
    - 223
    page_no: 4
  self_ref: '#/texts/96'
  text: "4 To illustrate why the dot products get large, assume that the components\
    \ of q and k are independent random variables with mean 0 and variance 1. Then\
    \ their dot product, q \xB7 k =\r\nP dk i=1 qiki, has mean 0 and variance dk ."
- children: []
  content_layer: furniture
  label: page_footer
  orig: '4'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 42.11199951171875
      coord_origin: BOTTOMLEFT
      l: 303.6285400390625
      r: 308.21136474609375
      t: 48.846717834472656
    charspan:
    - 0
    - 1
    page_no: 4
  self_ref: '#/texts/97'
  text: '4'
- children: []
  content_layer: body
  label: text
  orig: output values. These are concatenated and once again projected, resulting
    in the final values, as depicted in Figure 2.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 696.9661254882812
      coord_origin: BOTTOMLEFT
      l: 108.26898956298828
      r: 503.58050537109375
      t: 716.8414306640625
    charspan:
    - 0
    - 119
    page_no: 5
  self_ref: '#/texts/98'
  text: output values. These are concatenated and once again projected, resulting
    in the final values, as depicted in Figure 2.
- children: []
  content_layer: body
  label: text
  orig: Multi-head attention allows the model to jointly attend to information from
    different representation subspaces at different positions. With a single attention
    head, averaging inhibits this.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 669.6691284179688
      coord_origin: BOTTOMLEFT
      l: 108.12002563476562
      r: 503.8468017578125
      t: 689.54443359375
    charspan:
    - 0
    - 189
    page_no: 5
  self_ref: '#/texts/99'
  text: Multi-head attention allows the model to jointly attend to information from
    different representation subspaces at different positions. With a single attention
    head, averaging inhibits this.
- children: []
  content_layer: body
  label: formula
  orig: O
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 640.5985107421875
      coord_origin: BOTTOMLEFT
      l: 405.3432312011719
      r: 410.643310546875
      t: 645.6336059570312
    charspan:
    - 0
    - 1
    page_no: 5
  self_ref: '#/texts/100'
  text: ''
- children: []
  content_layer: body
  label: formula
  orig: "MultiHead(Q, K, V ) = Concat(head1 , ..., head h )W where head i = Attention(QW\r\
    \ni Q W\r\ni , KW \r\ni K W \r\ni , V W\r\ni V W\r\ni )"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 616.9672241210938
      coord_origin: BOTTOMLEFT
      l: 187.30862426757812
      r: 424.0652160644531
      t: 644.0899047851562
    charspan:
    - 0
    - 124
    page_no: 5
  self_ref: '#/texts/101'
  text: ''
- children: []
  content_layer: body
  label: text
  orig: "Where the projections are parameter matrices W\r\ni Q W\r\ni \u2208 R dmodel\xD7\
    dk , W \r\ni K W \r\ni \u2208 R dmodel\xD7dk , W\r\ni V W\r\ni \u2208 R dmodel\xD7\
    d v and W O \u2208 R hd v \xD7dmodel ."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 566.0714721679688
      coord_origin: BOTTOMLEFT
      l: 107.58081817626953
      r: 502.4200134277344
      t: 588.381591796875
    charspan:
    - 0
    - 156
    page_no: 5
  self_ref: '#/texts/102'
  text: "Where the projections are parameter matrices W\r\ni Q W\r\ni \u2208 R dmodel\xD7\
    dk , W \r\ni K W \r\ni \u2208 R dmodel\xD7dk , W\r\ni V W\r\ni \u2208 R dmodel\xD7\
    d v and W O \u2208 R hd v \xD7dmodel ."
- children: []
  content_layer: body
  label: text
  orig: In this work we employ h = 8 parallel attention layers, or heads. For each
    of these we use d k = d v = d model /h = 64. Due to the reduced dimension of each
    head, the total computational cost is similar to that of single-head attention
    with full dimensionality.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 526.0911254882812
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 504.0072021484375
      t: 556.986083984375
    charspan:
    - 0
    - 261
    page_no: 5
  self_ref: '#/texts/103'
  text: In this work we employ h = 8 parallel attention layers, or heads. For each
    of these we use d k = d v = d model /h = 64. Due to the reduced dimension of each
    head, the total computational cost is similar to that of single-head attention
    with full dimensionality.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.2.3 Applications of Attention in our Model
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 502.0486145019531
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 302.6292724609375
      t: 510.96514892578125
    charspan:
    - 0
    - 44
    page_no: 5
  self_ref: '#/texts/104'
  text: 3.2.3 Applications of Attention in our Model
- children: []
  content_layer: body
  label: text
  orig: 'The Transformer uses multi-head attention in three different ways:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 482.8650817871094
      coord_origin: BOTTOMLEFT
      l: 107.86036682128906
      r: 371.7497253417969
      t: 491.8314208984375
    charspan:
    - 0
    - 66
    page_no: 5
  self_ref: '#/texts/105'
  text: 'The Transformer uses multi-head attention in three different ways:'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: "\xB7 In \"encoder-decoder attention\" layers, the queries come from the previous\
    \ decoder layer, and the memory keys and values come from the output of the encoder.\
    \ This allows every position in the decoder to attend over all positions in the\
    \ input sequence. This mimics the typical encoder-decoder attention mechanisms\
    \ in sequence-to-sequence models such as [38, 2, 9]."
  parent:
    $ref: '#/groups/1'
  prov:
  - bbox:
      b: 419.0018005371094
      coord_origin: BOTTOMLEFT
      l: 135.79550170898438
      r: 504.6828308105469
      t: 470.9874572753906
    charspan:
    - 0
    - 364
    page_no: 5
  self_ref: '#/texts/106'
  text: "\xB7 In \"encoder-decoder attention\" layers, the queries come from the previous\
    \ decoder layer, and the memory keys and values come from the output of the encoder.\
    \ This allows every position in the decoder to attend over all positions in the\
    \ input sequence. This mimics the typical encoder-decoder attention mechanisms\
    \ in sequence-to-sequence models such as [38, 2, 9]."
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: "\xB7 The encoder contains self-attention layers. In a self-attention layer\
    \ all of the keys, values and queries come from the same place, in this case,\
    \ the output of the previous layer in the encoder. Each position in the encoder\
    \ can attend to all positions in the previous layer of the encoder."
  parent:
    $ref: '#/groups/1'
  prov:
  - bbox:
      b: 371.88336181640625
      coord_origin: BOTTOMLEFT
      l: 135.79550170898438
      r: 503.8063049316406
      t: 411.5144348144531
    charspan:
    - 0
    - 291
    page_no: 5
  self_ref: '#/texts/107'
  text: "\xB7 The encoder contains self-attention layers. In a self-attention layer\
    \ all of the keys, values and queries come from the same place, in this case,\
    \ the output of the previous layer in the encoder. Each position in the encoder\
    \ can attend to all positions in the previous layer of the encoder."
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: "\xB7 Similarly, self-attention layers in the decoder allow each position\
    \ in the decoder to attend to all positions in the decoder up to and including\
    \ that position. We need to prevent leftward information flow in the decoder to\
    \ preserve the auto-regressive property. We implement this inside of scaled dot-product\
    \ attention by masking out (setting to \u2212\u221E) all values in the input of\
    \ the softmax which correspond to illegal connections. See Figure 2."
  parent:
    $ref: '#/groups/1'
  prov:
  - bbox:
      b: 310.3490905761719
      coord_origin: BOTTOMLEFT
      l: 135.79550170898438
      r: 504.0054626464844
      t: 362.9514465332031
    charspan:
    - 0
    - 445
    page_no: 5
  self_ref: '#/texts/108'
  text: "\xB7 Similarly, self-attention layers in the decoder allow each position\
    \ in the decoder to attend to all positions in the decoder up to and including\
    \ that position. We need to prevent leftward information flow in the decoder to\
    \ preserve the auto-regressive property. We implement this inside of scaled dot-product\
    \ attention by masking out (setting to \u2212\u221E) all values in the input of\
    \ the softmax which correspond to illegal connections. See Figure 2."
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.3 Position-wise Feed-Forward Networks
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 286.8236083984375
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 292.5870361328125
      t: 293.87713623046875
    charspan:
    - 0
    - 39
    page_no: 5
  self_ref: '#/texts/109'
  text: 3.3 Position-wise Feed-Forward Networks
- children: []
  content_layer: body
  label: text
  orig: In addition to attention sub-layers, each of the layers in our encoder and
    decoder contains a fully connected feed-forward network, which is applied to each
    position separately and identically. This consists of two linear transformations
    with a ReLU activation in between.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 244.646484375
      coord_origin: BOTTOMLEFT
      l: 108.18291473388672
      r: 504.0885925292969
      t: 273.3984375
    charspan:
    - 0
    - 272
    page_no: 5
  self_ref: '#/texts/110'
  text: In addition to attention sub-layers, each of the layers in our encoder and
    decoder contains a fully connected feed-forward network, which is applied to each
    position separately and identically. This consists of two linear transformations
    with a ReLU activation in between.
- children: []
  content_layer: body
  label: formula
  orig: FFN(x) = max(0, xW1 + b1)W2 + b2 (2)
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 213.5853271484375
      coord_origin: BOTTOMLEFT
      l: 227.22976684570312
      r: 504.18914794921875
      t: 223.5379638671875
    charspan:
    - 0
    - 36
    page_no: 5
  self_ref: '#/texts/111'
  text: ''
- children: []
  content_layer: body
  label: text
  orig: While the linear transformations are the same across different positions,
    they use different parameters from layer to layer. Another way of describing this
    is as two convolutions with kernel size 1. The dimensionality of input and output
    is dmodel = 512, and the inner-layer has dimensionality df f = 2048 .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 158.64231872558594
      coord_origin: BOTTOMLEFT
      l: 107.58081817626953
      r: 505.0438232421875
      t: 201.08445739746094
    charspan:
    - 0
    - 307
    page_no: 5
  self_ref: '#/texts/112'
  text: While the linear transformations are the same across different positions,
    they use different parameters from layer to layer. Another way of describing this
    is as two convolutions with kernel size 1. The dimensionality of input and output
    is dmodel = 512, and the inner-layer has dimensionality df f = 2048 .
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.4 Embeddings and Softmax
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 133.99266052246094
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 239.86495971679688
      t: 142.92910766601562
    charspan:
    - 0
    - 26
    page_no: 5
  self_ref: '#/texts/113'
  text: 3.4 Embeddings and Softmax
- children: []
  content_layer: body
  label: text
  orig: "Similarly to other sequence transduction models, we use learned embeddings\
    \ to convert the input tokens and output tokens to vectors of dimension dmodel.\
    \ We also use the usual learned linear transformation and softmax function to\
    \ convert the decoder output to predicted next-token probabilities. In our model,\
    \ we share the same weight matrix between the two embedding layers and the pre-softmax\
    \ linear transformation, similar to [30]. In the embedding layers, we multiply\
    \ those weights by \u221Admodel ."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 108.12692260742188
      r: 505.1803894042969
      t: 122.44046020507812
    charspan:
    - 0
    - 497
    page_no: 5
  self_ref: '#/texts/114'
  text: "Similarly to other sequence transduction models, we use learned embeddings\
    \ to convert the input tokens and output tokens to vectors of dimension dmodel.\
    \ We also use the usual learned linear transformation and softmax function to\
    \ convert the decoder output to predicted next-token probabilities. In our model,\
    \ we share the same weight matrix between the two embedding layers and the pre-softmax\
    \ linear transformation, similar to [30]. In the embedding layers, we multiply\
    \ those weights by \u221Admodel ."
- children: []
  content_layer: furniture
  label: page_footer
  orig: '5'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982486724853516
      coord_origin: BOTTOMLEFT
      l: 303.82781982421875
      r: 307.87261962890625
      t: 48.96626663208008
    charspan:
    - 0
    - 1
    page_no: 5
  self_ref: '#/texts/115'
  text: '5'
- children: []
  content_layer: body
  label: caption
  orig: 'Table 1: Maximum path lengths, per-layer complexity and minimum number of
    sequential operations for different layer types. n is the sequence length, d is
    the representation dimension, k is the kernel size of convolutions and r the size
    of the neighborhood in restricted self-attention.'
  parent:
    $ref: '#/tables/0'
  prov:
  - bbox:
      b: 689.26513671875
      coord_origin: BOTTOMLEFT
      l: 107.85697937011719
      r: 503.79327392578125
      t: 720.0494384765625
    charspan:
    - 0
    - 285
    page_no: 6
  self_ref: '#/texts/116'
  text: 'Table 1: Maximum path lengths, per-layer complexity and minimum number of
    sequential operations for different layer types. n is the sequence length, d is
    the representation dimension, k is the kernel size of convolutions and r the size
    of the neighborhood in restricted self-attention.'
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 3.5 Positional Encoding
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 568.5206298828125
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 215.02821350097656
      t: 577.4471435546875
    charspan:
    - 0
    - 23
    page_no: 6
  self_ref: '#/texts/117'
  text: 3.5 Positional Encoding
- children: []
  content_layer: body
  label: text
  orig: Since our model contains no recurrence and no convolution, in order for the
    model to make use of the order of the sequence, we must inject some information
    about the relative or absolute position of the tokens in the sequence. To this
    end, we add "positional encodings" to the input embeddings at the bottoms of the
    encoder and decoder stacks. The positional encodings have the same dimension dmodel
    as the embeddings, so that the two can be summed. There are many choices of positional
    encodings, learned and fixed [9].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 494.4988098144531
      coord_origin: BOTTOMLEFT
      l: 108.02928924560547
      r: 504.7036437988281
      t: 557.3924560546875
    charspan:
    - 0
    - 520
    page_no: 6
  self_ref: '#/texts/118'
  text: Since our model contains no recurrence and no convolution, in order for the
    model to make use of the order of the sequence, we must inject some information
    about the relative or absolute position of the tokens in the sequence. To this
    end, we add "positional encodings" to the input embeddings at the bottoms of the
    encoder and decoder stacks. The positional encodings have the same dimension dmodel
    as the embeddings, so that the two can be summed. There are many choices of positional
    encodings, learned and fixed [9].
- children: []
  content_layer: body
  label: text
  orig: 'In this work, we use sine and cosine functions of different frequencies:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 477.5020751953125
      coord_origin: BOTTOMLEFT
      l: 108.17932891845703
      r: 389.00506591796875
      t: 486.45843505859375
    charspan:
    - 0
    - 72
    page_no: 6
  self_ref: '#/texts/119'
  text: 'In this work, we use sine and cosine functions of different frequencies:'
- children: []
  content_layer: body
  label: formula
  orig: P E (pos,2i) = sin(pos/10000 2i/dmodel ) P E (pos,2i+1) = cos (pos/10000 2i/dmodel
    )
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 425.8705139160156
      coord_origin: BOTTOMLEFT
      l: 226.09249877929688
      r: 385.3111877441406
      t: 455.61334228515625
    charspan:
    - 0
    - 84
    page_no: 6
  self_ref: '#/texts/120'
  text: ''
- children: []
  content_layer: body
  label: text
  orig: "where pos is the position and i is the dimension. That is, each dimension\
    \ of the positional encoding corresponds to a sinusoid. The wavelengths form a\
    \ geometric progression from 2\u03C0 to 10000 \xB7 2\u03C0. We chose this function\
    \ because we hypothesized it would allow the model to easily learn to attend by\
    \ relative positions, since for any fixed offset k , P Ep Epos+k can be represented\
    \ as a linear function of P Ep Epos ."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 360.4470520019531
      coord_origin: BOTTOMLEFT
      l: 107.85000610351562
      r: 504.5090637207031
      t: 413.7274475097656
    charspan:
    - 0
    - 414
    page_no: 6
  self_ref: '#/texts/121'
  text: "where pos is the position and i is the dimension. That is, each dimension\
    \ of the positional encoding corresponds to a sinusoid. The wavelengths form a\
    \ geometric progression from 2\u03C0 to 10000 \xB7 2\u03C0. We chose this function\
    \ because we hypothesized it would allow the model to easily learn to attend by\
    \ relative positions, since for any fixed offset k , P Ep Epos+k can be represented\
    \ as a linear function of P Ep Epos ."
- children: []
  content_layer: body
  label: text
  orig: We also experimented with using learned positional embeddings [9] instead,
    and found that the two versions produced nearly identical results (see Table 3
    row (E)). We chose the sinusoidal version because it may allow the model to extrapolate
    to sequence lengths longer than the ones encountered during training.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 312.00909423828125
      coord_origin: BOTTOMLEFT
      l: 107.58186340332031
      r: 503.9079284667969
      t: 353.70245361328125
    charspan:
    - 0
    - 311
    page_no: 6
  self_ref: '#/texts/122'
  text: We also experimented with using learned positional embeddings [9] instead,
    and found that the two versions produced nearly identical results (see Table 3
    row (E)). We chose the sinusoidal version because it may allow the model to extrapolate
    to sequence lengths longer than the ones encountered during training.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 4 Why Self-Attention
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 283.49810791015625
      coord_origin: BOTTOMLEFT
      l: 108.2271499633789
      r: 224.83816528320312
      t: 294.2099914550781
    charspan:
    - 0
    - 20
    page_no: 6
  self_ref: '#/texts/123'
  text: 4 Why Self-Attention
- children: []
  content_layer: body
  label: text
  orig: "In this section we compare various aspects of self-attention layers to the\
    \ recurrent and convolutional layers commonly used for mapping one variable-length\
    \ sequence of symbol representations (x1, ..., x n ) to another sequence of equal\
    \ length (z1, ..., z n ), with xi, zi \u2208 R d , such as a hidden layer in a\
    \ typical sequence transduction encoder or decoder. Motivating our use of self-attention\
    \ we consider three desiderata."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 219.24737548828125
      coord_origin: BOTTOMLEFT
      l: 107.82029724121094
      r: 505.165771484375
      t: 269.7884521484375
    charspan:
    - 0
    - 424
    page_no: 6
  self_ref: '#/texts/124'
  text: "In this section we compare various aspects of self-attention layers to the\
    \ recurrent and convolutional layers commonly used for mapping one variable-length\
    \ sequence of symbol representations (x1, ..., x n ) to another sequence of equal\
    \ length (z1, ..., z n ), with xi, zi \u2208 R d , such as a hidden layer in a\
    \ typical sequence transduction encoder or decoder. Motivating our use of self-attention\
    \ we consider three desiderata."
- children: []
  content_layer: body
  label: text
  orig: One is the total computational complexity per layer. Another is the amount
    of computation that can be parallelized, as measured by the minimum number of
    sequential operations required.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 189.88812255859375
      coord_origin: BOTTOMLEFT
      l: 108.0298843383789
      r: 503.8508605957031
      t: 209.76345825195312
    charspan:
    - 0
    - 184
    page_no: 6
  self_ref: '#/texts/125'
  text: One is the total computational complexity per layer. Another is the amount
    of computation that can be parallelized, as measured by the minimum number of
    sequential operations required.
- children: []
  content_layer: body
  label: text
  orig: The third is the path length between long-range dependencies in the network.
    Learning long-range dependencies is a key challenge in many sequence transduction
    tasks. One key factor affecting the ability to learn such dependencies is the
    length of the paths forward and backward signals have to traverse in the network.
    The shorter these paths between any combination of positions in the input and
    output sequences, the easier it is to learn long-range dependencies [12]. Hence
    we also compare the maximum path length between any two input and output positions
    in networks composed of the different layer types.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 108.04512023925781
      coord_origin: BOTTOMLEFT
      l: 107.86222839355469
      r: 504.007568359375
      t: 182.4654541015625
    charspan:
    - 0
    - 610
    page_no: 6
  self_ref: '#/texts/126'
  text: The third is the path length between long-range dependencies in the network.
    Learning long-range dependencies is a key challenge in many sequence transduction
    tasks. One key factor affecting the ability to learn such dependencies is the
    length of the paths forward and backward signals have to traverse in the network.
    The shorter these paths between any combination of positions in the input and
    output sequences, the easier it is to learn long-range dependencies [12]. Hence
    we also compare the maximum path length between any two input and output positions
    in networks composed of the different layer types.
- children: []
  content_layer: body
  label: text
  orig: As noted in Table 1, a self-attention layer connects all positions with a
    constant number of sequentially executed operations, whereas a recurrent layer
    requires O(n) sequential operations. In terms of computational complexity, self-attention
    layers are faster than recurrent layers when the sequence
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 107.7874526977539
      r: 504.50836181640625
      t: 100.6224594116211
    charspan:
    - 0
    - 300
    page_no: 6
  self_ref: '#/texts/127'
  text: As noted in Table 1, a self-attention layer connects all positions with a
    constant number of sequentially executed operations, whereas a recurrent layer
    requires O(n) sequential operations. In terms of computational complexity, self-attention
    layers are faster than recurrent layers when the sequence
- children: []
  content_layer: furniture
  label: page_footer
  orig: '6'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982486724853516
      coord_origin: BOTTOMLEFT
      l: 303.84771728515625
      r: 308.1715087890625
      t: 48.92641830444336
    charspan:
    - 0
    - 1
    page_no: 6
  self_ref: '#/texts/128'
  text: '6'
- children: []
  content_layer: body
  label: text
  orig: length n is smaller than the representation dimensionality d, which is most
    often the case with sentence representations used by state-of-the-art models in
    machine translations, such as word-piece [38] and byte-pair [31] representations.
    To improve computational performance for tasks involving very long sequences,
    self-attention could be restricted to considering only a neighborhood of size
    r in the input sequence centered around the respective output position. This would
    increase the maximum path length to O(n/r). We plan to investigate this approach
    further in future work.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 653.0112915039062
      coord_origin: BOTTOMLEFT
      l: 107.9365005493164
      r: 503.9720153808594
      t: 716.9510498046875
    charspan:
    - 0
    - 581
    page_no: 7
  self_ref: '#/texts/129'
  text: length n is smaller than the representation dimensionality d, which is most
    often the case with sentence representations used by state-of-the-art models in
    machine translations, such as word-piece [38] and byte-pair [31] representations.
    To improve computational performance for tasks involving very long sequences,
    self-attention could be restricted to considering only a neighborhood of size
    r in the input sequence centered around the respective output position. This would
    increase the maximum path length to O(n/r). We plan to investigate this approach
    further in future work.
- children: []
  content_layer: body
  label: text
  orig: "A single convolutional layer with kernel width k < n does not connect all\
    \ pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional\
    \ layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated\
    \ convolutions [18], increasing the length of the longest paths between any two\
    \ positions in the network. Convolutional layers are generally more expensive\
    \ than recurrent layers, by a factor of k. Separable convolutions [6], however,\
    \ decrease the complexity considerably, to O(k \xB7 n \xB7 d + n \xB7 d 2 ). Even\
    \ with k = n, however, the complexity of a separable convolution is equal to the\
    \ combination of a self-attention layer and a point-wise feed-forward layer, the\
    \ approach we take in our model."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 560.5880737304688
      coord_origin: BOTTOMLEFT
      l: 107.79118347167969
      r: 504.7054443359375
      t: 646.01708984375
    charspan:
    - 0
    - 731
    page_no: 7
  self_ref: '#/texts/130'
  text: "A single convolutional layer with kernel width k < n does not connect all\
    \ pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional\
    \ layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated\
    \ convolutions [18], increasing the length of the longest paths between any two\
    \ positions in the network. Convolutional layers are generally more expensive\
    \ than recurrent layers, by a factor of k. Separable convolutions [6], however,\
    \ decrease the complexity considerably, to O(k \xB7 n \xB7 d + n \xB7 d 2 ). Even\
    \ with k = n, however, the complexity of a separable convolution is equal to the\
    \ combination of a self-attention layer and a point-wise feed-forward layer, the\
    \ approach we take in our model."
- children: []
  content_layer: body
  label: text
  orig: As side benefit, self-attention could yield more interpretable models. We
    inspect attention distributions from our models and present and discuss examples
    in the appendix. Not only do individual attention heads clearly learn to perform
    different tasks, many appear to exhibit behavior related to the syntactic and
    semantic structure of the sentences.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 513.5244140625
      coord_origin: BOTTOMLEFT
      l: 107.7874526977539
      r: 503.85028076171875
      t: 553.1554565429688
    charspan:
    - 0
    - 350
    page_no: 7
  self_ref: '#/texts/131'
  text: As side benefit, self-attention could yield more interpretable models. We
    inspect attention distributions from our models and present and discuss examples
    in the appendix. Not only do individual attention heads clearly learn to perform
    different tasks, many appear to exhibit behavior related to the syntactic and
    semantic structure of the sentences.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5 Training
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 480.459228515625
      coord_origin: BOTTOMLEFT
      l: 108.26301574707031
      r: 170.02359008789062
      t: 491.17108154296875
    charspan:
    - 0
    - 10
    page_no: 7
  self_ref: '#/texts/132'
  text: 5 Training
- children: []
  content_layer: body
  label: text
  orig: This section describes the training regime for our models.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 456.2431335449219
      coord_origin: BOTTOMLEFT
      l: 107.86036682128906
      r: 336.7908935546875
      t: 465.20947265625
    charspan:
    - 0
    - 58
    page_no: 7
  self_ref: '#/texts/133'
  text: This section describes the training regime for our models.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.1 Training Data and Batching
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 429.5837097167969
      coord_origin: BOTTOMLEFT
      l: 108.21917724609375
      r: 249.35931396484375
      t: 438.51019287109375
    charspan:
    - 0
    - 30
    page_no: 7
  self_ref: '#/texts/134'
  text: 5.1 Training Data and Batching
- children: []
  content_layer: body
  label: text
  orig: We trained on the standard WMT 2014 English-German dataset consisting of about
    4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3],
    which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French,
    we used the significantly larger WMT 2014 English-French dataset consisting of
    36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence
    pairs were batched together by approximate sequence length. Each training batch
    contained a set of sentence pairs containing approximately 25000 source tokens
    and 25000 target tokens.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 343.1061096191406
      coord_origin: BOTTOMLEFT
      l: 107.58280944824219
      r: 505.1665344238281
      t: 417.5772705078125
    charspan:
    - 0
    - 589
    page_no: 7
  self_ref: '#/texts/135'
  text: We trained on the standard WMT 2014 English-German dataset consisting of about
    4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3],
    which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French,
    we used the significantly larger WMT 2014 English-French dataset consisting of
    36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence
    pairs were batched together by approximate sequence length. Each training batch
    contained a set of sentence pairs containing approximately 25000 source tokens
    and 25000 target tokens.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.2 Hardware and Schedule
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 318.3106689453125
      coord_origin: BOTTOMLEFT
      l: 108.21917724609375
      r: 232.86126708984375
      t: 325.38409423828125
    charspan:
    - 0
    - 25
    page_no: 7
  self_ref: '#/texts/136'
  text: 5.2 Hardware and Schedule
- children: []
  content_layer: body
  label: text
  orig: We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base
    models using the hyperparameters described throughout the paper, each training
    step took about 0.4 seconds. We trained the base models for a total of 100,000
    steps or 12 hours. For our big models,(described on the bottom line of table 3),
    step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5
    days).
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 251.78811645507812
      coord_origin: BOTTOMLEFT
      l: 107.58280944824219
      r: 503.8059997558594
      t: 304.39044189453125
    charspan:
    - 0
    - 398
    page_no: 7
  self_ref: '#/texts/137'
  text: We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base
    models using the hyperparameters described throughout the paper, each training
    step took about 0.4 seconds. We trained the base models for a total of 100,000
    steps or 12 hours. For our big models,(described on the bottom line of table 3),
    step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5
    days).
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.3 Optimizer
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 225.1386260986328
      coord_origin: BOTTOMLEFT
      l: 108.21917724609375
      r: 174.0321044921875
      t: 234.05514526367188
    charspan:
    - 0
    - 13
    page_no: 7
  self_ref: '#/texts/138'
  text: 5.3 Optimizer
- children: []
  content_layer: body
  label: text
  orig: "We used the Adam optimizer [20] with \u03B21 = 0 . 9 , \u03B22 = 0 . 98 and\
    \ \u03F5 = 10 \u2212 9 . We varied the learning rate over the course of training,\
    \ according to the formula:"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 193.19711303710938
      coord_origin: BOTTOMLEFT
      l: 107.58261108398438
      r: 503.69927978515625
      t: 214.51361083984375
    charspan:
    - 0
    - 161
    page_no: 7
  self_ref: '#/texts/139'
  text: "We used the Adam optimizer [20] with \u03B21 = 0 . 9 , \u03B22 = 0 . 98 and\
    \ \u03F5 = 10 \u2212 9 . We varied the learning rate over the course of training,\
    \ according to the formula:"
- children: []
  content_layer: body
  label: formula
  orig: "lrate = d \u2212 0 . 5 model \xB7 min(step _ num \u2212 0 . 5 , step _ num\
    \ \xB7 warmup _ steps \u2212 1 . 5 ) (3)"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 162.0142364501953
      coord_origin: BOTTOMLEFT
      l: 163.33035278320312
      r: 504.18914794921875
      t: 173.85958862304688
    charspan:
    - 0
    - 92
    page_no: 7
  self_ref: '#/texts/140'
  text: ''
- children: []
  content_layer: body
  label: text
  orig: This corresponds to increasing the learning rate linearly for the first warmup
    _ steps training steps, and decreasing it thereafter proportionally to the inverse
    square root of the step number. We used warmup _ steps = 4000 .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 117.75921630859375
      coord_origin: BOTTOMLEFT
      l: 107.86222839355469
      r: 504.68988037109375
      t: 148.30445861816406
    charspan:
    - 0
    - 225
    page_no: 7
  self_ref: '#/texts/141'
  text: This corresponds to increasing the learning rate linearly for the first warmup
    _ steps training steps, and decreasing it thereafter proportionally to the inverse
    square root of the step number. We used warmup _ steps = 4000 .
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 5.4 Regularization
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 90.86066436767578
      coord_origin: BOTTOMLEFT
      l: 108.21917724609375
      r: 193.33963012695312
      t: 99.78715515136719
    charspan:
    - 0
    - 18
    page_no: 7
  self_ref: '#/texts/142'
  text: 5.4 Regularization
- children: []
  content_layer: body
  label: text
  orig: 'We employ three types of regularization during training:'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 107.5818099975586
      r: 331.132568359375
      t: 78.80445861816406
    charspan:
    - 0
    - 56
    page_no: 7
  self_ref: '#/texts/143'
  text: 'We employ three types of regularization during training:'
- children: []
  content_layer: furniture
  label: page_footer
  orig: '7'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 42.04226303100586
      coord_origin: BOTTOMLEFT
      l: 303.708251953125
      r: 307.9822082519531
      t: 48.70724105834961
    charspan:
    - 0
    - 1
    page_no: 7
  self_ref: '#/texts/144'
  text: '7'
- children: []
  content_layer: body
  label: caption
  orig: 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art
    models on the English-to-German and English-to-French newstest2014 tests at a
    fraction of the training cost.'
  parent:
    $ref: '#/tables/1'
  prov:
  - bbox:
      b: 700.1741333007812
      coord_origin: BOTTOMLEFT
      l: 107.86036682128906
      r: 503.8038635253906
      t: 720.0494384765625
    charspan:
    - 0
    - 192
    page_no: 8
  self_ref: '#/texts/145'
  text: 'Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art
    models on the English-to-German and English-to-French newstest2014 tests at a
    fraction of the training cost.'
- children: []
  content_layer: body
  label: text
  orig: Residual Dropout We apply dropout [33] to the output of each sub-layer, before
    it is added to the sub-layer input and normalized. In addition, we apply dropout
    to the sums of the embeddings and the positional encodings in both the encoder
    and decoder stacks. For the base model, we use a rate of Pd Pdrop = 0 . 1 .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 475.5760498046875
      coord_origin: BOTTOMLEFT
      l: 108.05081176757812
      r: 504.50531005859375
      t: 518.0271606445312
    charspan:
    - 0
    - 314
    page_no: 8
  self_ref: '#/texts/146'
  text: Residual Dropout We apply dropout [33] to the output of each sub-layer, before
    it is added to the sub-layer input and normalized. In addition, we apply dropout
    to the sums of the embeddings and the positional encodings in both the encoder
    and decoder stacks. For the base model, we use a rate of Pd Pdrop = 0 . 1 .
- children: []
  content_layer: body
  label: text
  orig: "Label Smoothing During training, we employed label smoothing of value \u03F5\
    ls = 0 . 1 [36]. This hurts perplexity, as the model learns to be more unsure,\
    \ but improves accuracy and BLEU score."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 440.0010986328125
      coord_origin: BOTTOMLEFT
      l: 108.08966064453125
      r: 503.57916259765625
      t: 459.9660949707031
    charspan:
    - 0
    - 187
    page_no: 8
  self_ref: '#/texts/147'
  text: "Label Smoothing During training, we employed label smoothing of value \u03F5\
    ls = 0 . 1 [36]. This hurts perplexity, as the model learns to be more unsure,\
    \ but improves accuracy and BLEU score."
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 6 Results
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 411.382568359375
      coord_origin: BOTTOMLEFT
      l: 108.33474731445312
      r: 162.79067993164062
      t: 419.76318359375
    charspan:
    - 0
    - 9
    page_no: 8
  self_ref: '#/texts/148'
  text: 6 Results
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 6.1 Machine Translation
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 386.95648193359375
      coord_origin: BOTTOMLEFT
      l: 108.2789535522461
      r: 218.90365600585938
      t: 393.97015380859375
    charspan:
    - 0
    - 23
    page_no: 8
  self_ref: '#/texts/149'
  text: 6.1 Machine Translation
- children: []
  content_layer: body
  label: text
  orig: On the WMT 2014 English-to-German translation task, the big transformer model
    (Transformer (big) in Table 2) outperforms the best previously reported models
    (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art
    BLEU score of 28 . 4. The configuration of this model is listed in the bottom
    line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model
    surpasses all previously published models and ensembles, at a fraction of the
    training cost of any of the competitive models.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 309.5210876464844
      coord_origin: BOTTOMLEFT
      l: 108.1295166015625
      r: 504.5013732910156
      t: 373.0234375
    charspan:
    - 0
    - 521
    page_no: 8
  self_ref: '#/texts/150'
  text: On the WMT 2014 English-to-German translation task, the big transformer model
    (Transformer (big) in Table 2) outperforms the best previously reported models
    (including ensembles) by more than 2 . 0 BLEU, establishing a new state-of-the-art
    BLEU score of 28 . 4. The configuration of this model is listed in the bottom
    line of Table 3. Training took 3 . 5 days on 8 P100 GPUs. Even our base model
    surpasses all previously published models and ensembles, at a fraction of the
    training cost of any of the competitive models.
- children: []
  content_layer: body
  label: text
  orig: On the WMT 2014 English-to-French translation task, our big model achieves
    a BLEU score of 41 . 0 , outperforming all of the previously published single
    models, at less than 1/4 the training cost of the previous state-of-the-art model.
    The Transformer (big) model trained for English-to-French used dropout rate Pdrop
    = 0 . 1, instead of 0 . 3 .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 259.7170715332031
      coord_origin: BOTTOMLEFT
      l: 108.05081176757812
      r: 504.7077941894531
      t: 302.0894470214844
    charspan:
    - 0
    - 345
    page_no: 8
  self_ref: '#/texts/151'
  text: On the WMT 2014 English-to-French translation task, our big model achieves
    a BLEU score of 41 . 0 , outperforming all of the previously published single
    models, at less than 1/4 the training cost of the previous state-of-the-art model.
    The Transformer (big) model trained for English-to-French used dropout rate Pdrop
    = 0 . 1, instead of 0 . 3 .
- children: []
  content_layer: body
  label: text
  orig: "For the base models, we used a single model obtained by averaging the last\
    \ 5 checkpoints, which were written at 10-minute intervals. For the big models,\
    \ we averaged the last 20 checkpoints. We used beam search with a beam size of\
    \ 4 and length penalty \u03B1 = 0 . 6 [38]. These hyperparameters were chosen\
    \ after experimentation on the development set. We set the maximum output length\
    \ during inference to input length + 50, but terminate early when possible [38]."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 200.37112426757812
      coord_origin: BOTTOMLEFT
      l: 107.84603118896484
      r: 503.8649597167969
      t: 253.0232696533203
    charspan:
    - 0
    - 458
    page_no: 8
  self_ref: '#/texts/152'
  text: "For the base models, we used a single model obtained by averaging the last\
    \ 5 checkpoints, which were written at 10-minute intervals. For the big models,\
    \ we averaged the last 20 checkpoints. We used beam search with a beam size of\
    \ 4 and length penalty \u03B1 = 0 . 6 [38]. These hyperparameters were chosen\
    \ after experimentation on the development set. We set the maximum output length\
    \ during inference to input length + 50, but terminate early when possible [38]."
- children: []
  content_layer: body
  label: text
  orig: Table 2 summarizes our results and compares our translation quality and training
    costs to other model architectures from the literature. We estimate the number
    of floating point operations used to train a model by multiplying the training
    time, the number of GPUs used, and an estimate of the sustained single-precision
    floating-point capacity of each GPU 5 .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 151.255126953125
      coord_origin: BOTTOMLEFT
      l: 107.85697937011719
      r: 503.9773254394531
      t: 192.94845581054688
    charspan:
    - 0
    - 359
    page_no: 8
  self_ref: '#/texts/153'
  text: Table 2 summarizes our results and compares our translation quality and training
    costs to other model architectures from the literature. We estimate the number
    of floating point operations used to train a model by multiplying the training
    time, the number of GPUs used, and an estimate of the sustained single-precision
    floating-point capacity of each GPU 5 .
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 6.2 Model Variations
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 126.55864715576172
      coord_origin: BOTTOMLEFT
      l: 108.27896881103516
      r: 203.660888671875
      t: 133.61216735839844
    charspan:
    - 0
    - 20
    page_no: 8
  self_ref: '#/texts/154'
  text: 6.2 Model Variations
- children: []
  content_layer: body
  label: text
  orig: To evaluate the importance of different components of the Transformer, we
    varied our base model in different ways, measuring the change in performance on
    English-to-German translation on the
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 92.78911590576172
      coord_origin: BOTTOMLEFT
      l: 107.86358642578125
      r: 503.79376220703125
      t: 112.66445922851562
    charspan:
    - 0
    - 190
    page_no: 8
  self_ref: '#/texts/155'
  text: To evaluate the importance of different components of the Transformer, we
    varied our base model in different ways, measuring the change in performance on
    English-to-German translation on the
- children: []
  content_layer: body
  label: footnote
  orig: 5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100,
    respectively.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 70.05429077148438
      coord_origin: BOTTOMLEFT
      l: 120.84428405761719
      r: 452.93731689453125
      t: 79.92158508300781
    charspan:
    - 0
    - 90
    page_no: 8
  self_ref: '#/texts/156'
  text: 5 We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100,
    respectively.
- children: []
  content_layer: furniture
  label: page_footer
  orig: '8'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982486724853516
      coord_origin: BOTTOMLEFT
      l: 304.06689453125
      r: 307.9423522949219
      t: 48.846717834472656
    charspan:
    - 0
    - 1
    page_no: 8
  self_ref: '#/texts/157'
  text: '8'
- children: []
  content_layer: body
  label: caption
  orig: 'Table 3: Variations on the Transformer architecture. Unlisted values are
    identical to those of the base model. All metrics are on the English-to-German
    translation development set, newstest2013. Listed perplexities are per-wordpiece,
    according to our byte-pair encoding, and should not be compared to per-word perplexities.'
  parent:
    $ref: '#/tables/2'
  prov:
  - bbox:
      b: 678.3660888671875
      coord_origin: BOTTOMLEFT
      l: 107.85765838623047
      r: 503.9077453613281
      t: 720.0494384765625
    charspan:
    - 0
    - 323
    page_no: 9
  self_ref: '#/texts/158'
  text: 'Table 3: Variations on the Transformer architecture. Unlisted values are
    identical to those of the base model. All metrics are on the English-to-German
    translation development set, newstest2013. Listed perplexities are per-wordpiece,
    according to our byte-pair encoding, and should not be compared to per-word perplexities.'
- children: []
  content_layer: body
  label: text
  orig: development set, newstest2013. We used beam search as described in the previous
    section, but no checkpoint averaging. We present these results in Table 3.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 357.62310791015625
      coord_origin: BOTTOMLEFT
      l: 108.24906158447266
      r: 503.69232177734375
      t: 377.4984436035156
    charspan:
    - 0
    - 154
    page_no: 9
  self_ref: '#/texts/159'
  text: development set, newstest2013. We used beam search as described in the previous
    section, but no checkpoint averaging. We present these results in Table 3.
- children: []
  content_layer: body
  label: text
  orig: In Table 3 rows (A), we vary the number of attention heads and the attention
    key and value dimensions, keeping the amount of computation constant, as described
    in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best
    setting, quality also drops off with too many heads.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 319.4161071777344
      coord_origin: BOTTOMLEFT
      l: 108.07113647460938
      r: 504.7042236328125
      t: 350.200439453125
    charspan:
    - 0
    - 290
    page_no: 9
  self_ref: '#/texts/160'
  text: In Table 3 rows (A), we vary the number of attention heads and the attention
    key and value dimensions, keeping the amount of computation constant, as described
    in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best
    setting, quality also drops off with too many heads.
- children: []
  content_layer: body
  label: text
  orig: In Table 3 rows (B), we observe that reducing the attention key size dk hurts
    model quality. This suggests that determining compatibility is not easy and that
    a more sophisticated compatibility function than dot product may be beneficial.
    We further observe in rows (C) and (D) that, as expected, bigger models are better,
    and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
    sinusoidal positional encoding with learned positional embeddings [9], and observe
    nearly identical results to the base model.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 250.54437255859375
      coord_origin: BOTTOMLEFT
      l: 108.02928924560547
      r: 504.704345703125
      t: 312.10302734375
    charspan:
    - 0
    - 529
    page_no: 9
  self_ref: '#/texts/161'
  text: In Table 3 rows (B), we observe that reducing the attention key size dk hurts
    model quality. This suggests that determining compatibility is not easy and that
    a more sophisticated compatibility function than dot product may be beneficial.
    We further observe in rows (C) and (D) that, as expected, bigger models are better,
    and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
    sinusoidal positional encoding with learned positional embeddings [9], and observe
    nearly identical results to the base model.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 6.3 English Constituency Parsing
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 221.82066345214844
      coord_origin: BOTTOMLEFT
      l: 108.2789535522461
      r: 255.8051300048828
      t: 230.7471466064453
    charspan:
    - 0
    - 32
    page_no: 9
  self_ref: '#/texts/162'
  text: 6.3 English Constituency Parsing
- children: []
  content_layer: body
  label: text
  orig: 'To evaluate if the Transformer can generalize to other tasks we performed
    experiments on English constituency parsing. This task presents specific challenges:
    the output is subject to strong structural constraints and is significantly longer
    than the input. Furthermore, RNN sequence-to-sequence models have not been able
    to attain state-of-the-art results in small-data regimes [37].'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 168.0701141357422
      coord_origin: BOTTOMLEFT
      l: 107.86307525634766
      r: 503.8661193847656
      t: 209.76345825195312
    charspan:
    - 0
    - 384
    page_no: 9
  self_ref: '#/texts/163'
  text: 'To evaluate if the Transformer can generalize to other tasks we performed
    experiments on English constituency parsing. This task presents specific challenges:
    the output is subject to strong structural constraints and is significantly longer
    than the input. Furthermore, RNN sequence-to-sequence models have not been able
    to attain state-of-the-art results in small-data regimes [37].'
- children: []
  content_layer: body
  label: text
  orig: We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal
    (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also
    trained it in a semi-supervised setting, using the larger high-confidence and
    BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary
    of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the
    semi-supervised setting.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 108.04512023925781
      coord_origin: BOTTOMLEFT
      l: 107.58081817626953
      r: 504.68597412109375
      t: 160.75704956054688
    charspan:
    - 0
    - 425
    page_no: 9
  self_ref: '#/texts/164'
  text: We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal
    (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also
    trained it in a semi-supervised setting, using the larger high-confidence and
    BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary
    of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the
    semi-supervised setting.
- children: []
  content_layer: body
  label: text
  orig: We performed only a small number of experiments to select the dropout, both
    attention and residual (section 5.4), learning rates and beam size on the Section
    22 development set, all other parameters remained unchanged from the English-to-German
    base translation model. During inference, we
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 107.5816650390625
      r: 503.7939453125
      t: 100.6224594116211
    charspan:
    - 0
    - 289
    page_no: 9
  self_ref: '#/texts/165'
  text: We performed only a small number of experiments to select the dropout, both
    attention and residual (section 5.4), learning rates and beam size on the Section
    22 development set, all other parameters remained unchanged from the English-to-German
    base translation model. During inference, we
- children: []
  content_layer: furniture
  label: page_footer
  orig: '9'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.90278625488281
      coord_origin: BOTTOMLEFT
      l: 303.8078918457031
      r: 308.08184814453125
      t: 48.846717834472656
    charspan:
    - 0
    - 1
    page_no: 9
  self_ref: '#/texts/166'
  text: '9'
- children: []
  content_layer: body
  label: caption
  orig: 'Table 4: The Transformer generalizes well to English constituency parsing
    (Results are on Section 23 of WSJ)'
  parent:
    $ref: '#/tables/3'
  prov:
  - bbox:
      b: 700.5825805664062
      coord_origin: BOTTOMLEFT
      l: 107.85697937011719
      r: 503.3311462402344
      t: 720.0494384765625
    charspan:
    - 0
    - 108
    page_no: 10
  self_ref: '#/texts/167'
  text: 'Table 4: The Transformer generalizes well to English constituency parsing
    (Results are on Section 23 of WSJ)'
- children: []
  content_layer: body
  label: text
  orig: "increased the maximum output length to input length + 300. We used a beam\
    \ size of 21 and \u03B1 = 0 . 3 for both WSJ only and the semi-supervised setting."
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 509.2911071777344
      coord_origin: BOTTOMLEFT
      l: 108.1573257446289
      r: 503.8209228515625
      t: 529.1664428710938
    charspan:
    - 0
    - 149
    page_no: 10
  self_ref: '#/texts/168'
  text: "increased the maximum output length to input length + 300. We used a beam\
    \ size of 21 and \u03B1 = 0 . 3 for both WSJ only and the semi-supervised setting."
- children: []
  content_layer: body
  label: text
  orig: Our results in Table 4 show that despite the lack of task-specific tuning
    our model performs surprisingly well, yielding better results than all previously
    reported models with the exception of the Recurrent Neural Network Grammar [8].
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 471.7018127441406
      coord_origin: BOTTOMLEFT
      l: 108.05050659179688
      r: 505.16583251953125
      t: 501.8684387207031
    charspan:
    - 0
    - 235
    page_no: 10
  self_ref: '#/texts/169'
  text: Our results in Table 4 show that despite the lack of task-specific tuning
    our model performs surprisingly well, yielding better results than all previously
    reported models with the exception of the Recurrent Neural Network Grammar [8].
- children: []
  content_layer: body
  label: text
  orig: In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms
    the BerkeleyParser [29] even when training only on the WSJ training set of 40K
    sentences.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 443.7861022949219
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 505.17181396484375
      t: 463.66143798828125
    charspan:
    - 0
    - 170
    page_no: 10
  self_ref: '#/texts/170'
  text: In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms
    the BerkeleyParser [29] even when training only on the WSJ training set of 40K
    sentences.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: 7 Conclusion
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 417.7878112792969
      coord_origin: BOTTOMLEFT
      l: 108.20323944091797
      r: 182.86346435546875
      t: 426.2640380859375
    charspan:
    - 0
    - 12
    page_no: 10
  self_ref: '#/texts/171'
  text: 7 Conclusion
- children: []
  content_layer: body
  label: text
  orig: In this work, we presented the Transformer, the first sequence transduction
    model based entirely on attention, replacing the recurrent layers most commonly
    used in encoder-decoder architectures with multi-headed self-attention.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 373.42034912109375
      coord_origin: BOTTOMLEFT
      l: 108.1594009399414
      r: 503.8659973144531
      t: 402.1424560546875
    charspan:
    - 0
    - 227
    page_no: 10
  self_ref: '#/texts/172'
  text: In this work, we presented the Transformer, the first sequence transduction
    model based entirely on attention, replacing the recurrent layers most commonly
    used in encoder-decoder architectures with multi-headed self-attention.
- children: []
  content_layer: body
  label: text
  orig: For translation tasks, the Transformer can be trained significantly faster
    than architectures based on recurrent or convolutional layers. On both WMT 2014
    English-to-German and WMT 2014 English-to-French translation tasks, we achieve
    a new state of the art. In the former task our best model outperforms even all
    previously reported ensembles.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 322.2420959472656
      coord_origin: BOTTOMLEFT
      l: 108.12194061279297
      r: 504.00732421875
      t: 363.9354553222656
    charspan:
    - 0
    - 343
    page_no: 10
  self_ref: '#/texts/173'
  text: For translation tasks, the Transformer can be trained significantly faster
    than architectures based on recurrent or convolutional layers. On both WMT 2014
    English-to-German and WMT 2014 English-to-French translation tasks, we achieve
    a new state of the art. In the former task our best model outperforms even all
    previously reported ensembles.
- children: []
  content_layer: body
  label: text
  orig: We are excited about the future of attention-based models and plan to apply
    them to other tasks. We plan to extend the Transformer to problems involving input
    and output modalities other than text and to investigate local, restricted attention
    mechanisms to efficiently handle large inputs and outputs such as images, audio
    and video. Making generation less sequential is another research goals of ours.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 273.1260986328125
      coord_origin: BOTTOMLEFT
      l: 107.58161163330078
      r: 504.93133544921875
      t: 314.8204345703125
    charspan:
    - 0
    - 403
    page_no: 10
  self_ref: '#/texts/174'
  text: We are excited about the future of attention-based models and plan to apply
    them to other tasks. We plan to extend the Transformer to problems involving input
    and output modalities other than text and to investigate local, restricted attention
    mechanisms to efficiently handle large inputs and outputs such as images, audio
    and video. Making generation less sequential is another research goals of ours.
- children: []
  content_layer: body
  label: text
  orig: The code we used to train and evaluate our models is available at https://github.com/
    tensorflow/tensor2tensor .
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 247.18402099609375
      coord_origin: BOTTOMLEFT
      l: 107.86375427246094
      r: 504.3028259277344
      t: 265.8240051269531
    charspan:
    - 0
    - 112
    page_no: 10
  self_ref: '#/texts/175'
  text: The code we used to train and evaluate our models is available at https://github.com/
    tensorflow/tensor2tensor .
- children: []
  content_layer: body
  label: text
  orig: Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for
    their fruitful comments, corrections and inspiration.
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 212.2650909423828
      coord_origin: BOTTOMLEFT
      l: 108.0914535522461
      r: 503.78900146484375
      t: 232.2001953125
    charspan:
    - 0
    - 128
    page_no: 10
  self_ref: '#/texts/176'
  text: Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for
    their fruitful comments, corrections and inspiration.
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: References
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 186.3155975341797
      coord_origin: BOTTOMLEFT
      l: 108.31083679199219
      r: 163.2091064453125
      t: 194.7320556640625
    charspan:
    - 0
    - 10
    page_no: 10
  self_ref: '#/texts/177'
  text: References
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: '[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.
    arXiv preprint arXiv:1607.06450, 2016.'
  parent:
    $ref: '#/groups/2'
  prov:
  - bbox:
      b: 156.98223876953125
      coord_origin: BOTTOMLEFT
      l: 113.85771179199219
      r: 504.1795959472656
      t: 176.0904541015625
    charspan:
    - 0
    - 118
    page_no: 10
  self_ref: '#/texts/178'
  text: '[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.
    arXiv preprint arXiv:1607.06450, 2016.'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: '[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation
    by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.'
  parent:
    $ref: '#/groups/2'
  prov:
  - bbox:
      b: 127.422119140625
      coord_origin: BOTTOMLEFT
      l: 113.85770416259766
      r: 504.100830078125
      t: 147.29745483398438
    charspan:
    - 0
    - 153
    page_no: 10
  self_ref: '#/texts/179'
  text: '[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation
    by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: '[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration
    of neural machine translation architectures. CoRR, abs/1703.03906, 2017.'
  parent:
    $ref: '#/groups/2'
  prov:
  - bbox:
      b: 99.39723205566406
      coord_origin: BOTTOMLEFT
      l: 113.85771179199219
      r: 503.79888916015625
      t: 118.50545501708984
    charspan:
    - 0
    - 156
    page_no: 10
  self_ref: '#/texts/180'
  text: '[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration
    of neural machine translation architectures. CoRR, abs/1703.03906, 2017.'
- children: []
  content_layer: body
  enumerated: false
  label: list_item
  marker: '-'
  orig: '[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks
    for machine reading. arXiv preprint arXiv:1601.06733, 2016.'
  parent:
    $ref: '#/groups/2'
  prov:
  - bbox:
      b: 69.83811950683594
      coord_origin: BOTTOMLEFT
      l: 113.85769653320312
      r: 503.80621337890625
      t: 89.71345520019531
    charspan:
    - 0
    - 140
    page_no: 10
  self_ref: '#/texts/181'
  text: '[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks
    for machine reading. arXiv preprint arXiv:1601.06733, 2016.'
- children: []
  content_layer: furniture
  label: page_footer
  orig: '10'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982486724853516
      coord_origin: BOTTOMLEFT
      l: 302.12481689453125
      r: 310.74249267578125
      t: 48.846717834472656
    charspan:
    - 0
    - 2
    page_no: 10
  self_ref: '#/texts/182'
  text: '10'
- children: []
  content_layer: furniture
  label: page_footer
  orig: '11'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 42.11199951171875
      coord_origin: BOTTOMLEFT
      l: 302.12481689453125
      r: 309.925537109375
      t: 48.846717834472656
    charspan:
    - 0
    - 2
    page_no: 11
  self_ref: '#/texts/183'
  text: '11'
- children: []
  content_layer: furniture
  label: page_footer
  orig: '12'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 42.11199951171875
      coord_origin: BOTTOMLEFT
      l: 302.12481689453125
      r: 310.7325134277344
      t: 48.846717834472656
    charspan:
    - 0
    - 2
    page_no: 12
  self_ref: '#/texts/184'
  text: '12'
- children: []
  content_layer: body
  label: section_header
  level: 1
  orig: "Attention Visualizations\r\nInput-Input Laye Attention Visualizations\r\n\
    Input-Input Layer5"
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 709.833740234375
      coord_origin: BOTTOMLEFT
      l: 108.10759735107422
      r: 250.00534057617188
      t: 726.4456176757812
    charspan:
    - 0
    - 87
    page_no: 13
  self_ref: '#/texts/185'
  text: "Attention Visualizations\r\nInput-Input Laye Attention Visualizations\r\n\
    Input-Input Layer5"
- children: []
  content_layer: body
  label: caption
  orig: 'Figure 3: An example of the attention mechanism following long-distance dependencies
    in the encoder self-attention in layer 5 of 6. Many of the attention heads attend
    to a distant dependency of the verb ''making'', completing the phrase ''making...more
    difficult''. Attentions here shown only for the word ''making''. Different colors
    represent different heads. Best viewed in color.'
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 437.1680908203125
      coord_origin: BOTTOMLEFT
      l: 108.12194061279297
      r: 504.4957580566406
      t: 478.8624572753906
    charspan:
    - 0
    - 377
    page_no: 13
  self_ref: '#/texts/186'
  text: 'Figure 3: An example of the attention mechanism following long-distance dependencies
    in the encoder self-attention in layer 5 of 6. Many of the attention heads attend
    to a distant dependency of the verb ''making'', completing the phrase ''making...more
    difficult''. Attentions here shown only for the word ''making''. Different colors
    represent different heads. Best viewed in color.'
- children: []
  content_layer: body
  label: text
  orig: It
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 632.2380981445312
      coord_origin: BOTTOMLEFT
      l: 121.87755584716797
      r: 127.49385833740234
      t: 635.7852172851562
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/187'
  text: It
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 632.028564453125
      coord_origin: BOTTOMLEFT
      l: 133.5457305908203
      r: 139.200927734375
      t: 636.8280639648438
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/188'
  text: is
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 632.0291137695312
      coord_origin: BOTTOMLEFT
      l: 145.2139892578125
      r: 150.7836151123047
      t: 637.0308837890625
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/189'
  text: in
- children: []
  content_layer: body
  label: text
  orig: this spirit that
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.6557006835938
      coord_origin: BOTTOMLEFT
      l: 156.8821563720703
      r: 185.87429809570312
      t: 647.8817749023438
    charspan:
    - 0
    - 16
    page_no: 13
  self_ref: '#/texts/190'
  text: this spirit that
- children: []
  content_layer: body
  label: text
  orig: a
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.795166015625
      coord_origin: BOTTOMLEFT
      l: 193.33412170410156
      r: 197.54246520996094
      t: 635.513427734375
    charspan:
    - 0
    - 1
    page_no: 13
  self_ref: '#/texts/191'
  text: a
- children: []
  content_layer: body
  label: text
  orig: majority
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 632.0291137695312
      coord_origin: BOTTOMLEFT
      l: 203.55543518066406
      r: 210.75083923339844
      t: 658.6715087890625
    charspan:
    - 0
    - 8
    page_no: 13
  self_ref: '#/texts/192'
  text: majority
- children: []
  content_layer: body
  label: text
  orig: of
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.7718505859375
      coord_origin: BOTTOMLEFT
      l: 215.13035583496094
      r: 220.87890625
      t: 638.27490234375
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/193'
  text: of
- children: []
  content_layer: body
  label: text
  orig: "American\r\ngovernments"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.51513671875
      coord_origin: BOTTOMLEFT
      l: 226.8923797607422
      r: 245.75596618652344
      t: 676.173828125
    charspan:
    - 0
    - 21
    page_no: 13
  self_ref: '#/texts/194'
  text: "American\r\ngovernments"
- children: []
  content_layer: body
  label: text
  orig: "have\r\npassed\r\nnewlaws\r\nsince\r\n2009\r\nmaking"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.74072265625
      coord_origin: BOTTOMLEFT
      l: 250.22882080078125
      r: 327.4339294433594
      t: 656.0656127929688
    charspan:
    - 0
    - 42
    page_no: 13
  self_ref: '#/texts/195'
  text: "have\r\npassed\r\nnewlaws\r\nsince\r\n2009\r\nmaking"
- children: []
  content_layer: body
  label: text
  orig: the
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.6551513671875
      coord_origin: BOTTOMLEFT
      l: 331.90667724609375
      r: 337.5618896484375
      t: 642.0087280273438
    charspan:
    - 0
    - 3
    page_no: 13
  self_ref: '#/texts/196'
  text: the
- children: []
  content_layer: body
  label: text
  orig: registration
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 632.020263671875
      coord_origin: BOTTOMLEFT
      l: 343.574951171875
      r: 350.7703552246094
      t: 669.451904296875
    charspan:
    - 0
    - 12
    page_no: 13
  self_ref: '#/texts/197'
  text: registration
- children: []
  content_layer: body
  label: text
  orig: or
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.7723999023438
      coord_origin: BOTTOMLEFT
      l: 356.6899719238281
      r: 360.8983154296875
      t: 638.5399780273438
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/198'
  text: or
- children: []
  content_layer: body
  label: text
  orig: "voting\r\nprocess\r\nmore"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.6172485351562
      coord_origin: BOTTOMLEFT
      l: 366.91180419921875
      r: 395.9034423828125
      t: 658.4459228515625
    charspan:
    - 0
    - 21
    page_no: 13
  self_ref: '#/texts/199'
  text: "voting\r\nprocess\r\nmore"
- children: []
  content_layer: body
  label: text
  orig: diff
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.7791137695312
      coord_origin: BOTTOMLEFT
      l: 401.82257080078125
      r: 407.57110595703125
      t: 640.0013427734375
    charspan:
    - 0
    - 4
    page_no: 13
  self_ref: '#/texts/200'
  text: diff
- children: []
  content_layer: body
  label: text
  orig: fficult
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 639.6590576171875
      coord_origin: BOTTOMLEFT
      l: 401.82257080078125
      r: 407.57110595703125
      t: 655.52783203125
    charspan:
    - 0
    - 7
    page_no: 13
  self_ref: '#/texts/201'
  text: fficult
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 632.2230224609375
      coord_origin: BOTTOMLEFT
      l: 418.3768005371094
      r: 419.1546936035156
      t: 633.0009155273438
    charspan:
    - 0
    - 1
    page_no: 13
  self_ref: '#/texts/202'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 631.9425048828125
      coord_origin: BOTTOMLEFT
      l: 425.1522216796875
      r: 502.3734130859375
      t: 656.602783203125
    charspan:
    - 0
    - 47
    page_no: 13
  self_ref: '#/texts/203'
  text: "<EOS>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>"
- children: []
  content_layer: body
  label: text
  orig: It
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 547.0164184570312
      coord_origin: BOTTOMLEFT
      l: 121.87755584716797
      r: 127.49385833740234
      t: 550.5635375976562
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/204'
  text: It
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 545.5111694335938
      coord_origin: BOTTOMLEFT
      l: 133.5457305908203
      r: 139.200927734375
      t: 550.3106689453125
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/205'
  text: is
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 545.0736083984375
      coord_origin: BOTTOMLEFT
      l: 145.2139892578125
      r: 150.7836151123047
      t: 550.0753784179688
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/206'
  text: in
- children: []
  content_layer: body
  label: text
  orig: "this\r\nspirit\r\nthat"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 534.4324951171875
      coord_origin: BOTTOMLEFT
      l: 156.88267517089844
      r: 185.87429809570312
      t: 550.56103515625
    charspan:
    - 0
    - 18
    page_no: 13
  self_ref: '#/texts/207'
  text: "this\r\nspirit\r\nthat"
- children: []
  content_layer: body
  label: text
  orig: a
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 546.56884765625
      coord_origin: BOTTOMLEFT
      l: 193.33412170410156
      r: 197.54246520996094
      t: 550.287109375
    charspan:
    - 0
    - 1
    page_no: 13
  self_ref: '#/texts/208'
  text: a
- children: []
  content_layer: body
  label: text
  orig: majority
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 523.8983764648438
      coord_origin: BOTTOMLEFT
      l: 203.55543518066406
      r: 210.75083923339844
      t: 550.540771484375
    charspan:
    - 0
    - 8
    page_no: 13
  self_ref: '#/texts/209'
  text: majority
- children: []
  content_layer: body
  label: text
  orig: of
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 544.3844604492188
      coord_origin: BOTTOMLEFT
      l: 215.13035583496094
      r: 220.87890625
      t: 550.8875122070312
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/210'
  text: of
- children: []
  content_layer: body
  label: text
  orig: "American\r\ngovernments"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 505.89654541015625
      coord_origin: BOTTOMLEFT
      l: 226.8923797607422
      r: 245.75596618652344
      t: 550.3057861328125
    charspan:
    - 0
    - 21
    page_no: 13
  self_ref: '#/texts/211'
  text: "American\r\ngovernments"
- children: []
  content_layer: body
  label: text
  orig: have
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 534.2601928710938
      coord_origin: BOTTOMLEFT
      l: 250.22882080078125
      r: 255.88401794433594
      t: 550.2922973632812
    charspan:
    - 0
    - 4
    page_no: 13
  self_ref: '#/texts/212'
  text: have
- children: []
  content_layer: body
  label: text
  orig: "passed\r\nnew\r\nlaws\r\nsince\r\n2009\r\nmaking"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 526.044189453125
      coord_origin: BOTTOMLEFT
      l: 261.89697265625
      r: 327.4339294433594
      t: 550.54931640625
    charspan:
    - 0
    - 38
    page_no: 13
  self_ref: '#/texts/213'
  text: "passed\r\nnew\r\nlaws\r\nsince\r\n2009\r\nmaking"
- children: []
  content_layer: body
  label: text
  orig: the
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 539.941650390625
      coord_origin: BOTTOMLEFT
      l: 331.90667724609375
      r: 337.5618896484375
      t: 550.2952270507812
    charspan:
    - 0
    - 3
    page_no: 13
  self_ref: '#/texts/214'
  text: the
- children: []
  content_layer: body
  label: text
  orig: registration
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 512.6410522460938
      coord_origin: BOTTOMLEFT
      l: 343.574951171875
      r: 350.7703552246094
      t: 550.0726928710938
    charspan:
    - 0
    - 12
    page_no: 13
  self_ref: '#/texts/215'
  text: registration
- children: []
  content_layer: body
  label: text
  orig: or
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 543.9556884765625
      coord_origin: BOTTOMLEFT
      l: 356.69049072265625
      r: 360.8988342285156
      t: 550.7232666015625
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/216'
  text: or
- children: []
  content_layer: body
  label: text
  orig: "voting\r\nprocess\r\nmore"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 523.8917236328125
      coord_origin: BOTTOMLEFT
      l: 366.91180419921875
      r: 395.9034423828125
      t: 550.3085327148438
    charspan:
    - 0
    - 21
    page_no: 13
  self_ref: '#/texts/217'
  text: "voting\r\nprocess\r\nmore"
- children: []
  content_layer: body
  label: text
  orig: diff
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 526.810302734375
      coord_origin: BOTTOMLEFT
      l: 401.82257080078125
      r: 407.57110595703125
      t: 535.0325317382812
    charspan:
    - 0
    - 4
    page_no: 13
  self_ref: '#/texts/218'
  text: diff
- children: []
  content_layer: body
  label: text
  orig: fficult
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 534.6902465820312
      coord_origin: BOTTOMLEFT
      l: 401.82257080078125
      r: 407.57110595703125
      t: 550.5590209960938
    charspan:
    - 0
    - 7
    page_no: 13
  self_ref: '#/texts/219'
  text: fficult
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 549.161865234375
      coord_origin: BOTTOMLEFT
      l: 418.37640380859375
      r: 419.154296875
      t: 549.9397583007812
    charspan:
    - 0
    - 1
    page_no: 13
  self_ref: '#/texts/220'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad><pad>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>"
  parent:
    $ref: '#/pictures/3'
  prov:
  - bbox:
      b: 525.52978515625
      coord_origin: BOTTOMLEFT
      l: 425.1522216796875
      r: 502.3734130859375
      t: 550.1885986328125
    charspan:
    - 0
    - 45
    page_no: 13
  self_ref: '#/texts/221'
  text: "<EOS>\r\n<pad><pad>\r\n<pad>\r\n<pad>\r\n<pad>\r\n<pad>"
- children: []
  content_layer: furniture
  label: page_footer
  orig: '13'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982486724853516
      coord_origin: BOTTOMLEFT
      l: 302.12481689453125
      r: 310.30413818359375
      t: 48.846717834472656
    charspan:
    - 0
    - 2
    page_no: 13
  self_ref: '#/texts/222'
  text: '13'
- children: []
  content_layer: body
  label: text
  orig: Input-Input Layer5
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 672.3953247070312
      coord_origin: BOTTOMLEFT
      l: 109.33056640625
      r: 278.09710693359375
      t: 690.4949340820312
    charspan:
    - 0
    - 18
    page_no: 14
  self_ref: '#/texts/223'
  text: Input-Input Layer5
- children: []
  content_layer: body
  label: caption
  orig: 'Figure 4: Two attention heads, also in layer 5 of 6, apparently involved
    in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions
    from just the word ''its'' for attention heads 5 and 6. Note that the attentions
    are very sharp for this word.'
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 146.18310546875
      coord_origin: BOTTOMLEFT
      l: 108.11955261230469
      r: 504.53125
      t: 177.01724243164062
    charspan:
    - 0
    - 266
    page_no: 14
  self_ref: '#/texts/224'
  text: 'Figure 4: Two attention heads, also in layer 5 of 6, apparently involved
    in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions
    from just the word ''its'' for attention heads 5 and 6. Note that the attentions
    are very sharp for this word.'
- children: []
  content_layer: body
  label: text
  orig: The Law will never
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 576.8137817382812
      coord_origin: BOTTOMLEFT
      l: 124.62284088134766
      r: 173.32603454589844
      t: 600.2196655273438
    charspan:
    - 0
    - 18
    page_no: 14
  self_ref: '#/texts/225'
  text: The Law will never
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.390869140625
      coord_origin: BOTTOMLEFT
      l: 180.52911376953125
      r: 187.3030242919922
      t: 586.764404296875
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/226'
  text: be
- children: []
  content_layer: body
  label: text
  orig: perfect
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.4008178710938
      coord_origin: BOTTOMLEFT
      l: 194.39382934570312
      r: 203.02195739746094
      t: 605.20458984375
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/227'
  text: perfect
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.55859375
      coord_origin: BOTTOMLEFT
      l: 214.2216796875
      r: 216.46722412109375
      t: 578.5462646484375
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/228'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.3914794921875
      coord_origin: BOTTOMLEFT
      l: 222.45840454101562
      r: 243.20883178710938
      t: 589.672119140625
    charspan:
    - 0
    - 8
    page_no: 14
  self_ref: '#/texts/229'
  text: "but\r\nits"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.0746459960938
      coord_origin: BOTTOMLEFT
      l: 250.41189575195312
      r: 271.1622314453125
      t: 620.6806030273438
    charspan:
    - 0
    - 19
    page_no: 14
  self_ref: '#/texts/230'
  text: "application\r\nshould"
- children: []
  content_layer: body
  label: text
  orig: "be\r\njust"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 576.3665161132812
      coord_origin: BOTTOMLEFT
      l: 278.36480712890625
      r: 300.9599914550781
      t: 591.2188110351562
    charspan:
    - 0
    - 8
    page_no: 14
  self_ref: '#/texts/231'
  text: "be\r\njust"
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.0833740234375
      coord_origin: BOTTOMLEFT
      l: 310.1657409667969
      r: 310.9856872558594
      t: 579.59912109375
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/232'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 576.9529418945312
      coord_origin: BOTTOMLEFT
      l: 320.2940979003906
      r: 327.0680236816406
      t: 590.9200439453125
    charspan:
    - 0
    - 4
    page_no: 14
  self_ref: '#/texts/233'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.4008178710938
      coord_origin: BOTTOMLEFT
      l: 334.2710876464844
      r: 341.0450134277344
      t: 583.1497802734375
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/234'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 576.8131713867188
      coord_origin: BOTTOMLEFT
      l: 348.2474670410156
      r: 382.97430419921875
      t: 596.3988037109375
    charspan:
    - 0
    - 13
    page_no: 14
  self_ref: '#/texts/235'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.3995971679688
      coord_origin: BOTTOMLEFT
      l: 390.1768798828125
      r: 398.7956848144531
      t: 607.73779296875
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/236'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.5592041015625
      coord_origin: BOTTOMLEFT
      l: 409.8929138183594
      r: 412.1384582519531
      t: 578.546875
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/237'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.4014282226562
      coord_origin: BOTTOMLEFT
      l: 418.1302490234375
      r: 424.8016662597656
      t: 583.3926391601562
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/238'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.4008178710938
      coord_origin: BOTTOMLEFT
      l: 433.83984375
      r: 440.7255859375
      t: 589.1223754882812
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/239'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.0927124023438
      coord_origin: BOTTOMLEFT
      l: 446.0831298828125
      r: 454.59942626953125
      t: 606.182373046875
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/240'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.6325073242188
      coord_origin: BOTTOMLEFT
      l: 465.798583984375
      r: 466.7303466796875
      t: 578.5642700195312
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/241'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 577.2976684570312
      coord_origin: BOTTOMLEFT
      l: 473.9154052734375
      r: 496.5293273925781
      t: 606.8345947265625
    charspan:
    - 0
    - 12
    page_no: 14
  self_ref: '#/texts/242'
  text: "<EOS>\r\n<pad>"
- children: []
  content_layer: body
  label: text
  orig: "The\r\nLaw\r\nwill\r\nnever\r\nput-In"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 457.188720703125
      coord_origin: BOTTOMLEFT
      l: 124.62284088134766
      r: 173.32664489746094
      t: 480.0075988769531
    charspan:
    - 0
    - 29
    page_no: 14
  self_ref: '#/texts/243'
  text: "The\r\nLaw\r\nwill\r\nnever\r\nput-In"
- children: []
  content_layer: body
  label: text
  orig: "be\r\np"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 470.1229248046875
      coord_origin: BOTTOMLEFT
      l: 180.52911376953125
      r: 187.3030242919922
      t: 479.4964599609375
    charspan:
    - 0
    - 5
    page_no: 14
  self_ref: '#/texts/244'
  text: "be\r\np"
- children: []
  content_layer: body
  label: text
  orig: "perfect\r\nu"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 452.011474609375
      coord_origin: BOTTOMLEFT
      l: 194.39382934570312
      r: 203.02195739746094
      t: 479.8152770996094
    charspan:
    - 0
    - 10
    page_no: 14
  self_ref: '#/texts/245'
  text: "perfect\r\nu"
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 478.06610107421875
      coord_origin: BOTTOMLEFT
      l: 214.2216796875
      r: 216.46722412109375
      t: 479.05377197265625
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/246'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits\r\nLay"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 467.53375244140625
      coord_origin: BOTTOMLEFT
      l: 222.45840454101562
      r: 243.20883178710938
      t: 479.81439208984375
    charspan:
    - 0
    - 13
    page_no: 14
  self_ref: '#/texts/247'
  text: "but\r\nits\r\nLay"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould\r\nyer5"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 435.6673889160156
      coord_origin: BOTTOMLEFT
      l: 250.41189575195312
      r: 271.1622314453125
      t: 479.227294921875
    charspan:
    - 0
    - 25
    page_no: 14
  self_ref: '#/texts/248'
  text: "application\r\nshould\r\nyer5"
- children: []
  content_layer: body
  label: text
  orig: "be\r\n5"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 470.1229248046875
      coord_origin: BOTTOMLEFT
      l: 278.36480712890625
      r: 285.13873291015625
      t: 479.4964599609375
    charspan:
    - 0
    - 5
    page_no: 14
  self_ref: '#/texts/249'
  text: "be\r\n5"
- children: []
  content_layer: body
  label: text
  orig: just
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 464.9615478515625
      coord_origin: BOTTOMLEFT
      l: 292.3411865234375
      r: 300.9599914550781
      t: 479.8138427734375
    charspan:
    - 0
    - 4
    page_no: 14
  self_ref: '#/texts/250'
  text: just
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 477.0766296386719
      coord_origin: BOTTOMLEFT
      l: 310.1663818359375
      r: 310.986328125
      t: 479.5924072265625
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/251'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 465.5504150390625
      coord_origin: BOTTOMLEFT
      l: 320.2947082519531
      r: 327.0686340332031
      t: 479.5175476074219
    charspan:
    - 0
    - 4
    page_no: 14
  self_ref: '#/texts/252'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 473.7676696777344
      coord_origin: BOTTOMLEFT
      l: 334.2710876464844
      r: 341.0450134277344
      t: 479.51666259765625
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/253'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 460.22747802734375
      coord_origin: BOTTOMLEFT
      l: 348.2474670410156
      r: 382.97430419921875
      t: 479.8131408691406
    charspan:
    - 0
    - 13
    page_no: 14
  self_ref: '#/texts/254'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 448.9134826660156
      coord_origin: BOTTOMLEFT
      l: 390.1768798828125
      r: 398.7956848144531
      t: 479.2516784667969
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/255'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 478.06610107421875
      coord_origin: BOTTOMLEFT
      l: 409.8935241699219
      r: 412.1390686035156
      t: 479.05377197265625
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/256'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 473.2442321777344
      coord_origin: BOTTOMLEFT
      l: 418.1302490234375
      r: 424.8016662597656
      t: 479.2354736328125
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/257'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 468.0749206542969
      coord_origin: BOTTOMLEFT
      l: 433.83984375
      r: 440.7255859375
      t: 479.7965087890625
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/258'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 450.1386413574219
      coord_origin: BOTTOMLEFT
      l: 446.0831298828125
      r: 454.59942626953125
      t: 479.228271484375
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/259'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 478.1412353515625
      coord_origin: BOTTOMLEFT
      l: 465.798583984375
      r: 466.7303466796875
      t: 479.072998046875
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/260'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "The\r\nLaw\r\nwill\r\nnever\r\nbe\r\nperfect\r\n,\r\nbut\r\nits\r\nplication\r\
    \nshould\r\nbe\r\njust\r\n-\r\nthis\r\nis\r\nwhat\r\nwe\r\nare\r\nmissing\r\n\
    ,\r\nin\r\nmy\r\nopinion\r\n.\r\n<EOS>\r\n<pad>\r\nInput-Input Layer5"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 449.8341979980469
      coord_origin: BOTTOMLEFT
      l: 109.34551239013672
      r: 496.5293273925781
      t: 479.3710632324219
    charspan:
    - 0
    - 170
    page_no: 14
  self_ref: '#/texts/261'
  text: "The\r\nLaw\r\nwill\r\nnever\r\nbe\r\nperfect\r\n,\r\nbut\r\nits\r\nplication\r\
    \nshould\r\nbe\r\njust\r\n-\r\nthis\r\nis\r\nwhat\r\nwe\r\nare\r\nmissing\r\n\
    ,\r\nin\r\nmy\r\nopinion\r\n.\r\n<EOS>\r\n<pad>\r\nInput-Input Layer5"
- children: []
  content_layer: body
  label: text
  orig: The Law will never
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.5621032714844
      coord_origin: BOTTOMLEFT
      l: 124.80961608886719
      r: 174.06002807617188
      t: 383.2309875488281
    charspan:
    - 0
    - 18
    page_no: 14
  self_ref: '#/texts/262'
  text: The Law will never
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.1456604003906
      coord_origin: BOTTOMLEFT
      l: 181.3440399169922
      r: 188.19407653808594
      t: 369.62451171875
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/263'
  text: be
- children: []
  content_layer: body
  label: text
  orig: perfect
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.1557312011719
      coord_origin: BOTTOMLEFT
      l: 195.36451721191406
      r: 204.08958435058594
      t: 388.27191162109375
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/264'
  text: perfect
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.31524658203125
      coord_origin: BOTTOMLEFT
      l: 215.4151611328125
      r: 217.68594360351562
      t: 361.31402587890625
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/265'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.14630126953125
      coord_origin: BOTTOMLEFT
      l: 223.74441528320312
      r: 244.72801208496094
      t: 372.5649108886719
    charspan:
    - 0
    - 8
    page_no: 14
  self_ref: '#/texts/266'
  text: "but\r\nits"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.825927734375
      coord_origin: BOTTOMLEFT
      l: 252.0120086669922
      r: 272.9954528808594
      t: 403.9217529296875
    charspan:
    - 0
    - 19
    page_no: 14
  self_ref: '#/texts/267'
  text: "application\r\nshould"
- children: []
  content_layer: body
  label: text
  orig: "be\r\njust"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.1098327636719
      coord_origin: BOTTOMLEFT
      l: 280.27899169921875
      r: 303.1280212402344
      t: 374.1290283203125
    charspan:
    - 0
    - 8
    page_no: 14
  self_ref: '#/texts/268'
  text: "be\r\njust"
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.834716796875
      coord_origin: BOTTOMLEFT
      l: 312.4372253417969
      r: 313.2663879394531
      t: 362.3787536621094
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/269'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.70281982421875
      coord_origin: BOTTOMLEFT
      l: 322.6793518066406
      r: 329.52935791015625
      t: 373.8268737792969
    charspan:
    - 0
    - 4
    page_no: 14
  self_ref: '#/texts/270'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.1557312011719
      coord_origin: BOTTOMLEFT
      l: 336.8134460449219
      r: 343.6634521484375
      t: 365.96929931640625
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/271'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.56146240234375
      coord_origin: BOTTOMLEFT
      l: 350.94683837890625
      r: 386.0638122558594
      t: 379.3671875
    charspan:
    - 0
    - 13
    page_no: 14
  self_ref: '#/texts/272'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.15447998046875
      coord_origin: BOTTOMLEFT
      l: 393.34735107421875
      r: 402.06298828125
      t: 390.83355712890625
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/273'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.3158874511719
      coord_origin: BOTTOMLEFT
      l: 413.284912109375
      r: 415.5556945800781
      t: 361.3146667480469
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/274'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.15625
      coord_origin: BOTTOMLEFT
      l: 421.61480712890625
      r: 428.3611755371094
      t: 366.21478271484375
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/275'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.1557312011719
      coord_origin: BOTTOMLEFT
      l: 437.50091552734375
      r: 444.4640197753906
      t: 372.0090026855469
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/276'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 359.8441467285156
      coord_origin: BOTTOMLEFT
      l: 449.88177490234375
      r: 458.4937744140625
      t: 389.2606201171875
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/277'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.3900146484375
      coord_origin: BOTTOMLEFT
      l: 469.8187255859375
      r: 470.7609558105469
      t: 361.3322448730469
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/278'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 360.0514221191406
      coord_origin: BOTTOMLEFT
      l: 478.0267333984375
      r: 500.894775390625
      t: 389.9201965332031
    charspan:
    - 0
    - 12
    page_no: 14
  self_ref: '#/texts/279'
  text: "<EOS>\r\n<pad>"
- children: []
  content_layer: body
  label: text
  orig: "The\r\nLaw\r\nwill\r\nnever"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 238.59295654296875
      coord_origin: BOTTOMLEFT
      l: 124.80961608886719
      r: 174.06065368652344
      t: 261.668212890625
    charspan:
    - 0
    - 21
    page_no: 14
  self_ref: '#/texts/280'
  text: "The\r\nLaw\r\nwill\r\nnever"
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 251.6724853515625
      coord_origin: BOTTOMLEFT
      l: 181.3440399169922
      r: 188.19407653808594
      t: 261.1513366699219
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/281'
  text: be
- children: []
  content_layer: body
  label: text
  orig: perfect
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 233.3575439453125
      coord_origin: BOTTOMLEFT
      l: 195.36451721191406
      r: 204.08958435058594
      t: 261.4737548828125
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/282'
  text: perfect
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 259.70489501953125
      coord_origin: BOTTOMLEFT
      l: 215.4151611328125
      r: 217.68594360351562
      t: 260.70367431640625
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/283'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 249.05421447753906
      coord_origin: BOTTOMLEFT
      l: 223.74441528320312
      r: 244.72801208496094
      t: 261.47283935546875
    charspan:
    - 0
    - 8
    page_no: 14
  self_ref: '#/texts/284'
  text: "but\r\nits"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 216.82980346679688
      coord_origin: BOTTOMLEFT
      l: 252.0120086669922
      r: 272.9954528808594
      t: 260.879150390625
    charspan:
    - 0
    - 19
    page_no: 14
  self_ref: '#/texts/285'
  text: "application\r\nshould"
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 251.6724853515625
      coord_origin: BOTTOMLEFT
      l: 280.27899169921875
      r: 287.1289978027344
      t: 261.1513366699219
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/286'
  text: be
- children: []
  content_layer: body
  label: text
  orig: just
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 246.453125
      coord_origin: BOTTOMLEFT
      l: 294.4123840332031
      r: 303.1280212402344
      t: 261.4723205566406
    charspan:
    - 0
    - 4
    page_no: 14
  self_ref: '#/texts/287'
  text: just
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 258.7042236328125
      coord_origin: BOTTOMLEFT
      l: 312.4378967285156
      r: 313.2670593261719
      t: 261.2482604980469
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/288'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 247.04861450195312
      coord_origin: BOTTOMLEFT
      l: 322.6800231933594
      r: 329.530029296875
      t: 261.17266845703125
    charspan:
    - 0
    - 4
    page_no: 14
  self_ref: '#/texts/289'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 255.3582000732422
      coord_origin: BOTTOMLEFT
      l: 336.8134460449219
      r: 343.6634521484375
      t: 261.1717834472656
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/290'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 241.6657257080078
      coord_origin: BOTTOMLEFT
      l: 350.94683837890625
      r: 386.0638122558594
      t: 261.471435546875
    charspan:
    - 0
    - 13
    page_no: 14
  self_ref: '#/texts/291'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 230.22463989257812
      coord_origin: BOTTOMLEFT
      l: 393.34735107421875
      r: 402.06298828125
      t: 260.9037170410156
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/292'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 259.70489501953125
      coord_origin: BOTTOMLEFT
      l: 413.2855224609375
      r: 415.5563049316406
      t: 260.70367431640625
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/293'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 254.82887268066406
      coord_origin: BOTTOMLEFT
      l: 421.61480712890625
      r: 428.3611755371094
      t: 260.8874206542969
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/294'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 249.6014862060547
      coord_origin: BOTTOMLEFT
      l: 437.50091552734375
      r: 444.4640197753906
      t: 261.45477294921875
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/295'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 231.4636688232422
      coord_origin: BOTTOMLEFT
      l: 449.88177490234375
      r: 458.4937744140625
      t: 260.8801574707031
    charspan:
    - 0
    - 7
    page_no: 14
  self_ref: '#/texts/296'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 259.7809143066406
      coord_origin: BOTTOMLEFT
      l: 469.8187255859375
      r: 470.7609558105469
      t: 260.72314453125
    charspan:
    - 0
    - 1
    page_no: 14
  self_ref: '#/texts/297'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>"
  parent:
    $ref: '#/pictures/4'
  prov:
  - bbox:
      b: 231.1558074951172
      coord_origin: BOTTOMLEFT
      l: 478.0262451171875
      r: 500.894775390625
      t: 261.0245666503906
    charspan:
    - 0
    - 12
    page_no: 14
  self_ref: '#/texts/298'
  text: "<EOS>\r\n<pad>"
- children: []
  content_layer: furniture
  label: page_footer
  orig: '14'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 42.11198425292969
      coord_origin: BOTTOMLEFT
      l: 302.12481689453125
      r: 310.70263671875
      t: 48.846702575683594
    charspan:
    - 0
    - 2
    page_no: 14
  self_ref: '#/texts/299'
  text: '14'
- children: []
  content_layer: body
  label: text
  orig: Input-Input Layer5
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 658.0967407226562
      coord_origin: BOTTOMLEFT
      l: 108.30757141113281
      r: 277.5202941894531
      t: 676.2442016601562
    charspan:
    - 0
    - 18
    page_no: 15
  self_ref: '#/texts/300'
  text: Input-Input Layer5
- children: []
  content_layer: body
  label: caption
  orig: 'Figure 5: Many of the attention heads exhibit behaviour that seems related
    to the structure of the sentence. We give two such examples above, from two different
    heads from the encoder self-attention at layer 5 of 6. The heads clearly learned
    to perform different tasks.'
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 158.0651092529297
      coord_origin: BOTTOMLEFT
      l: 108.12194061279297
      r: 503.8548889160156
      t: 188.90025329589844
    charspan:
    - 0
    - 269
    page_no: 15
  self_ref: '#/texts/301'
  text: 'Figure 5: Many of the attention heads exhibit behaviour that seems related
    to the structure of the sentence. We give two such examples above, from two different
    heads from the encoder self-attention at layer 5 of 6. The heads clearly learned
    to perform different tasks.'
- children: []
  content_layer: body
  label: text
  orig: The Law will never
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.262451171875
      coord_origin: BOTTOMLEFT
      l: 123.64026641845703
      r: 172.47222900390625
      t: 585.730224609375
    charspan:
    - 0
    - 18
    page_no: 15
  self_ref: '#/texts/302'
  text: The Law will never
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.841064453125
      coord_origin: BOTTOMLEFT
      l: 179.69436645507812
      r: 186.48619079589844
      t: 572.2393798828125
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/303'
  text: be
- children: []
  content_layer: body
  label: text
  orig: perfect
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.8510131835938
      coord_origin: BOTTOMLEFT
      l: 193.595703125
      r: 202.2466278076172
      t: 590.7283325195312
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/304'
  text: perfect
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 563.0092163085938
      coord_origin: BOTTOMLEFT
      l: 213.47598266601562
      r: 215.7274627685547
      t: 563.99951171875
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/305'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.8416748046875
      coord_origin: BOTTOMLEFT
      l: 221.7344970703125
      r: 242.53976440429688
      t: 575.15478515625
    charspan:
    - 0
    - 8
    page_no: 15
  self_ref: '#/texts/306'
  text: "but\r\nits"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.5240478515625
      coord_origin: BOTTOMLEFT
      l: 249.76190185546875
      r: 270.5670471191406
      t: 606.2452392578125
    charspan:
    - 0
    - 19
    page_no: 15
  self_ref: '#/texts/307'
  text: "application\r\nshould"
- children: []
  content_layer: body
  label: text
  orig: "be\r\njust"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 561.8140258789062
      coord_origin: BOTTOMLEFT
      l: 277.7886962890625
      r: 300.4436340332031
      t: 576.7056274414062
    charspan:
    - 0
    - 8
    page_no: 15
  self_ref: '#/texts/308'
  text: "be\r\njust"
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.5327758789062
      coord_origin: BOTTOMLEFT
      l: 309.6737060546875
      r: 310.495849609375
      t: 565.05517578125
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/309'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.4019775390625
      coord_origin: BOTTOMLEFT
      l: 319.8288269042969
      r: 326.6206359863281
      t: 576.4060668945312
    charspan:
    - 0
    - 4
    page_no: 15
  self_ref: '#/texts/310'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.8510131835938
      coord_origin: BOTTOMLEFT
      l: 333.8427734375
      r: 340.63458251953125
      t: 568.6151733398438
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/311'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.2618408203125
      coord_origin: BOTTOMLEFT
      l: 347.8561096191406
      r: 382.6747131347656
      t: 581.8992919921875
    charspan:
    - 0
    - 13
    page_no: 15
  self_ref: '#/texts/312'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.8497924804688
      coord_origin: BOTTOMLEFT
      l: 389.8963623046875
      r: 398.5379638671875
      t: 593.2681884765625
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/313'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 563.0098266601562
      coord_origin: BOTTOMLEFT
      l: 409.6645202636719
      r: 411.916015625
      t: 564.0001220703125
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/314'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.8515014648438
      coord_origin: BOTTOMLEFT
      l: 417.92364501953125
      r: 424.6127014160156
      t: 568.8585815429688
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/315'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.8510131835938
      coord_origin: BOTTOMLEFT
      l: 433.6747741699219
      r: 440.5787048339844
      t: 574.6035766601562
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/316'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.5421142578125
      coord_origin: BOTTOMLEFT
      l: 445.950439453125
      r: 454.4892578125
      t: 591.7086791992188
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/317'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 563.0833740234375
      coord_origin: BOTTOMLEFT
      l: 465.7179870605469
      r: 466.6522216796875
      t: 564.017578125
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/318'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 562.7476806640625
      coord_origin: BOTTOMLEFT
      l: 473.85638427734375
      r: 496.5299987792969
      t: 592.3626098632812
    charspan:
    - 0
    - 12
    page_no: 15
  self_ref: '#/texts/319'
  text: "<EOS>\r\n<pad>"
- children: []
  content_layer: body
  label: text
  orig: "The\r\nLaw\r\nwill\r\nnever\r\nput-In"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 442.3211364746094
      coord_origin: BOTTOMLEFT
      l: 123.64026641845703
      r: 172.4728546142578
      t: 465.2003479003906
    charspan:
    - 0
    - 29
    page_no: 15
  self_ref: '#/texts/320'
  text: "The\r\nLaw\r\nwill\r\nnever\r\nput-In"
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 455.2895202636719
      coord_origin: BOTTOMLEFT
      l: 179.69436645507812
      r: 186.48619079589844
      t: 464.6878356933594
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/321'
  text: be
- children: []
  content_layer: body
  label: text
  orig: "perfect\r\nu"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 437.13018798828125
      coord_origin: BOTTOMLEFT
      l: 193.595703125
      r: 202.2466278076172
      t: 465.00750732421875
    charspan:
    - 0
    - 10
    page_no: 15
  self_ref: '#/texts/322'
  text: "perfect\r\nu"
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 463.25372314453125
      coord_origin: BOTTOMLEFT
      l: 213.47598266601562
      r: 215.7274627685547
      t: 464.2439880371094
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/323'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits\r\nLay"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 452.6935119628906
      coord_origin: BOTTOMLEFT
      l: 221.7344970703125
      r: 242.53976440429688
      t: 465.0066223144531
    charspan:
    - 0
    - 13
    page_no: 15
  self_ref: '#/texts/324'
  text: "but\r\nits\r\nLay"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould\r\ner5"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 420.742919921875
      coord_origin: BOTTOMLEFT
      l: 249.76190185546875
      r: 270.5670471191406
      t: 464.4179992675781
    charspan:
    - 0
    - 24
    page_no: 15
  self_ref: '#/texts/325'
  text: "application\r\nshould\r\ner5"
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 455.2895202636719
      coord_origin: BOTTOMLEFT
      l: 277.7886962890625
      r: 284.58050537109375
      t: 464.6878356933594
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/326'
  text: be
- children: []
  content_layer: body
  label: text
  orig: just
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 450.114501953125
      coord_origin: BOTTOMLEFT
      l: 291.8020324707031
      r: 300.4436340332031
      t: 465.0060729980469
    charspan:
    - 0
    - 4
    page_no: 15
  self_ref: '#/texts/327'
  text: just
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 462.2615051269531
      coord_origin: BOTTOMLEFT
      l: 309.6743469238281
      r: 310.4964904785156
      t: 464.7839050292969
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/328'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 450.7049255371094
      coord_origin: BOTTOMLEFT
      l: 319.8294372558594
      r: 326.6212463378906
      t: 464.708984375
    charspan:
    - 0
    - 4
    page_no: 15
  self_ref: '#/texts/329'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 458.94390869140625
      coord_origin: BOTTOMLEFT
      l: 333.8427734375
      r: 340.63458251953125
      t: 464.7080993652344
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/330'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 445.3677978515625
      coord_origin: BOTTOMLEFT
      l: 347.8561096191406
      r: 382.6747131347656
      t: 465.0052490234375
    charspan:
    - 0
    - 13
    page_no: 15
  self_ref: '#/texts/331'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 434.02392578125
      coord_origin: BOTTOMLEFT
      l: 389.8963623046875
      r: 398.5379638671875
      t: 464.4423522949219
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/332'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 463.25372314453125
      coord_origin: BOTTOMLEFT
      l: 409.6651306152344
      r: 411.9166259765625
      t: 464.2439880371094
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/333'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 458.4190979003906
      coord_origin: BOTTOMLEFT
      l: 417.92364501953125
      r: 424.6127014160156
      t: 464.4261779785156
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/334'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 453.2361145019531
      coord_origin: BOTTOMLEFT
      l: 433.6747741699219
      r: 440.5787048339844
      t: 464.98870849609375
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/335'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 435.2524108886719
      coord_origin: BOTTOMLEFT
      l: 445.950439453125
      r: 454.4892578125
      t: 464.4189758300781
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/336'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 463.3291015625
      coord_origin: BOTTOMLEFT
      l: 465.7179870605469
      r: 466.6522216796875
      t: 464.2633056640625
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/337'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "The\r\nLaw\r\nwill\r\nnever\r\nbe\r\nperfect\r\n,\r\nbut\r\nits\r\nplication\r\
    \nshould\r\nbe\r\njust\r\n-\r\nthis\r\nis\r\nwhat\r\nwe\r\nare\r\nmissing\r\n\
    ,\r\nin\r\nmy\r\nopinion\r\n.\r\n<EOS>\r\n<pad>\r\nInput-Input Layer5"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 434.9471740722656
      coord_origin: BOTTOMLEFT
      l: 108.10050201416016
      r: 496.5299987792969
      t: 464.5621643066406
    charspan:
    - 0
    - 170
    page_no: 15
  self_ref: '#/texts/338'
  text: "The\r\nLaw\r\nwill\r\nnever\r\nbe\r\nperfect\r\n,\r\nbut\r\nits\r\nplication\r\
    \nshould\r\nbe\r\njust\r\n-\r\nthis\r\nis\r\nwhat\r\nwe\r\nare\r\nmissing\r\n\
    ,\r\nin\r\nmy\r\nopinion\r\n.\r\n<EOS>\r\n<pad>\r\nInput-Input Layer5"
- children: []
  content_layer: body
  label: text
  orig: The Law will never
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 343.9552917480469
      coord_origin: BOTTOMLEFT
      l: 123.44071960449219
      r: 172.29661560058594
      t: 367.4345703125
    charspan:
    - 0
    - 18
    page_no: 15
  self_ref: '#/texts/339'
  text: The Law will never
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.5341491699219
      coord_origin: BOTTOMLEFT
      l: 179.5222930908203
      r: 186.31744384765625
      t: 353.93707275390625
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/340'
  text: be
- children: []
  content_layer: body
  label: text
  orig: perfect
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.54412841796875
      coord_origin: BOTTOMLEFT
      l: 193.43045043945312
      r: 202.08563232421875
      t: 372.43511962890625
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/341'
  text: perfect
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.702392578125
      coord_origin: BOTTOMLEFT
      l: 213.3204803466797
      r: 215.57305908203125
      t: 345.6931457519531
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/342'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.5347900390625
      coord_origin: BOTTOMLEFT
      l: 221.58302307128906
      r: 242.3985137939453
      t: 356.85394287109375
    charspan:
    - 0
    - 8
    page_no: 15
  self_ref: '#/texts/343'
  text: "but\r\nits"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.2170104980469
      coord_origin: BOTTOMLEFT
      l: 249.6241912841797
      r: 270.4395446777344
      t: 387.9595947265625
    charspan:
    - 0
    - 19
    page_no: 15
  self_ref: '#/texts/344'
  text: "application\r\nshould"
- children: []
  content_layer: body
  label: text
  orig: "be\r\njust"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 343.50665283203125
      coord_origin: BOTTOMLEFT
      l: 277.6647033691406
      r: 300.33074951171875
      t: 358.405517578125
    charspan:
    - 0
    - 8
    page_no: 15
  self_ref: '#/texts/345'
  text: "be\r\njust"
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.2257080078125
      coord_origin: BOTTOMLEFT
      l: 309.56536865234375
      r: 310.38787841796875
      t: 346.7493591308594
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/346'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.0948486328125
      coord_origin: BOTTOMLEFT
      l: 319.7254333496094
      r: 326.5205993652344
      t: 358.10577392578125
    charspan:
    - 0
    - 4
    page_no: 15
  self_ref: '#/texts/347'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.54412841796875
      coord_origin: BOTTOMLEFT
      l: 333.7463073730469
      r: 340.5414733886719
      t: 350.3111267089844
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/348'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 343.95465087890625
      coord_origin: BOTTOMLEFT
      l: 347.7664794921875
      r: 382.6022033691406
      t: 363.6017150878906
    charspan:
    - 0
    - 13
    page_no: 15
  self_ref: '#/texts/349'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.5428771972656
      coord_origin: BOTTOMLEFT
      l: 389.8273620605469
      r: 398.47320556640625
      t: 374.9762268066406
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/350'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.7030334472656
      coord_origin: BOTTOMLEFT
      l: 409.605224609375
      r: 411.8578186035156
      t: 345.69378662109375
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/351'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.54473876953125
      coord_origin: BOTTOMLEFT
      l: 417.8683776855469
      r: 424.56072998046875
      t: 350.5547790527344
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/352'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.54412841796875
      coord_origin: BOTTOMLEFT
      l: 433.62725830078125
      r: 440.5345764160156
      t: 356.3024597167969
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/353'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.23504638671875
      coord_origin: BOTTOMLEFT
      l: 445.9089050292969
      r: 454.4519348144531
      t: 373.4158935546875
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/354'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.77655029296875
      coord_origin: BOTTOMLEFT
      l: 465.6861877441406
      r: 466.6208801269531
      t: 345.71124267578125
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/355'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 344.440673828125
      coord_origin: BOTTOMLEFT
      l: 473.8279724121094
      r: 496.5133056640625
      t: 374.0701904296875
    charspan:
    - 0
    - 12
    page_no: 15
  self_ref: '#/texts/356'
  text: "<EOS>\r\n<pad>"
- children: []
  content_layer: body
  label: text
  orig: "The\r\nLaw\r\nwill\r\nnever"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 223.9551239013672
      coord_origin: BOTTOMLEFT
      l: 123.44071960449219
      r: 172.2972412109375
      t: 246.8455352783203
    charspan:
    - 0
    - 21
    page_no: 15
  self_ref: '#/texts/357'
  text: "The\r\nLaw\r\nwill\r\nnever"
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 236.92987060546875
      coord_origin: BOTTOMLEFT
      l: 179.5222930908203
      r: 186.31744384765625
      t: 246.33279418945312
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/358'
  text: be
- children: []
  content_layer: body
  label: text
  orig: perfect
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 218.76165771484375
      coord_origin: BOTTOMLEFT
      l: 193.43045043945312
      r: 202.08563232421875
      t: 246.6526336669922
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/359'
  text: perfect
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 244.89796447753906
      coord_origin: BOTTOMLEFT
      l: 213.3204803466797
      r: 215.57305908203125
      t: 245.88873291015625
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/360'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: "but\r\nits"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 234.33258056640625
      coord_origin: BOTTOMLEFT
      l: 221.58302307128906
      r: 242.3985137939453
      t: 246.65171813964844
    charspan:
    - 0
    - 8
    page_no: 15
  self_ref: '#/texts/361'
  text: "but\r\nits"
- children: []
  content_layer: body
  label: text
  orig: "application\r\nshould"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 202.36630249023438
      coord_origin: BOTTOMLEFT
      l: 249.6241912841797
      r: 270.4395446777344
      t: 246.06280517578125
    charspan:
    - 0
    - 19
    page_no: 15
  self_ref: '#/texts/362'
  text: "application\r\nshould"
- children: []
  content_layer: body
  label: text
  orig: be
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 236.92987060546875
      coord_origin: BOTTOMLEFT
      l: 277.6647033691406
      r: 284.4598693847656
      t: 246.33279418945312
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/363'
  text: be
- children: []
  content_layer: body
  label: text
  orig: just
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 231.7523193359375
      coord_origin: BOTTOMLEFT
      l: 291.6849060058594
      r: 300.33074951171875
      t: 246.65118408203125
    charspan:
    - 0
    - 4
    page_no: 15
  self_ref: '#/texts/364'
  text: just
- children: []
  content_layer: body
  label: text
  orig: '-'
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 243.9052734375
      coord_origin: BOTTOMLEFT
      l: 309.56536865234375
      r: 310.38787841796875
      t: 246.42892456054688
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/365'
  text: '-'
- children: []
  content_layer: body
  label: text
  orig: this
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 232.34304809570312
      coord_origin: BOTTOMLEFT
      l: 319.72607421875
      r: 326.521240234375
      t: 246.35397338867188
    charspan:
    - 0
    - 4
    page_no: 15
  self_ref: '#/texts/366'
  text: this
- children: []
  content_layer: body
  label: text
  orig: is
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 240.58607482910156
      coord_origin: BOTTOMLEFT
      l: 333.7463073730469
      r: 340.5414733886719
      t: 246.3530731201172
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/367'
  text: is
- children: []
  content_layer: body
  label: text
  orig: "what\r\nwe\r\nare"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 227.00328063964844
      coord_origin: BOTTOMLEFT
      l: 347.7664794921875
      r: 382.6022033691406
      t: 246.6503448486328
    charspan:
    - 0
    - 13
    page_no: 15
  self_ref: '#/texts/368'
  text: "what\r\nwe\r\nare"
- children: []
  content_layer: body
  label: text
  orig: missing
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 215.65396118164062
      coord_origin: BOTTOMLEFT
      l: 389.8273620605469
      r: 398.47320556640625
      t: 246.0872802734375
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/369'
  text: missing
- children: []
  content_layer: body
  label: text
  orig: ','
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 244.89796447753906
      coord_origin: BOTTOMLEFT
      l: 409.6058349609375
      r: 411.8584289550781
      t: 245.88873291015625
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/370'
  text: ','
- children: []
  content_layer: body
  label: text
  orig: in
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 240.0609893798828
      coord_origin: BOTTOMLEFT
      l: 417.8683776855469
      r: 424.56072998046875
      t: 246.0709991455078
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/371'
  text: in
- children: []
  content_layer: body
  label: text
  orig: my
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 234.87547302246094
      coord_origin: BOTTOMLEFT
      l: 433.62725830078125
      r: 440.5345764160156
      t: 246.63380432128906
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/372'
  text: my
- children: []
  content_layer: body
  label: text
  orig: opinion
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 216.8829345703125
      coord_origin: BOTTOMLEFT
      l: 445.9089050292969
      r: 454.4519348144531
      t: 246.06378173828125
    charspan:
    - 0
    - 7
    page_no: 15
  self_ref: '#/texts/373'
  text: opinion
- children: []
  content_layer: body
  label: text
  orig: .
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 244.97335815429688
      coord_origin: BOTTOMLEFT
      l: 465.6861877441406
      r: 466.6208801269531
      t: 245.90805053710938
    charspan:
    - 0
    - 1
    page_no: 15
  self_ref: '#/texts/374'
  text: .
- children: []
  content_layer: body
  label: text
  orig: "<EOS>\r\n<pad>"
  parent:
    $ref: '#/pictures/5'
  prov:
  - bbox:
      b: 216.57754516601562
      coord_origin: BOTTOMLEFT
      l: 473.8279724121094
      r: 496.5133056640625
      t: 246.20703125
    charspan:
    - 0
    - 12
    page_no: 15
  self_ref: '#/texts/375'
  text: "<EOS>\r\n<pad>"
- children: []
  content_layer: furniture
  label: page_footer
  orig: '15'
  parent:
    $ref: '#/body'
  prov:
  - bbox:
      b: 41.982479095458984
      coord_origin: BOTTOMLEFT
      l: 302.12481689453125
      r: 310.3638916015625
      t: 48.96625900268555
    charspan:
    - 0
    - 2
    page_no: 15
  self_ref: '#/texts/376'
  text: '15'
version: 1.4.0
