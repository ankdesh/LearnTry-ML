{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcded1a6-2430-4707-a78c-c82f4c5ee6fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "**Note: For other Non-OpenAI models, you can check out: https://python.langchain.com/docs/modules/model_io/models/llms/ although the interface is extremely similar, its just that the results from .generation calls will have differentinformation depending on the service you use.**\n",
    "\n",
    "## Text Model Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49cecee-a933-4f48-b1e1-dca183fd07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71ca3e5-05dd-40e9-a2d3-26e186a365af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"/home/ankdesh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/\" # \"meta-llama/Llama-2-7b-hf\"\n",
    "model_id = \"EleutherAI/pythia-1b\"\n",
    "#model_id = \"/home/ankdesh/.cache/huggingface/hub/models--stabilityai--stablelm-zephyr-3b/snapshots/9974c58a0ec4be4cd6f55e814a2a93b9cf163823/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, \n",
    "                max_new_tokens=200, trust_remote_code=True, device=0, \n",
    "                torch_dtype=torch.float16)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e8ada-ae30-40a7-84ea-64cd08887315",
   "metadata": {},
   "source": [
    "## Text Model Call\n",
    "\n",
    "This is the simplest way to get a text autocomplete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93f7d94f-5683-48be-b852-86eb8ac90ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon.\n",
      "\n",
      "Pluto is the only planet in the solar system that has a moon\n"
     ]
    }
   ],
   "source": [
    "print(llm('Here is a fun fact about Pluto:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78677192-2c35-43fb-89f2-8d5ce9d7e30e",
   "metadata": {},
   "source": [
    "You can also use generate to get full output with more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "802ffd32-a6bc-4319-875c-ea1dffc5e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# NEEDS TO BE A LIST, EVEN FOR JUST ONE STRING\n",
    "result = llm.generate(['Here is a fun fact about Pluto:',\n",
    "                     'Here is a fun fact about Mars:']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf0c3c6a-eab9-4b6c-a785-63f799cc23a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLMResult',\n",
       " 'description': 'Class that contains all results for a batched LLM call.',\n",
       " 'type': 'object',\n",
       " 'properties': {'generations': {'title': 'Generations',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'array', 'items': {'$ref': '#/definitions/Generation'}}},\n",
       "  'llm_output': {'title': 'Llm Output', 'type': 'object'},\n",
       "  'run': {'title': 'Run',\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/RunInfo'}}},\n",
       " 'required': ['generations'],\n",
       " 'definitions': {'Generation': {'title': 'Generation',\n",
       "   'description': 'A single text generation output.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'title': 'Generation Info', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'RunInfo': {'title': 'RunInfo',\n",
       "   'description': 'Class that contains metadata for a single execution of a Chain or model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'run_id': {'title': 'Run Id',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'}},\n",
       "   'required': ['run_id']}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff190d3a-a99c-4079-8a9c-3ef150efc75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text=' it is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon.\\n\\nPluto is the only planet in the solar system that has a moon')], [Generation(text=' It has a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot of water.\\n\\nThe Red Planet is also home to a lot')]] llm_output=None run=[RunInfo(run_id=UUID('1395660c-73d9-4a4d-895f-09ee49707be7')), RunInfo(run_id=UUID('847f764c-1b20-4f82-9a11-6f3ed6f23d3a'))]\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e7d94-7452-4907-a0e1-06347146075f",
   "metadata": {},
   "source": [
    "# Chat Models\n",
    "\n",
    "The most popular models are actually chat models, that have a System Message and then a series of Assistant and Human Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6c88ca-2af7-4339-a377-3ca960404573",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = llm # Try with LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb524f31-3d3a-4a4b-bc00-d21843490193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa286838-f569-4e05-9ecf-e19b7e481f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = AIMessage(content=\"Can you tell me a fact about Earth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a0e9703-5b14-442c-a98e-8ca57de3c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me a fact about Earth?\n"
     ]
    }
   ],
   "source": [
    "def get_str_from_msg(list_str):\n",
    "    # Convert the messages to strings\n",
    "    messages_str = [hm.content for message in list_str]\n",
    "    # Join the messages into a single string\n",
    "    messages_combined = \"\\n\".join(messages_str)\n",
    "    return messages_combined\n",
    "print (get_str_from_msg([hm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6555a0-3bfb-49e3-97aa-186eb5a528a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = llm(get_str_from_msg([HumanMessage(content=\"Can you tell me a fact about Earth?\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58180fce-e04b-41d2-9ae8-87a41b232890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be3fb5bc-13e3-45de-98a9-45fe1361d30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = chat(get_str_from_msg([SystemMessage(content='You are a very rude teenager who only wants to party and not answer questions'),\n",
    "               HumanMessage(content='Can you tell me a fact about Earth?')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "013daa46-53e4-47dc-83c9-5037c8286feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?\\nCan you tell me a fact about Earth?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5026430-caaa-4f9c-938e-328b2f383c5b",
   "metadata": {},
   "source": [
    "## Extra Parameters and Args\n",
    "\n",
    "Here we add in some extra parameters and args, note we chose some pretty extreme values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe01c99b-b14f-4358-a532-765a19bb5666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "result = chat(get_str_from_msg([HumanMessage(content='Can you tell me a fact about Earth?')]),\n",
    "                 temperature=2,presence_penalty=1,max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91a67f85-9c18-4ac3-9b2e-ff021c121a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\nA:\\n\\nThe answer is yes.\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b698-ba1b-4c2f-a23a-5e757dd84e2a",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "Making the same exact request often? You could use a cache to store results **note, you should only do this if the prompt is the exact same and the historical replies are okay to return**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3341f0b-6524-4711-a019-ab9ba497e4cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m----> 4\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(openai_api_key\u001b[38;5;241m=\u001b[39m\u001b[43mapi_key\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639f253-5b37-4ffc-b028-c22b3df2b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a fact about Mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d706fd5-6067-4d7e-80ab-cd96ccd4a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will notice this reply is instant!\n",
    "llm.predict('Tell me a fact about Mars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee731f-1933-4260-a16a-63a2dccbbacc",
   "metadata": {},
   "source": [
    "You can also use SQLite Caches: https://python.langchain.com/docs/modules/model_io/models/chat/how_to/chat_model_caching#sqlite-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e061d-c254-4e64-b81a-5dbd4f0cc59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
