{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcded1a6-2430-4707-a78c-c82f4c5ee6fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b99d72-d919-4362-a3e9-14e361f231b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Understanding Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cf51224-6f3b-4093-a0e1-338c8eae22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdb72664-ce8d-4d88-b003-8a930ff2c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Pluto was reclassified as a dwarf planet in 2006, making it the largest object in the Kuiper belt.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(openai_api_key=api_key)\n",
    "print(llm('Here is a fun fact about Pluto:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579bfe0-103e-430e-bac6-fa9fa50055b5",
   "metadata": {},
   "source": [
    "You can also use generate to get full output with more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24db968b-ef5a-4a31-bbb9-a08e48cb0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDS TO BE A LIST, EVEN FOR JUST ONE STRING\n",
    "result = llm.generate(['Here is a fun fact about Pluto:',\n",
    "                     'Here is a fun fact about Mars:']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23e6c06-1543-42a5-94e7-dbb6bf8f4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLMResult',\n",
       " 'description': 'Class that contains all results for a batched LLM call.',\n",
       " 'type': 'object',\n",
       " 'properties': {'generations': {'title': 'Generations',\n",
       "   'type': 'array',\n",
       "   'items': {'type': 'array', 'items': {'$ref': '#/definitions/Generation'}}},\n",
       "  'llm_output': {'title': 'Llm Output', 'type': 'object'},\n",
       "  'run': {'title': 'Run',\n",
       "   'type': 'array',\n",
       "   'items': {'$ref': '#/definitions/RunInfo'}}},\n",
       " 'required': ['generations'],\n",
       " 'definitions': {'Generation': {'title': 'Generation',\n",
       "   'description': 'A single text generation output.',\n",
       "   'type': 'object',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'title': 'Generation Info', 'type': 'object'},\n",
       "    'type': {'title': 'Type',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'type': 'string'}},\n",
       "   'required': ['text']},\n",
       "  'RunInfo': {'title': 'RunInfo',\n",
       "   'description': 'Class that contains metadata for a single execution of a Chain or model.',\n",
       "   'type': 'object',\n",
       "   'properties': {'run_id': {'title': 'Run Id',\n",
       "     'type': 'string',\n",
       "     'format': 'uuid'}},\n",
       "   'required': ['run_id']}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6575b5-3331-4ec6-9281-8f793ab541dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='\\n\\nPluto is the only dwarf planet in our Solar System that has a moon larger than itself. Its largest moon, Charon, is over half the size of Pluto.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nMars has the tallest mountain of any planet in our solar system. The mountain, called Olympus Mons, is 21 kilometers (13 miles) tall and about 600 kilometers (370 miles) in diameter.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 93, 'prompt_tokens': 16, 'completion_tokens': 77}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('dbc7e940-9937-4f6f-81ae-e9d4987fd66f')), RunInfo(run_id=UUID('70bcf323-cd28-4363-915d-6ffc5246e457'))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d37ccf-99d2-4aa6-ab73-0bd22e55b7b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Language Model Templates\n",
    "\n",
    "### No Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e34d990-cc7c-47f6-a191-854a575aabca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# An example prompt with no input variables\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a fact\")\n",
    "no_input_prompt.format()\n",
    "# -> \"Tell me a fact.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be521325-8963-4505-8bf3-83b05e8a796a",
   "metadata": {},
   "source": [
    "### Single Input Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361467f3-81d3-4553-9a22-1198f44598c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact about Mars.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example prompt with one input variable\n",
    "one_input_prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a fact about {topic}.\")\n",
    "# Notice how the stirng \"topic\" gets automatically converted to a parameter name, very convienent! \n",
    "one_input_prompt.format(topic=\"Mars\")\n",
    "# -> \"Tell me a fact about Mars\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8c513-b8d0-4616-9015-e0133cfc45d4",
   "metadata": {},
   "source": [
    "### Multiple Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aba25263-4b12-40d8-8c56-e21b9bc37891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a fact about Mars for a student 8th Grade level.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example prompt with multiple input variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"], \n",
    "    template=\"Tell me a fact about {topic} for a student {level} level.\"\n",
    ")\n",
    "multiple_input_prompt.format(topic='Mars',level='8th Grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d47473-7d74-4bbf-875e-898bc9cdb1ec",
   "metadata": {},
   "source": [
    "# Chat Model Templates\n",
    "\n",
    "Chat models require a list of chat messages called a prompt, which is different from a raw string that you would input into a language model. Each message in the prompt is associated with a role, such as AI, human, or system.\n",
    "\n",
    "For instance, when using the OpenAI Chat Completion API, a chat message can be assigned the role of AI, human, or system. The model is designed to pay closer attention to instructions provided in system chat messages.\n",
    "\n",
    "To simplify the process of constructing and working with prompts, LangChain offers various prompt templates. It is highly recommended to utilize these chat-related prompt templates instead of PromptTemplate when interacting with chat models. This will allow you to fully harness the potential of the underlying chat model and enhance your experience.\n",
    "\n",
    "We will favor these models in the course due to upcoming changes in the OpenAI ecosystem where chat agents will be favored over text completion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923b8070-19ab-4972-89f0-57b6e56053fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbc8eeab-11ff-4c46-b69b-e963066980d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template=\"You are an {language} coding expert great at writing recursive functions for everything\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d93298f9-18d3-43af-bc23-633f5f3ab269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2348e5e3-e878-403a-94e9-be61359fbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template=\"{problem_statement}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d01b14b6-f422-4a08-bd0e-bba0f494563d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['problem_statement']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "706756df-1264-4121-8043-b733e60188c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52fdf3cf-f3d9-4108-ae72-b630487d9e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language', 'problem_statement']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b19ae01-3be8-41c9-a470-3fd6fc69e801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are an python coding expert great at writing recursive functions for everything'),\n",
       " HumanMessage(content='count number of nodes in binary tree')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a chat completion from the formatted messages\n",
    "chat_prompt.format_prompt(language=\"python\", problem_statement=\"count number of nodes in binary tree\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73121537-8cee-42fc-ba0e-fc1c18154957",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = chat_prompt.format_prompt(language=\"python\", problem_statement=\"count number of nodes in binary tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f92882ca-beb4-429b-ad73-d6850ea682d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "print (type(request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f739bc95-42c3-4c01-9f77-6dcfcc6ec21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an python coding expert great at writing recursive functions for everything\n",
      "Human: count number of nodes in binary tree\n"
     ]
    }
   ],
   "source": [
    "print (request.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43e55dd4-f863-4673-b297-2945af66d2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are an python coding expert great at writing recursive functions for everything'), HumanMessage(content='count number of nodes in binary tree')]\n"
     ]
    }
   ],
   "source": [
    "print (request.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276622f-d81c-40ed-864b-63d4c4311d84",
   "metadata": {},
   "source": [
    "## Prompt Templates with an LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1496d66-efec-4cd5-a62e-d33f9669941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8978391d-9c09-4b99-8520-27491ece4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat(request.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88f47c1b-123c-4f47-8ee1-b72c88cbd048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To count the number of nodes in a binary tree, we can use a recursive function that traverses through the tree and increments a counter for each node encountered.\\n\\nHere\\'s an example implementation:\\n\\n```python\\nclass Node:\\n    def __init__(self, value):\\n        self.value = value\\n        self.left = None\\n        self.right = None\\n\\ndef count_nodes(root):\\n    if root is None:\\n        return 0\\n\\n    return 1 + count_nodes(root.left) + count_nodes(root.right)\\n```\\n\\nIn this implementation, the `count_nodes` function takes the root node of the binary tree as input and returns the total number of nodes in the tree.\\n\\nWe start by checking if the root node is None. If it is, then the tree is empty and we return 0.\\n\\nIf the root node is not None, we increment the counter by 1 and recursively call the `count_nodes` function on both the left and right subtrees. The sum of the counts from the left and right subtrees, along with the root node, gives us the total number of nodes in the binary tree.\\n\\nHere\\'s an example usage:\\n\\n```python\\n# Create a binary tree\\nroot = Node(1)\\nroot.left = Node(2)\\nroot.right = Node(3)\\nroot.left.left = Node(4)\\nroot.left.right = Node(5)\\n\\n# Count the number of nodes\\nnum_nodes = count_nodes(root)\\nprint(\"Number of nodes:\", num_nodes)\\n```\\n\\nOutput:\\n```\\nNumber of nodes: 5\\n```\\n\\nIn this example, the binary tree has 5 nodes, including the root node.'\n"
     ]
    }
   ],
   "source": [
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ae9e58d-00b9-4ba4-af37-9ff35a4bef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To count the number of nodes in a binary tree, we can use a recursive function that traverses through the tree and increments a counter for each node encountered.\n",
      "\n",
      "Here's an example implementation:\n",
      "\n",
      "```python\n",
      "class Node:\n",
      "    def __init__(self, value):\n",
      "        self.value = value\n",
      "        self.left = None\n",
      "        self.right = None\n",
      "\n",
      "def count_nodes(root):\n",
      "    if root is None:\n",
      "        return 0\n",
      "\n",
      "    return 1 + count_nodes(root.left) + count_nodes(root.right)\n",
      "```\n",
      "\n",
      "In this implementation, the `count_nodes` function takes the root node of the binary tree as input and returns the total number of nodes in the tree.\n",
      "\n",
      "We start by checking if the root node is None. If it is, then the tree is empty and we return 0.\n",
      "\n",
      "If the root node is not None, we increment the counter by 1 and recursively call the `count_nodes` function on both the left and right subtrees. The sum of the counts from the left and right subtrees, along with the root node, gives us the total number of nodes in the binary tree.\n",
      "\n",
      "Here's an example usage:\n",
      "\n",
      "```python\n",
      "# Create a binary tree\n",
      "root = Node(1)\n",
      "root.left = Node(2)\n",
      "root.right = Node(3)\n",
      "root.left.left = Node(4)\n",
      "root.left.right = Node(5)\n",
      "\n",
      "# Count the number of nodes\n",
      "num_nodes = count_nodes(root)\n",
      "print(\"Number of nodes:\", num_nodes)\n",
      "```\n",
      "\n",
      "Output:\n",
      "```\n",
      "Number of nodes: 5\n",
      "```\n",
      "\n",
      "In this example, the binary tree has 5 nodes, including the root node.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ee656-ec4b-4c5b-b839-1b3c2f269f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
