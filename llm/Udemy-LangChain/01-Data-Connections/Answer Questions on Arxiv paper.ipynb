{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcded1a6-2430-4707-a78c-c82f4c5ee6fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "#  Document Loading Exercise \n",
    "\n",
    "## Answering a Single Question\n",
    "\n",
    "Using the Wikipedia Document Loader Integration,can you make a function that accepts a famous historical figure name and a question about them, and then uses a ChatModel to answer questions with the additional context? Notice how in our example, the query doesn't mention the famous person. Keep in mind there are many potential ways to solve this problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a539244-89c6-4019-8608-1cdf51ba1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "\n",
    "def answer_question_about(person_name,question):\n",
    "    \n",
    "    '''\n",
    "    Use the Wikipedia Document Loader to help answer questions about someone, insert it as additional helpful context.\n",
    "    '''\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9074e58-b4af-4a21-a503-a0c58621a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c1d6d8-1177-4ae8-ae82-96282dbb4beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ArxivLoader(query=\"2310.01796\", load_max_docs=2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "291af9e5-82d2-4ed8-bf32-44b37cb510f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMParser: A LLM-based Log Parsing Framework\n",
      "Zhihan Jiang†, Jinyang Liu†, Zhuangbin Chen‡, Yichen Li†, Junjie Huang†,\n",
      "Yintong Huo†, Pinjia He§, Jiazhen Gu†, Michael R. Lyu†\n",
      "† The Chinese University of Hong Kong, Hong Kong, China,\n",
      "{zhjiang22, jyliu, ycli21, jjhuang23, ythong, jzgu, lyu}@cse.cuhk.edu.hk\n",
      "‡Sun Yat-sen University, Zhuhai, China, chenzhb36@mail.sysu.edu.cn\n",
      "§The Chinese University of Hong Kong, Shenzhen, China, hepinjia@cuhk.edu.cn\n",
      "Abstract—The process of log parsing, which converts log\n",
      "messages into structured formats, is a crucial step for various\n",
      "log analysis tasks. Although numerous log parsers have been\n",
      "proposed, their effectiveness on complex log data is often hin-\n",
      "dered due to reliance on human-made rules or learning-based\n",
      "models with limited training data. The recent rise of powerful\n",
      "large language models (LLMs) shows potential for log parsing\n",
      "due to their extensive pre-trained knowledge related to code\n",
      "and logging. However, their accuracy is currently limited due to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2a9bb54-e383-40fc-98c3-25c6a75d586e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"{'Published': '2023-10-03', 'Title': 'LLMParser: A LLM-based Log Parsing \"\n",
      " \"Framework', 'Authors': 'Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen \"\n",
      " \"Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, Michael R. Lyu', \"\n",
      " \"'Summary': 'The process of log parsing, which converts log messages into \"\n",
      " 'structured\\\\nformats, is a crucial step for various log analysis tasks. '\n",
      " 'Although numerous\\\\nlog parsers have been proposed, their effectiveness on '\n",
      " 'complex log data is\\\\noften hindered due to reliance on human-made rules or '\n",
      " 'learning-based models\\\\nwith limited training data. The recent rise of '\n",
      " 'powerful large language models\\\\n(LLMs) shows potential for log parsing due '\n",
      " 'to their extensive pre-trained\\\\nknowledge related to code and logging. '\n",
      " 'However, their accuracy is currently\\\\nlimited due to the lack of '\n",
      " 'specialized log parsing capabilities. Additionally,\\\\nthe inconsistency of '\n",
      " 'their answers and significant overhead obstruct the\\\\npractical '\n",
      " 'implementation of LLM-based log parsing.\\\\n  To tackle these challenges, we '\n",
      " 'introduce LLMParser, the first practical\\\\nLLM-based log parsing framework. '\n",
      " 'LLMParser enables accurate and robust log\\\\nparsing by leveraging the '\n",
      " 'in-context learning (ICL) capability of the LLM,\\\\nemploying a hierarchical '\n",
      " 'candidate sampling algorithm, and selecting\\\\nhigh-quality demonstrations. '\n",
      " 'LLMParser also includes a novel adaptive parsing\\\\ncache component to store '\n",
      " 'and refine the templates generated by the LLM. This\\\\ndesign aids in '\n",
      " 'addressing the inefficiency of LLMs by rapid matching to\\\\npreviously parsed '\n",
      " 'log templates. LLMParser also adaptively updates the\\\\ntemplates in the '\n",
      " 'parsing cache to ensure consistent parsed results. Extensive\\\\nevaluation on '\n",
      " 'large-scale public datasets demonstrates that LLMParser surpasses\\\\nthe '\n",
      " 'state-of-the-art methods. Furthermore, LLMParser significantly reduces '\n",
      " 'the\\\\nquery times to LLMs, achieving efficiency comparable to the most '\n",
      " \"efficient\\\\nbaseline, Drain.'}\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint (str(docs[0].metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd03404f-d2d5-4461-8af1-d7d75880914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2023-10-03', 'Title': 'LLMParser: A LLM-based Log Parsing Framework', 'Authors': 'Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, Michael R. Lyu', 'Summary': 'The process of log parsing, which converts log messages into structured formats, is a crucial step for various log analysis tasks. Although numerous log parsers have been proposed, their effectiveness on complex log data is often hindered due to reliance on human-made rules or learning-based models with limited training data. The recent rise of powerful large language models (LLMs) shows potential for log parsing due to their extensive pre-trained knowledge related to code and logging. However, their accuracy is currently limited due to the lack of specialized log parsing capabilities. Additionally, the inconsistency of their answers and significant overhead obstruct the practical implementation of LLM-based log parsing.   To tackle these challenges, we introduce LLMParser, the first practical LLM-based log parsing framework. LLMParser enables accurate and robust log parsing by leveraging the in-context learning (ICL) capability of the LLM, employing a hierarchical candidate sampling algorithm, and selecting high-quality demonstrations. LLMParser also includes a novel adaptive parsing cache component to store and refine the templates generated by the LLM. This design aids in addressing the inefficiency of LLMs by rapid matching to previously parsed log templates. LLMParser also adaptively updates the templates in the parsing cache to ensure consistent parsed results. Extensive evaluation on large-scale public datasets demonstrates that LLMParser surpasses the state-of-the-art methods. Furthermore, LLMParser significantly reduces the query times to LLMs, achieving efficiency comparable to the most efficient baseline, Drain.'}\n"
     ]
    }
   ],
   "source": [
    "paper_summary = str(docs[0].metadata).replace(\"\\\\n\",\" \")\n",
    "print (paper_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b315247-1619-403f-a4d8-a1449752cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def answer_question_arxiv_summary(arxiv_code,question):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    docs = ArxivLoader(query=arxiv_code, load_max_docs=2).load()\n",
    "    paper_summary = str(docs[0].metadata).replace(\"\\\\n\",\" \")\n",
    "    \n",
    "    model = ChatOpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "    system_prompt = SystemMessagePromptTemplate.from_template(\"You are a researcher and need to \\\n",
    "    answer the question based on a research paper's Summary\")\n",
    "\n",
    "    human_prompt = HumanMessagePromptTemplate.from_template(\"{paper_summary} \\n {question}\")\n",
    "        \n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_prompt,human_prompt])\n",
    "\n",
    "    request = chat_prompt.format_prompt(paper_summary=paper_summary, question=question)\n",
    "    \n",
    "    answer = model(request.to_messages()).content\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50e36862-2c6f-49b8-8781-1fc2630539ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The title of the paper is \"LLMParser: A LLM-based Log Parsing Framework\".'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question_arxiv_summary(\"2310.01796\", \"What is the title of paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ff6c5b8-e7e0-4fc4-8ca8-9288c706a290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The LLM method used in the paper for log parsing is the large language model (LLM). LLMParser, the log parsing framework introduced in the paper, leverages the in-context learning (ICL) capability of the LLM for accurate and robust log parsing.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question_arxiv_summary(\"2310.01796\", \"What is LLM method used in the paper for log parsing\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049c2028-368b-48d5-9cc4-f07bd43fae9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
