{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83afba23-7e76-43f5-b230-22780c1cfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "149454c2-b1f7-43df-86b1-c13ddb4495fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/ankdesh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/home/ankdesh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da76686-9dff-4753-abbb-c0732a3bd41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bfd9775-4128-44c0-bfbd-3a40173ec080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "sentences = [\"This is me\", \"A 2nd sentence\"]\n",
    "model_base_name = \"/home/ankdesh/.cache/huggingface/hub/models--bert-base-uncased/snapshots/1dbc166cf8765166998eff31ade2eb64c8a40076/\" # \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModel.from_pretrained(model_base_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_base_name)\n",
    "feature_extraction = pipeline('feature-extraction', model=model, tokenizer=tokenizer)\n",
    "embeddings = feature_extraction(sentences) # output should be of size (seq_len, embedding_dim) but is of size (seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1442640d-7f40-4f46-8490-f438fb4aa3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print (len(embeddings[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a8b839-848f-4e63-8a36-5d56562ee853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [-0.6485,  0.6739, -0.0932,  ...,  0.4475,  0.6696,  0.1820],\n",
       "          [-0.6270, -0.0633, -0.3143,  ...,  0.3427,  0.4636,  0.4594],\n",
       "          [ 0.1365,  0.2253, -0.9982,  ...,  0.7263,  0.7519,  0.2361],\n",
       "          [-0.3643, -0.1617,  0.0902,  ..., -0.1785,  0.1282, -0.0451]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1262,  0.0348, -0.0886,  ...,  0.0337,  0.1094,  0.0400],\n",
       "          [ 0.0178,  0.3126,  0.1216,  ..., -0.0405,  0.7481,  0.0525],\n",
       "          [-0.3870, -0.3250, -0.2099,  ...,  0.1536,  0.7361,  0.2984],\n",
       "          [ 0.5721,  0.3080, -0.9041,  ...,  0.3541,  0.6504,  0.6652],\n",
       "          [-0.0852, -0.0108,  0.0267,  ..., -0.2968,  0.5621,  0.1638]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.0154, -0.2096, -0.2084,  ...,  0.1810,  0.1479,  0.0438],\n",
       "          [ 0.1086,  0.6461,  0.5067,  ...,  0.2403,  0.7962, -0.0587],\n",
       "          [-0.4878, -0.1401,  0.2317,  ...,  0.1730,  0.6730,  0.2165],\n",
       "          [ 0.4000, -0.1070, -0.4797,  ...,  0.9555,  0.9279,  0.8852],\n",
       "          [-0.0753, -0.1174,  0.1691,  ..., -0.0315,  0.4616, -0.0108]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.0193, -0.2532, -0.0382,  ...,  0.2943,  0.1534,  0.2204],\n",
       "          [-0.0384,  0.2882,  0.7148,  ...,  0.0700, -0.0573, -0.2595],\n",
       "          [-0.3048, -0.0178,  0.7792,  ..., -0.2209,  0.8506,  0.5111],\n",
       "          [ 0.3653,  0.0041, -0.4302,  ...,  0.6315,  0.9041,  0.7526],\n",
       "          [-0.0673, -0.0950,  0.1219,  ...,  0.0452,  0.0757,  0.0075]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.2427, -0.4520, -0.4953,  ...,  0.5066, -0.0227,  0.5995],\n",
       "          [-0.1372,  0.4794,  0.7730,  ..., -0.0054, -0.2235, -0.5218],\n",
       "          [ 0.1311, -0.1049,  0.8514,  ..., -0.3912,  1.0834,  0.8974],\n",
       "          [ 0.2535, -0.1032, -0.6412,  ...,  0.1239,  0.3670,  0.7172],\n",
       "          [-0.0338, -0.0509,  0.0188,  ...,  0.0106,  0.0456, -0.0325]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.3908, -0.0850, -0.5383,  ..., -0.1428,  0.2821,  0.8762],\n",
       "          [-0.0857,  0.6797,  0.4381,  ...,  0.1509,  0.0162, -0.1086],\n",
       "          [ 0.0445, -0.4608,  1.2875,  ..., -0.0155,  1.0336,  0.8051],\n",
       "          [-0.3148, -0.0684, -0.6009,  ...,  0.2488,  0.2449,  0.9686],\n",
       "          [-0.0212, -0.0345,  0.0217,  ...,  0.0235,  0.0024, -0.0428]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.2339, -0.2713, -0.4761,  ..., -0.1133,  0.3371,  0.8755],\n",
       "          [-0.3896,  0.5896,  0.7245,  ...,  0.1052,  0.6465, -0.0353],\n",
       "          [ 0.0447, -0.2593,  1.1528,  ..., -0.3241,  0.7767,  0.6230],\n",
       "          [-0.6934,  0.4161, -0.4138,  ...,  0.3042,  0.0390,  0.6712],\n",
       "          [ 0.0115, -0.0334, -0.0120,  ...,  0.0091, -0.0227, -0.0396]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.2771, -0.2955, -0.0384,  ..., -0.0209,  0.4242,  0.7662],\n",
       "          [-0.7346,  0.8481,  0.1978,  ..., -0.3065,  0.6352, -0.3583],\n",
       "          [ 0.0803, -0.1294,  0.9225,  ..., -0.0735,  0.8131,  0.5295],\n",
       "          [-0.8425, -0.0346, -0.7503,  ...,  0.2329,  0.4666,  0.8315],\n",
       "          [-0.0171, -0.0549, -0.0133,  ..., -0.0068,  0.0259, -0.0569]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1405,  0.1388, -0.6952,  ..., -0.6759,  0.4530,  0.7790],\n",
       "          [-0.2959,  0.4148, -0.0246,  ..., -0.5057,  0.6150, -0.1583],\n",
       "          [ 0.3615, -0.2533,  0.4882,  ...,  0.1145,  1.1895,  0.6521],\n",
       "          [-0.6944,  0.2089, -1.2917,  ...,  0.1180,  0.4007,  0.7850],\n",
       "          [-0.0072, -0.0544,  0.0087,  ..., -0.0385, -0.0344, -0.0746]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[ 0.1220,  0.4239, -1.1153,  ..., -0.5337,  0.2006,  0.5745],\n",
       "          [-0.9289,  0.4129,  0.4026,  ..., -0.4253,  0.6596, -0.4188],\n",
       "          [ 0.0186, -0.1726,  0.0463,  ..., -0.1634,  0.8974,  0.1996],\n",
       "          [-0.9142,  0.0253, -1.1209,  ...,  0.0897,  0.3622,  0.3906],\n",
       "          [-0.0116, -0.0302,  0.0263,  ..., -0.0360, -0.0958, -0.0520]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.2439,  0.2551, -0.7614,  ..., -0.1479, -0.4327,  0.3700],\n",
       "          [-0.4626,  0.0306,  0.6021,  ..., -0.4227,  0.5686, -0.1784],\n",
       "          [ 0.1267, -0.1366,  0.0685,  ...,  0.2028,  0.4350,  0.1274],\n",
       "          [-0.9527, -0.1730, -0.6591,  ...,  0.4839, -0.1122,  0.2771],\n",
       "          [-0.0204,  0.0083, -0.0803,  ...,  0.1446, -0.0808, -0.0270]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.0010, -0.0589, -0.0993,  ..., -0.2956, -0.1877,  0.1563],\n",
       "          [-0.3669, -0.0581,  0.7168,  ..., -0.6673,  0.6066,  0.2980],\n",
       "          [-0.0384, -0.5164,  0.3364,  ...,  0.0315,  0.2902,  0.7317],\n",
       "          [-0.5749, -0.5942,  0.1018,  ...,  0.4901,  0.1482,  0.6615],\n",
       "          [ 0.0239, -0.0018, -0.0264,  ...,  0.0122, -0.0397,  0.0065]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[[-0.1113,  0.0562, -0.0361,  ..., -0.2194,  0.0860,  0.0852],\n",
       "          [-0.6367, -0.0271,  0.5083,  ..., -0.6209,  0.4845,  0.5720],\n",
       "          [ 0.0934, -0.3753,  0.3588,  ...,  0.0797,  0.1955,  0.6951],\n",
       "          [-0.5003, -0.5218,  0.0553,  ...,  0.5494,  0.2736,  0.1844],\n",
       "          [ 0.7751, -0.0908, -0.2745,  ..., -0.0484, -0.6279, -0.3425]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.IntTensor([tokenizer(sentences)['input_ids'][0]]),return_dict=True, output_hidden_states=True)['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0d172-7861-4300-b280-db11a37b75da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
