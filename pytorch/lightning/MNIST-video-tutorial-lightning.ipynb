{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST('data', train=True, download=True, \n",
    "                           transform=transforms.ToTensor())\n",
    "train, val = random_split (train_data,[55000, 5000])\n",
    "train_loader = DataLoader (train, batch_size=32)\n",
    "val_loader = DataLoader(val, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(28 * 28, 10)\n",
    "\n",
    "def forward(self, x):\n",
    "    return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    return loss\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss 0.43\n",
      "Epoch 1, validation loss 0.40\n",
      "Epoch 2, train loss 0.40\n",
      "Epoch 2, validation loss 0.37\n",
      "Epoch 3, train loss 0.38\n",
      "Epoch 3, validation loss 0.33\n",
      "Epoch 4, train loss 0.35\n",
      "Epoch 4, validation loss 0.30\n",
      "Epoch 5, train loss 0.32\n",
      "Epoch 5, validation loss 0.28\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 5\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = []\n",
    "    for batch in train_loader:\n",
    "        x,y = batch\n",
    "        x = x.view(x.size(0), -1).cuda()\n",
    "        \n",
    "        l = model(x)\n",
    "        J = loss(l, y.cuda())\n",
    "        model.zero_grad()\n",
    "        J.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(J.item())\n",
    "        \n",
    "    print (f'Epoch {epoch+1}, train loss {torch.tensor(losses).mean():.2f}')\n",
    "\n",
    "    losses = []\n",
    "    for batch in val_loader:\n",
    "        x,y = batch\n",
    "        x = x.view(x.size(0), -1).cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l = model(x)\n",
    "        J = loss(l, y.cuda())\n",
    "        \n",
    "    losses.append(J.item())\n",
    "    print (f'Epoch {epoch+1}, validation loss {torch.tensor(losses).mean():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
