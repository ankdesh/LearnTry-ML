{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Coding Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "osxVhElHKapV",
        "JXpmtF-uKapX",
        "SiC14wElKape",
        "NahVYLPKKapm",
        "pFCwsZzVKapy",
        "EjIP1ggBKap1",
        "xnv7LMUXKap6",
        "jGNAZCCYKap7",
        "yfnMhy6xKap9",
        "8x7I47G6KaqE",
        "ooeqV6GxKaqL",
        "XxhEIraGKaqb",
        "rsUT-G8YKaqd",
        "wtBXmPJIKaql"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIvqaAUzKaos"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers\n",
        "\n",
        "print('TF version:', tf.__version__)\n",
        "print('TFP version:', tfp.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9HBWGVKaox"
      },
      "source": [
        "# Probabilistic layers and Bayesian neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVYM_IToKaox"
      },
      "source": [
        "## Coding tutorials\n",
        "#### [1. The DistributionLambda layer](#coding_tutorial_1)\n",
        "#### [2. Probabilistic layers](#coding_tutorial_2)\n",
        "#### [3. The DenseVariational layer](#coding_tutorial_3)\n",
        "#### [4. Reparameterization layers](#coding_tutorial_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHdmVjG1Kaoy"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_1\"></a>\n",
        "## The `DistributionLambda` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8sUI97Kaoy"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGyQeDUaKao2"
      },
      "source": [
        "#### Create a probabilistic model using the `DistributionLambda` layer\n",
        "\n",
        "Create a model whose first layer represents:\n",
        "\n",
        "$$\n",
        "y = \\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dgf5p8Kao2"
      },
      "source": [
        "# Create a sigmoid model, first deterministic, then probabilistic\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(input_shape=(1,), units=1, activation='sigmoid',\n",
        "          kernel_initializer=tf.constant_initializer(1),\n",
        "          bias_initializer=tf.constant_initializer(0)),\n",
        "])\n",
        "\n",
        "# Plot the function\n",
        "x_plot = np.linspace(-5, 5, 100)\n",
        "plt.scatter(x_plot, model.predict(x_plot), alpha=0.4)\n",
        "plt.plot(x_plot, 1/(1 + np.exp(-x_plot)), color='r', alpha=0.8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THWcaknbKao5"
      },
      "source": [
        "# Create a constant input for this model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-N5tHQZKao8"
      },
      "source": [
        "# Explore the feedforward object...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ugBgTsxGKao-"
      },
      "source": [
        "# ... and its behaviour under repeated calls\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6I0aMgpKapD"
      },
      "source": [
        "#### Use the forward model to create probabilistic training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSL5TMgVKapD"
      },
      "source": [
        "# Use the model to create 500 training points\n",
        "\n",
        "x_train = np.linspace(-5, 5, 500)[:, np.newaxis]\n",
        "y_train = model.predict(x_train)\n",
        "\n",
        "# Plot the data and the mean of the distribution\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "ax.scatter(x_train, y_train, alpha=0.04, color='blue', label='samples')\n",
        "ax.plot(x_train, model(x_train).mean().numpy().flatten(), \n",
        "        color='red', alpha=0.8, label='mean')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abtwq7yCKapG"
      },
      "source": [
        "#### Create a new probabilistic model with the wrong weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZDJVsXmKapG"
      },
      "source": [
        "# Create a new version of the model, with the wrong weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yENtha1KapJ"
      },
      "source": [
        "#### Train the new model with the negative loglikelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VagGrOU7KapJ"
      },
      "source": [
        "# Define negative loglikelihood, which we will use for training\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQaoho3KapL"
      },
      "source": [
        "# Compile untrained model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnFKk9G5KapO"
      },
      "source": [
        "# Train model, record weights after each epoch\n",
        "\n",
        "epochs = [0]\n",
        "training_weights = [model_untrained.weights[0].numpy()[0, 0]]\n",
        "training_bias = [model_untrained.weights[1].numpy()[0]]\n",
        "for epoch in range(100):\n",
        "    model_untrained.fit(x=x_train, y=y_train, epochs=1, verbose=False)\n",
        "    epochs.append(epoch)\n",
        "    training_weights.append(model_untrained.weights[0].numpy()[0, 0])\n",
        "    training_bias.append(model_untrained.weights[1].numpy()[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQUK9RCYKapQ"
      },
      "source": [
        "# Plot the model weights as they train, converging to the correct values\n",
        "\n",
        "plt.plot(epochs, training_weights, label='weight')\n",
        "plt.plot(epochs, training_bias, label='bias')\n",
        "plt.axhline(y=1, label='true_weight', color='k', linestyle=':')\n",
        "plt.axhline(y=0, label='true_bias', color='k', linestyle='--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js3g6c9OKapS"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_2\"></a>\n",
        "## Probabilistic layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H07K-uaiKapT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osxVhElHKapV"
      },
      "source": [
        "#### Create data\n",
        "\n",
        "The data you'll be working with is artifically created from the following equation:\n",
        "$$ y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-CA1wLXKapV"
      },
      "source": [
        "# Create and plot 100 points of training data\n",
        "\n",
        "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
        "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.4)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXpmtF-uKapX"
      },
      "source": [
        "#### Deterministic linear regression with MSE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF5MF6OPKapY"
      },
      "source": [
        "# Create and train deterministic linear model using mean squared error loss\n",
        "\n",
        "# Create linear regression via Sequential model\n",
        "model = Sequential([\n",
        "    Dense(units=1, input_shape=(1,))\n",
        "])\n",
        "model.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
        "model.summary()\n",
        "model.fit(x_train, y_train, epochs=200, verbose=False)\n",
        "\n",
        "# Plot the data and model\n",
        "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "plt.plot(x_train, model.predict(x_train), color='red', alpha=0.8, label='model')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhKQ05n6Kapb"
      },
      "source": [
        "# Examine the model predictions\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiC14wElKape"
      },
      "source": [
        "#### Probabilistic linear regression with both user-defined and learned variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx8f6mmwKape"
      },
      "source": [
        "# Create probabilistic regression with normal distribution as final layer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2NRMucxKapg"
      },
      "source": [
        "# Train model using the negative loglikelihood\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSJlVYI1Kapi"
      },
      "source": [
        "# Examine the distribution created as a feedforward value\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXepFof7Kapk"
      },
      "source": [
        "# Plot the data and a sample from the model\n",
        "\n",
        "y_model = model(x_train)\n",
        "y_sample = y_model.sample()\n",
        "y_hat = y_model.mean()\n",
        "y_sd = y_model.stddev()\n",
        "y_hat_m2sd = y_hat - 2 * y_sd\n",
        "y_hat_p2sd = y_hat + 2 * y_sd\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
        "ax1.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "ax1.scatter(x_train, y_sample, alpha=0.4, color='red', label='model sample')\n",
        "ax1.legend()\n",
        "ax2.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
        "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
        "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
        "ax2.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NahVYLPKKapm"
      },
      "source": [
        "#### Probabilistic linear regression with nonlinear learned mean & variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj0z3rEtKapn"
      },
      "source": [
        "Let's change the data to being nonlinear:\n",
        "$$ y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOF25FdlKapn"
      },
      "source": [
        "# Create and plot 10000 data points\n",
        "\n",
        "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
        "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbrN-Dv7Kapp"
      },
      "source": [
        "# Create probabilistic regression: normal distribution with fixed variance\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(input_shape=(1,), units=8, activation='sigmoid'),\n",
        "    Dense(tfpl.IndependentNormal.params_size(event_shape=1)),\n",
        "    tfpl.IndependentNormal(event_shape=1)\n",
        "])\n",
        "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.01))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRn7haPjKapr"
      },
      "source": [
        "# Train model\n",
        "\n",
        "model.fit(x_train, y_train, epochs=200, verbose=False)\n",
        "model.evaluate(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWdceGQPKapu"
      },
      "source": [
        "# Plot the data and a sample from the model\n",
        "\n",
        "y_model = model(x_train)\n",
        "y_sample = y_model.sample()\n",
        "y_hat = y_model.mean()\n",
        "y_sd = y_model.stddev()\n",
        "y_hat_m2sd = y_hat - 2 * y_sd\n",
        "y_hat_p2sd = y_hat + 2 * y_sd\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), sharey=True)\n",
        "ax1.scatter(x_train, y_train, alpha=0.2, label='data')\n",
        "ax1.scatter(x_train, y_sample, alpha=0.2, color='red', label='model sample')\n",
        "ax1.legend()\n",
        "ax2.scatter(x_train, y_train, alpha=0.2, label='data')\n",
        "ax2.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
        "ax2.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
        "ax2.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
        "ax2.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk_oEVCmKapw"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_3\"></a>\n",
        "## The `DenseVariational` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR8mg4KbKapw"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFCwsZzVKapy"
      },
      "source": [
        "#### Create linear data with Gaussian noise\n",
        "\n",
        "The data you'll be working with is the same as you used before:\n",
        "$$ y_i = x_i + \\frac{3}{10}\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed. We'll be running a Bayesian linear regression on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPbuqlB5Kapz"
      },
      "source": [
        "# Use the same data as before -- create and plot 100 data points\n",
        "\n",
        "x_train = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
        "y_train = x_train + 0.3*np.random.randn(100)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjIP1ggBKap1"
      },
      "source": [
        "#### Create the prior and posterior distribution for model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDaiVcweKap1"
      },
      "source": [
        "# Define the prior weight distribution -- all N(0, 1) -- and not trainable\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GdB92laKap5"
      },
      "source": [
        "# Define variational posterior weight distribution -- multivariate Gaussian\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnv7LMUXKap6"
      },
      "source": [
        "#### Aside: analytical posterior\n",
        "\n",
        "In this tutorial, we're using a variational posterior because, in most settings, it's not possible to derive an analytical one. However, in this simple setting, it is possible. Specifically, running a Bayesian linear regression on $x_i$ and $y_i$ with $i=1, \\ldots, n$ and a unit Gaussian prior on both $\\alpha$ and $\\beta$:\n",
        "\n",
        "$$\n",
        "y_i = \\alpha + \\beta x_i + \\epsilon_i, \\quad \n",
        "\\epsilon_i \\sim N(0, \\sigma^2), \\quad \n",
        "\\alpha \\sim N(0, 1), \\quad \n",
        "\\beta \\sim N(0, 1)\n",
        "$$\n",
        "\n",
        "gives a multivariate Gaussian posterior on $\\alpha$ and $\\beta$:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\alpha \\\\\n",
        "\\beta\n",
        "\\end{pmatrix}\n",
        "\\sim\n",
        "N(\\mathbf{\\mu}, \\mathbf{\\Sigma})\n",
        "$$\n",
        "where\n",
        "$$ \n",
        "\\mathbf{\\mu}\n",
        "= \n",
        "\\mathbf{\\Sigma} \n",
        "\\begin{pmatrix}\n",
        "\\hat{n} \\bar{y} \\\\\n",
        "\\hat{n} \\overline{xy}\n",
        "\\end{pmatrix},\n",
        "\\quad\n",
        "\\mathbf{\\Sigma} = \n",
        "\\frac{1}{(\\hat{n} + 1)(\\hat{n} \\overline{x^2} + 1) - \\hat{n}^2 \\bar{x}^2}\n",
        "\\begin{pmatrix}\n",
        "\\hat{n} \\overline{x^2} + 1 & -\\hat{n} \\bar{x} \\\\\n",
        "-\\hat{n} \\bar{x} & \\hat{n} + 1\n",
        "\\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "In the above, $\\hat{n} = \\frac{n}{\\sigma^2}$ and $\\bar{t} = \\frac{1}{n}\\sum_{i=1}^n t_i$ for any $t$. In general, however, it's not possible to determine the analytical form for the posterior. For example, in models with a hidden layer with nonlinear activation function, the analytical posterior cannot be determined in general, and variational methods as below are useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGNAZCCYKap7"
      },
      "source": [
        "#### Create the model with `DenseVariational` layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ee6qHUTKap7"
      },
      "source": [
        "# Create linear regression model with weight uncertainty: weights are\n",
        "# distributed according to posterior (and, indirectly, prior) distribution\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfnMhy6xKap9"
      },
      "source": [
        "#### Train model and inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0864YLRGKap-"
      },
      "source": [
        "# Fit the model, just like a deterministic linear regression\n",
        "\n",
        "model.fit(x_train, y_train, epochs=500, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzCT1byTKaqA"
      },
      "source": [
        "# Check out the parameters of the prior and posterior distribution\n",
        "\n",
        "dummy_input = np.array([[0]])\n",
        "model_prior = model.layers[0]._prior(dummy_input)\n",
        "model_posterior = model.layers[0]._posterior(dummy_input)\n",
        "print('prior mean:           ', model_prior.mean().numpy())\n",
        "print('prior variance:       ', model_prior.variance().numpy())\n",
        "print('posterior mean:       ', model_posterior.mean().numpy())\n",
        "print('posterior covariance: ', model_posterior.covariance().numpy()[0])\n",
        "print('                      ', model_posterior.covariance().numpy()[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RWg-xARKaqC"
      },
      "source": [
        "# Plot an ensemble of linear regressions, with weights sampled from\n",
        "# the posterior distribution\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.4, label='data')\n",
        "for _ in range(10):\n",
        "    y_model = model(x_train)\n",
        "    if _ == 0:\n",
        "        plt.plot(x_train, y_model, color='red', alpha=0.8, label='model')\n",
        "    else:\n",
        "        plt.plot(x_train, y_model, color='red', alpha=0.8)        \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x7I47G6KaqE"
      },
      "source": [
        "#### Explore the effect of sample size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B3Sqy4aKaqF"
      },
      "source": [
        "# Create two datasets, one with 1000 points, another with 100\n",
        "\n",
        "x_train_1000 = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
        "y_train_1000 = x_train_1000 + 0.3*np.random.randn(1000)[:, np.newaxis]\n",
        "\n",
        "x_train_100 = np.linspace(-1, 1, 100)[:, np.newaxis]\n",
        "y_train_100 = x_train_100 + 0.3*np.random.randn(100)[:, np.newaxis]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
        "ax1.scatter(x_train_1000, y_train_1000, alpha=0.1)\n",
        "ax2.scatter(x_train_100, y_train_100, alpha=0.4)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDpDiW14KaqH"
      },
      "source": [
        "# Train a model on each dataset\n",
        "\n",
        "model_1000 = Sequential([tfpl.DenseVariational(input_shape=(1,), \n",
        "                                               units=1,\n",
        "                                               make_prior_fn=prior, \n",
        "                                               make_posterior_fn=posterior,\n",
        "                                               kl_weight=1/1000)])\n",
        "\n",
        "model_100 = Sequential([tfpl.DenseVariational(input_shape=(1,), \n",
        "                                              units=1,\n",
        "                                              make_prior_fn=prior, \n",
        "                                              make_posterior_fn=posterior,\n",
        "                                              kl_weight=1/100)])\n",
        "\n",
        "model_1000.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
        "model_100.compile(loss=MeanSquaredError(), optimizer=RMSprop(learning_rate=0.005))\n",
        "\n",
        "model_1000.fit(x_train_1000, y_train_1000, epochs=50, verbose=False)\n",
        "model_100.fit(x_train_100, y_train_100, epochs=500, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_q1wfCwKaqJ"
      },
      "source": [
        "# Plot an ensemble of linear regressions from each model\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
        "for _ in range(10):\n",
        "    y_model_1000 = model_1000(x_train_1000)\n",
        "    ax1.scatter(x_train_1000, y_train_1000, color='C0', alpha=0.02)\n",
        "    ax1.plot(x_train_1000, y_model_1000, color='red', alpha=0.8)\n",
        "    y_model_100 = model_100(x_train_100)\n",
        "    ax2.scatter(x_train_100, y_train_100, color='C0', alpha=0.05)\n",
        "    ax2.plot(x_train_100, y_model_100, color='red', alpha=0.8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooeqV6GxKaqL"
      },
      "source": [
        "#### Put it all together: nonlinear probabilistic regression with weight uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ-gVKXVKaqM"
      },
      "source": [
        "Let's change the data to being nonlinear:\n",
        "$$ y_i = x_i^3 + \\frac{1}{10}(2 + x_i)\\epsilon_i$$\n",
        "where $\\epsilon_i \\sim N(0, 1)$ are independent and identically distributed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNopZXPcKaqM"
      },
      "source": [
        "# Create and plot 1000 data points\n",
        "\n",
        "x_train = np.linspace(-1, 1, 1000)[:, np.newaxis]\n",
        "y_train = np.power(x_train, 3) + 0.1*(2+x_train)*np.random.randn(1000)[:, np.newaxis]\n",
        "\n",
        "plt.scatter(x_train, y_train, alpha=0.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF9yxmDiKaqO"
      },
      "source": [
        "# Create probabilistic regression with one hidden layer, weight uncertainty\n",
        "\n",
        "model = Sequential([\n",
        "    tfpl.DenseVariational(units=8,\n",
        "                          input_shape=(1,),\n",
        "                          make_prior_fn=prior,\n",
        "                          make_posterior_fn=posterior,\n",
        "                          kl_weight=1/x_train.shape[0],\n",
        "                          activation='sigmoid'),\n",
        "    tfpl.DenseVariational(units=tfpl.IndependentNormal.params_size(1),\n",
        "                          make_prior_fn=prior,\n",
        "                          make_posterior_fn=posterior,\n",
        "                          kl_weight=1/x_train.shape[0]),\n",
        "    tfpl.IndependentNormal(1)\n",
        "])\n",
        "\n",
        "def nll(y_true, y_pred):\n",
        "    return -y_pred.log_prob(y_true)\n",
        "\n",
        "model.compile(loss=nll, optimizer=RMSprop(learning_rate=0.005))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGlYg-jnKaqP"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(x_train, y_train, epochs=100, verbose=False)\n",
        "model.evaluate(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdeHFDzXKaqR"
      },
      "source": [
        "# Plot an ensemble of trained probabilistic regressions\n",
        "\n",
        "plt.scatter(x_train, y_train, marker='.', alpha=0.2, label='data')\n",
        "for _ in range(5):\n",
        "    y_model = model(x_train)\n",
        "    y_hat = y_model.mean()\n",
        "    y_hat_m2sd = y_hat - 2 * y_model.stddev()\n",
        "    y_hat_p2sd = y_hat + 2 * y_model.stddev()\n",
        "    if _ == 0:\n",
        "        plt.plot(x_train, y_hat, color='red', alpha=0.8, label='model $\\mu$')\n",
        "        plt.plot(x_train, y_hat_m2sd, color='green', alpha=0.8, label='model $\\mu \\pm 2 \\sigma$')\n",
        "        plt.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)\n",
        "    else:\n",
        "        plt.plot(x_train, y_hat, color='red', alpha=0.8)\n",
        "        plt.plot(x_train, y_hat_m2sd, color='green', alpha=0.8)\n",
        "        plt.plot(x_train, y_hat_p2sd, color='green', alpha=0.8)        \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkinPW72KaqU"
      },
      "source": [
        "***\n",
        "<a id=\"coding_tutorial_4\"></a>\n",
        "## Reparameterization layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKiqoElQKaqV"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVagTkYgKaqX"
      },
      "source": [
        "#### Load in the HAR dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icl9TqZAKaqY"
      },
      "source": [
        "You'll be working with the [Human Activity Recognition (HAR) Using Smartphones](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) dataset. It consists of the readings from an accelerometer (which measures acceleration) carried by a human doing different activities. The six activities are walking horizontally, walking upstairs, walking downstairs, sitting, standing and laying down. The accelerometer is inside a smartphone, and, every 0.02 seconds (50 times per second), it takes six readings: linear and gyroscopic acceleration in the x, y and z directions. See [this link](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) for details and download. If you use it in your own research, please cite the following paper:\n",
        "\n",
        "- Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \n",
        "\n",
        "The goal is to use the accelerometer data to predict the activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLpIPEaEKe25"
      },
      "source": [
        "#### Import the data\n",
        "\n",
        "The dataset required for this coding tutorial can be downloaded from the following link:\n",
        "\n",
        "https://drive.google.com/file/d/1U_O3bhvuSAzQDKHGWcBIAO80iV9FVmDo/view?usp=sharing\n",
        "\n",
        "You should store this file in Drive for use in this Colab notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15FeLLY8KfBU"
      },
      "source": [
        "# Run this cell to connect to your Drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_p1wpaTKaqY"
      },
      "source": [
        "# Load the HAR dataset and create some data processing functions\n",
        "\n",
        "# Function to load the data from file\n",
        "def load_HAR_data():\n",
        "    data_dir = '/path/to/HAR/'\n",
        "    x_train = np.load(os.path.join(data_dir, 'x_train.npy'))[..., :6]\n",
        "    y_train = np.load(os.path.join(data_dir, 'y_train.npy')) - 1\n",
        "    x_test  = np.load(os.path.join(data_dir, 'x_test.npy'))[..., :6]\n",
        "    y_test  = np.load(os.path.join(data_dir, 'y_test.npy')) - 1\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# Dictionary containing the labels and the associated activities\n",
        "label_to_activity = {0: 'walking horizontally', 1: 'walking upstairs', 2: 'walking downstairs',\n",
        "                     3: 'sitting', 4: 'standing', 5: 'laying'}\n",
        "\n",
        "# Function to change integer labels to one-hot labels\n",
        "def integer_to_onehot(data_integer):\n",
        "    data_onehot = np.zeros(shape=(data_integer.shape[0], data_integer.max()+1))\n",
        "    for row in range(data_integer.shape[0]):\n",
        "        integer = int(data_integer[row])\n",
        "        data_onehot[row, integer] = 1\n",
        "    return data_onehot\n",
        "\n",
        "# Load the data\n",
        "(x_train, y_train), (x_test, y_test) = load_HAR_data()\n",
        "y_train_oh = integer_to_onehot(y_train)\n",
        "y_test_oh = integer_to_onehot(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUybzk8lKaqa"
      },
      "source": [
        "# Inspect some of the data by making plots\n",
        "\n",
        "def make_plots(num_examples_per_category):\n",
        "    for label in range(6):\n",
        "        x_label = x_train[y_train[:, 0] == label]\n",
        "        for i in range(num_examples_per_category):\n",
        "            fig, ax = plt.subplots(figsize=(10, 1))\n",
        "            ax.imshow(x_label[100*i].T, cmap='Greys', vmin=-1, vmax=1)\n",
        "            ax.axis('off')\n",
        "            if i == 0:\n",
        "                ax.set_title(label_to_activity[label])\n",
        "            plt.show()\n",
        "        \n",
        "make_plots(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxhEIraGKaqb"
      },
      "source": [
        "#### 1D deterministic convolutional neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CV7Sxy6Kaqc"
      },
      "source": [
        "# Create standard deterministic model with:\n",
        "# - Conv1D\n",
        "# - MaxPooling\n",
        "# - Flatten\n",
        "# - Dense with Softmax\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(input_shape=(128, 6), filters=8, kernel_size=16, activation='relu'),\n",
        "    MaxPooling1D(pool_size=16),\n",
        "    Flatten(),\n",
        "    Dense(units=6, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsUT-G8YKaqd"
      },
      "source": [
        "#### Probabilistic 1D convolutional neural network, with both weight and output uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euajafSYKaqe"
      },
      "source": [
        "# Create probablistic model with the following layers:\n",
        "#  - Conv1D\n",
        "#  - MaxPooling\n",
        "#  - Flatten\n",
        "#  - Dense\n",
        "#  - OneHotCategorical\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbS5vk0_Kaqg"
      },
      "source": [
        "# Replace analytical Kullback-Leibler divergence with approximated one\n",
        "\n",
        "def kl_approx(q, p, q_tensor):\n",
        "    return tf.reduce_mean(q.log_prob(q_tensor) - p.log_prob(q_tensor))\n",
        "\n",
        "divergence_fn = lambda q, p, q_tensor : kl_approx(q, p, q_tensor) / x_train.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkwLmGHaKaqh"
      },
      "source": [
        "# Compile the model using the negative loglikelihood\n",
        "\n",
        "def nll(y_true, y_pred):\n",
        "    return -y_pred.log_prob(y_true)\n",
        "\n",
        "model.compile(loss=nll,\n",
        "              optimizer=RMSprop(learning_rate=0.005),\n",
        "              metrics=['accuracy'],\n",
        "              experimental_run_tf_function=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibkhBetoKaqj"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model.fit(x_train, y_train_oh, epochs=20, verbose=False)\n",
        "model.evaluate(x_train, y_train_oh)\n",
        "model.evaluate(x_test, y_test_oh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtBXmPJIKaql"
      },
      "source": [
        "#### Inspect model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBnw9p_tKaqm"
      },
      "source": [
        "# Define function to analyse model predictions versus true labels\n",
        "\n",
        "def analyse_model_predictions(image_num):\n",
        "\n",
        "    # Show the accelerometer data\n",
        "    print('------------------------------')\n",
        "    print('Accelerometer data:')\n",
        "    fig, ax = plt.subplots(figsize=(10, 1))\n",
        "    ax.imshow(x_test[image_num].T, cmap='Greys', vmin=-1, vmax=1)\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print the true activity\n",
        "    print('------------------------------')\n",
        "    print('True activity:', label_to_activity[y_test[image_num, 0]])\n",
        "    print('')\n",
        "\n",
        "    # Print the probabilities the model assigns\n",
        "    print('------------------------------')\n",
        "    print('Model estimated probabilities:')\n",
        "    # Create ensemble of predicted probabilities\n",
        "    predicted_probabilities = np.empty(shape=(200, 6))\n",
        "    for i in range(200):\n",
        "        predicted_probabilities[i] = model(x_test[image_num][np.newaxis, ...]).mean().numpy()[0]\n",
        "    pct_2p5 = np.array([np.percentile(predicted_probabilities[:, i], 2.5) for i in range(6)])\n",
        "    pct_97p5 = np.array([np.percentile(predicted_probabilities[:, i], 97.5) for i in range(6)])\n",
        "    # Make the plots\n",
        "    fig, ax = plt.subplots(figsize=(9, 3))\n",
        "    bar = ax.bar(np.arange(6), pct_97p5, color='red')\n",
        "    bar[y_test[image_num, 0]].set_color('green')\n",
        "    bar = ax.bar(np.arange(6), pct_2p5-0.02, color='white', linewidth=1, edgecolor='white')\n",
        "    ax.set_xticklabels([''] + [activity for activity in label_to_activity.values()],\n",
        "                       rotation=45, horizontalalignment='right')\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_ylabel('Probability')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibhqPDcJKaqo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwg676VGKaqr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9SAg6bKaqu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}